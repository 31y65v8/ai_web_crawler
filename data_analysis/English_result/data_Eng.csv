title,url,last_modified,crawl_time,language,keyword,source,text
Psychological Factors Underlying Attitudes toward AI Tools - Article - Faculty & Research - Harvard Business School,https://www.hbs.edu/faculty/Pages/item.aspx?num=64842,未知,2024-11-21 17:03:08,en,AI AND attitude,google,"Psychological Factors Underlying Attitudes toward AI Tools - Article - Faculty & Research - Harvard Business School Skip to Main Content HBS Home About Academic Programs Alumni Faculty & Research Baker Library Giving Harvard Business Review Initiatives News Recruit Map / Directions Faculty & Research Faculty ResearchFeatured TopicsAcademic Units HBS Home About Academic Programs Alumni Faculty & Research Baker Library Giving Harvard Business Review Initiatives News Recruit Map / Directions Search HBS Home About Academic Programs Alumni Faculty & Research Baker Library Giving Harvard Business Review Initiatives News Recruit Map / Directions Faculty & Research Faculty Research Featured Topics Academic Units Faculty & Research Faculty Research Featured Topics Academic Units …→ Harvard Business School→ Faculty & Research→ Publications Publications November 2023 Article Nature Human Behaviour Psychological Factors Underlying Attitudes toward AI Tools By: Julian De Freitas, Stuti Agarwal, B. Schmitt and N. Haslam Format:Print | Pages:10 Email Print ShareFacebookLinkedInTwitterEmail Abstract What are the psychological factors driving attitudes toward AI tools, and how can resistance to AI systems be overcome when they are beneficial? In this perspective, we first organize the main sources of resistance into five main categories: opacity, emotionlessness, rigidity, autonomy, and group membership. We relate each of these barriers to fundamental aspects of cognition, then cover empirical studies providing correlational or causal evidence for how the barrier influences attitudes toward AI tools. Second, we separate each of the five barriers into AI-related and user-related factors, which is of practical relevance in developing interventions toward adoption of beneficial AI tools. Third, we highlight potential risks arising from these well-intentioned interventions. Fourth, we explain how the current perspective applies to various stakeholders, including how to approach interventions that carry known risks, and point to outstanding questions for future work. Keywords Policy; Self; AI and Machine Learning; Attitudes; Technology Adoption Citation De Freitas, Julian, Stuti Agarwal, B. Schmitt, and N. Haslam. ""Psychological Factors Underlying Attitudes toward AI Tools."" (pdf) Nature Human Behaviour 7, no. 11 (November 2023): 1845–1854. Read Now About The Author Julian De Freitas Marketing →More Publications More from the Authors October 2024 Faculty Research AI and Brand Management: Promises and Perils By: Julian De Freitas and Elie Ofek 2024 Faculty Research Lessons from an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships By: Julian De Freitas, Noah Castelo, Ahmet Uğuralp and Zeliha Uğuralp September 23, 2024 Wall Street Journal AI Wants to Make You Less Lonely. Does It Work? By: Julian De Freitas More from the Authors AI and Brand Management: Promises and Perils By: Julian De Freitas and Elie Ofek Lessons from an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships By: Julian De Freitas, Noah Castelo, Ahmet Uğuralp and Zeliha Uğuralp AI Wants to Make You Less Lonely. Does It Work? By: Julian De Freitas ǁ Harvard Business School Soldiers Field Boston, MA 02163 →Map & Directions →More Contact Information Make a Gift Site Map Jobs Harvard University Trademarks Policies Accessibility Digital Accessibility Copyright © President & Fellows of Harvard College. PDF ×"
"
      Public attitudes to data and AI: Tracker survey (Wave 3) - GOV.UK
  ",https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-3/public-attitudes-to-data-and-ai-tracker-survey-wave-3,未知,2024-11-21 17:03:49,en,AI AND attitude,google,"Public attitudes to data and AI: Tracker survey (Wave 3) - GOV.UK Cookies on GOV.UK We use some essential cookies to make this website work. We’d like to set additional cookies to understand how you use GOV.UK, remember your settings and improve government services. We also use cookies set by other sites to help us deliver content from their services. You have accepted additional cookies. You can change your cookie settings at any time. You have rejected additional cookies. You can change your cookie settings at any time. Accept additional cookies Reject additional cookies View cookies Hide this message Skip to main content GOV.UK Navigation menu Menu Menu Search GOV.UK × Search GOV.UK Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments Departments, agencies and public bodies News News stories, speeches, letters and notices Guidance and regulation Detailed guidance, regulations and rules Research and statistics Reports, analysis and official statistics Policy papers and consultations Consultations and strategy Transparency Data, Freedom of Information releases and corporate reports Search Search GOV.UK Search Home Public attitudes to data and AI: Tracker survey (Wave 3) Centre forData Ethicsand Innovation Department forScience, Innovation& Technology Research and analysis Public attitudes to data and AI: Tracker survey (Wave 3) Updated 12 February 2024 Contents 1. Foreword 2. Executive summary 3. Introduction 4. The value of data to society 5. The risks of data use to society 6. Attitudes towards AI 7. Perceived impact of AI 8. Preferences for how AI is used 9. Attitudes of those with very low digital familiarity 10. Methodology 11. Annex Print this page © Crown copyright 2024 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visit nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-3/public-attitudes-to-data-and-ai-tracker-survey-wave-3 1. Foreword Advances in artificial intelligence (AI) and other data-driven technologies have huge potential for public good and have already begun to tackle some of the biggest challenges facing our society. From speeding up the diagnosis of diseases to making transport more efficient and predicting extreme weather events, AI-powered breakthroughs will improve the daily lives of people across the UK. The UK Government’s commitment to establishing the country as a leader in AI will help ensure that UK citizens continue to benefit from the transformational power of AI in the years to come. To ensure that the benefits of AI are felt across the UK, it is essential to build justified trust in these systems. Understanding public attitudes towards AI and data-driven technologies will ensure that innovation works for everyone and does not propagate or exacerbate inequalities. The recent explosion of generative AI into public view means that it is more important than ever that we understand the public’s hopes, expectations and concerns as these technologies develop. The Centre for Data Ethics and Innovation (CDEI) leads the Government’s work to enable trustworthy innovation using data and AI; its Public Attitudes to Data and AI (PADAI) Tracker Survey has monitored public attitudes to data-driven technology and AI since 2021. Building on Wave 2, this third iteration of the survey has a greater focus on AI to help us understand the challenges that come with the rapid advancements in the sector. We understand that public opinions relating to data and AI are greatly nuanced and context dependent. To ensure inclusive public engagement that captures a range of voices, CDEI’s Tracker Survey interviews 4,000 members of the UK public, with an additional 200 interviews with digitally excluded adults to ensure their views are adequately reflected in the work. The findings from the Tracker Survey, which is the first in the world of its kind, will continue to underpin the government’s approach to AI and data. We hope that, as with the two previous iterations of the survey, it will also inform work happening across civil society, academia, and industry, emphasising the importance of centring public voices in the wider AI discussion. Viscount Camrose, Minister for Artificial Intelligence and Intellectual Property 2. Executive summary 1. The public increasingly recognises the potential societal benefits of data use but is sceptical about their equitable distribution. The public identified the cost of living, health and the economy as the most promising domains in which data can be leveraged for societal good. These mirror what the public perceive as the greatest issues currently facing the UK, highlighting the public’s confidence in the transformative power of data. The public also recognises the opportunity for data to have individual-level benefits, with most people agreeing that data is useful for creating products and services that benefit them as individuals. However, concerns about equity persist. A third of the public disagree that all groups in society will benefit equally from data use, with digitally excluded and older people among the most concerned. 2. The public’s key concern regarding data use remains its security, though there is growing confidence that organisations will face consequences for any breaches. The primary public concerns about data use are the potential for insecure data storage to lead to hacking or data theft, and the fear that data will be sold to other organisations for profit. These concerns may be reinforced by real-life stories reported in the media. This year’s survey found an increase in respondents recalling media narratives portraying data use in a negative light, with data breaches and leaks standing out as the most memorable incidents. Although these concerns are underscored by a feeling of limited control over personal data, there are signs that this is changing. A growing proportion of the public agrees that, when data misuse occurs, organisations are held accountable. 3. As understanding and experience of AI grows, the public is becoming more pessimistic about its impact on society. Following the emergence of large language models into public view in late 2022, their use has become relatively widespread; a third of the public report using chatbots at least once a month in their day-to-day lives. In parallel, self-reported awareness and understanding of AI has increased across society, including among older people, people belonging to lower socio-economic grades and people with lower digital familiarity. However, alongside this increased understanding, there are ongoing anxieties linked to AI. A growing proportion of the public think that AI will have a net negative impact on society, with words such as ‘scary’, ‘worry’ and ‘unsure’ commonly used to express feelings associated with it. 4. While AI is expected to produce increased day-to-day convenience and improved public services, apprehensions remain about job displacement and human de-skilling. The public is optimistic about the potential for AI to streamline everyday tasks, and to improve key public services including healthcare, policing, and education. Nonetheless, a spectrum of risks is recognised by the public. Most notably, there is widespread concern that AI will displace jobs, particularly among non-graduates, and that AI will erode human creativity and problem-solving skills. Recognition of several existential risks relating to AI was also widespread, likely reflecting the increased visibility of this narrative in the months leading up to survey fieldwork. 5. While preferences about whether AI is used are largely situation dependent, risk mitigation strategies can help to alleviate public concern. When asked to choose between two potential applications of AI, the public were generally more influenced by the specific application than by any associated risks or benefits. The public is particularly positive towards the use of AI to detect cancer and to identify people in need of financial support. In contrast, the public is averse to AI being used to mark students’ homework or assess a person’s risk of failing to repay a loan. The context dependent nature of AI preferences may underlie the mixture of unease and positivity seen in other areas of the survey. Encouragingly, the survey also shows that clear and effective risk mitigation strategies can reduce the impact of risks on the public’s appetite for AI use. 6. Those with very low digital familiarity feel less in control of their data, but increasingly recognise the benefits of data use and trust accountability for data misuse. Members of the public with very low digital familiarity tend to feel they have less control over their data compared with the broader UK population, though this gap has been closing over time. Within this group, there’s increasing confidence that organisations are being held accountable for data misuse, and a growing acknowledgement of the constructive impacts that data can have at both a societal and individual level. However, despite these shifts, those with very low digital familiarity remain relatively pessimistic, with the majority anticipating a negative or neutral impact of AI on society. 3. Introduction The Centre for Data Ethics and Innovation (CDEI)’s Public Attitudes to Data and AI (PADAI) Tracker Survey monitors public attitudes towards data-driven technologies, including artificial intelligence (AI), over time. This report summarises findings from the third wave (Wave 3) of research and identifies how public attitudes have changed since the previous waves (Wave 1 and Wave 2). The research was conducted by Savanta on behalf of the CDEI. The research uses a mixed-mode data collection approach comprising online interviews (Computer Assisted Web Interviews - CAWI) and a smaller telephone survey (Computer Assisted Telephone Interviews - CATI) to ensure that those with very low digital familiarity are represented in the data. Key information on each survey wave, including survey mode, respondent profile, sample size and fieldwork dates can be found in Table 1. Full details of the methodology, including notes on interpreting the data in this report, are provided in the Methodology section. Table 1: Overview of survey Waves 1, 2, and 3 Wave 1 (Dec 2021/Jan 2022) Wave 1 (Dec 2021/Jan 2022) Wave 2 (Jun/Jul 2022) Wave 2 (Jun/Jul 2022) Wave 3 (Aug/Sept 2023) Wave 3 (Aug/Sept 2023) CAWI (Online sample) CATI (Telephone sample) CAWI (Online sample) CATI (Telephone sample) CAWI (Online sample) CATI (Telephone sample) Respondents Demographically representative sample of UK adults (18+) ‘Very low digital familiarity’ Demographically representative sample of UK adults (18+) ‘Very low digital familiarity’ Demographically representative sample of UK adults (18+) ‘Very low digital familiarity’ Number of interviews 4,250 200 4,320 200 4,225 209 Fieldwork dates 29 November to 20 December 2021 15 December to 14 January 2022 27 June to 18 July 2022 1 to 20 July 2022 11 to 23 August 2023 15 August to 7 September 2023 4. The value of data to society 4.1 Summary The value the public places on data collection and analysis has increased in the last year; with 57% of the public recognising the personal benefits of data and 44% acknowledging its societal value. There is alignment between the public’s perception of the key areas where data can contribute to societal good - health, cost of living, and the economy - and public perceptions of the greatest issues facing the UK, demonstrating the public’s belief in the power of data. However, there is still scepticism around the equal distribution of these benefits across society, with only 33% in agreement that all groups reap the benefits equally. 4.2 The value of data use to society Respondents were presented with a series of statements relating to the value of data use and asked to indicate the extent to which they agreed or disagreed with each. As shown in Figure 1, the public are more likely to recognise the individual-level benefits of data than the societal-level benefits. However, over the last year, there has been a positive shift in the public’s view of the value of data at both a societal and individual level. The majority of the public (57%) agrees that data is useful for creating products and services that benefit them as individuals (increased from 53% at Wave 2 and 51% at Wave 1)[footnote 1], with only 13% disagreeing. A smaller proportion of the public (44%) agrees that data collection and analysis is good for society, with 33% remaining neutral on this matter. However, this still represents a positive shift. Figure 1: Attitudes towards the statements “Data is useful for creating products and services that benefit me” and “Collecting and analysing data is good for society” over time Q12. Summary Table Net Agree: Please indicate how much you agree or disagree with each of the following statements? BASE: All online respondents: November/December 2021 (Wave 1), n=4,250, June/July 2022 (Wave 2), n=4,320, August 2023 (Wave 3), n=4,225 Those with high digital familiarity[footnote 2] are more likely than those with low digital familiarity to agree that data is useful for creating products and services that benefit them (66% and 37% respectively) and that data collection and analysis is good for society (51% and 32% respectively). Moreover, regression analysis revealed that younger people and those from higher socio-economic grades (ABC1) were more likely to recognise both the individual-level and societal-level benefits of data use than older people and those in lower socio-economic grades (C2DE) respectively (Annex, Models 1 and 2). To provide further insight into how data use can provide societal-level value, respondents were asked where they saw the greatest opportunity for data use to benefit the public. Health (21%), the cost of living (18%), and the economy (8%) emerged as areas where the public sees the most promise for data-driven improvements. As demonstrated in Figure 2, these perceived opportunities coincide with what the public perceives as the most important issues facing the country. This finding highlights the public’s confidence in the transformative power of data. Despite this, the public sees limited potential for data to improve other tested areas like crime, housing, education, and inequality. Figure 2: The most important issues facing the country and opportunities for data use, August 2023 (Showing any issues for the UK selected by at least 5% of respondents; axes split along median value) Q15b. Which of the following do you think are the most important issues facing the country for you personally at this time? and Q16. In which of these issues, if any, do you think the use of data presents the greatest opportunity for making improvements that benefit the public in this country? BASE: All online respondents: August 2023 (Wave 3), n=4,225 4.3 The equity of benefits of data use Despite the increased public recognition of the benefits of data at both individual and societal levels, the public expresses doubt that these advantages are felt equally across society. As shown in Figure 3, only a third (33%) agree that all groups in society benefit equally from data use, with a similar proportion (32%) disagreeing with this statement. There has been an increase in the proportion of participants that agree with this statement since Wave 2 (26%). Figure 3: Attitudes towards the statement “All groups in society benefit equally from data use” over time Q12. Summary Table Net Agree: Please indicate how much you agree or disagree with each of the following statements? BASE: All online respondents: November/December 2021 (Wave 1), n=4,250, June/July 2022 (Wave 2), n=4,320, August 2023 (Wave 3), n=4,225 There is a clear positive relationship between digital familiarity and perceptions of equity. People with high digital familiarity are the most optimistic about the equal distribution of data benefits across society; 38% agree that all groups will benefit equally. In comparison, those with medium (28%) or low (25%) digital familiarity are substantially less likely to agree. Concerns are especially pronounced among those with very low digital familiarity (who were interviewed via the telephone survey), of whom 46% disagree that all groups in society benefit equally from data use, compared with 32% of the public overall. 4.4 Trust in data actors to benefit society While the public increasingly recognises the benefits of data use for society, trust in data actors to achieve this societal benefit varies and remains strongly related to trust in those actors to use data effectively, safely, transparently and with accountability (see Table 2). The NHS continues to inspire the highest public trust to use data to benefit society (75%), followed by university researchers (63%) and pharmaceutical companies (62%). More modest levels of trust are seen with respect to the government and large technology companies (41% each), whereas social media companies inspire the lowest levels of trust (28%). No clear pattern emerges to suggest a difference between levels of trust in the public and private sectors to use data to benefit society. Researchers in universities and pharmaceutical companies are trusted to a similar extent (63% and 62% respectively), as are the government and big technology companies (41% each). Trust in actors to use data to benefit society has remained relatively unchanged since Wave 2, except for regulators (51% in Wave 2 compared with 56% in Wave 3), in which trust has increased, and university researchers (66% in Wave 2 compared with 63% in Wave 3), in which trust has decreased. It is important to note that, to aid respondent understanding, the Wave 3 question wording was changed to include examples of specific regulators. This may have contributed to the observed increase in trust levels. Table 2: Trust in organisations, and in their actions with data (Showing % Sum: Trust) Q14. To what extent, if at all, do you trust the [organisation] to…? BASE: Approximately half of all online respondents per organisation shown, August 2023 (Wave 3), n = 2,059 – 2,171 5. The risks of data use to society 5.1 Summary The greatest public concern about data use remains its security, with a particular focus on the potential for insecure storage to lead to hacking or theft, and apprehensions about data being sold for profit. These concerns are possibly influenced by news stories in the media, with an observed increase in respondents recalling stories portraying data use negatively, and with data breaches and leaks being particularly memorable themes. Despite these concerns and a prevailing sentiment of limited control over personal data, the public increasingly believe that organisations are held to account when they misuse data, indicating a growing confidence in the accountability mechanisms in place. 5.2 Concerns regarding the use of data in society Respondents were asked what they perceived as the greatest risks of data use in society; results are provided in Figure 4. The most frequently selected risks are insecure data storage leading to possible hacking or theft (57%), and data being sold to other organisations for profit (55%). Concerns about data risks vary across the population. Older individuals, particularly those aged 55 and above, are more likely to express worry about data not being held securely (69%) and being sold to other organisations for profit (64%), compared with 35-54-year-olds (56% and 54% respectively), and 18-34-year-olds (42% and 44%). A larger proportion of those with high (60%) or medium (57%) digital familiarity are concerned about data security than those with low digital familiarity (46%). In addition, more women (59%) are worried about data security and hacking than men (56%), but a higher proportion of men (37%) consider data being used for surveillance purposes a risk than women (28%). Figure 4: Greatest risks for data use in society (Showing % selected each option) Q17b. Which of the following do you think represent the greatest risks for data use in society? BASE: All online respondents: August 2023 (Wave 3), n=4,225 The high level of concern regarding data security is underpinned by a feeling among two in five members of the public (40%) that they lack control over who uses their data and how. Although public confidence that individuals have control over their data is divided (35% agree while 40% disagree), agreement that individuals have control has increased since previous waves (from 29% in Wave 2 and 33% in Wave 1). Figure 5: Attitudes towards the statement “I have control over who uses my data and how” over time Q12. Summary Table Net Agree: Please indicate how much you agree or disagree with each of the following statements? BASE: All online respondents: November/December 2021 (Wave 1), n=4,250, June/July 2022 (Wave 2), n=4,320, August 2023 (Wave 3), n=4,225 5.3 Public recall of data usage media stories Respondents were asked about their exposure to news stories regarding data use in the last six months, through articles, TV, or radio. Around half of the public (48%) reported having seen a news story about data, an increase from 40% in Wave 2 and 37% in Wave 1. However, as the salience of stories about data has increased, there has also been a shift towards a more negative sentiment in the narratives recalled by respondents. As shown in Figure 6, the majority (65%) of those who recalled news stories reported that data use was portrayed negatively, an increase since Wave 2 (53%) and Wave 1 (37%). This suggests that the risks of data use are becoming increasingly prominent in the public’s consciousness, and that the media may contribute to these negative associations. Figure 6: The recalled presentation of data in news stories over time (Showing % selected each option) Q11. Overall, do you think this news story presented the way data was being used positively or negatively? Base: All respondents who have read, seen or heard a story about data being used recently, and could remember the story: November/December 2021 (Wave 1), n = 1,499, June/July 2022 (Wave 2), n = 1,678, August 2023 (Wave 3),n = 1,303 Among respondents who could recall the subject of a relevant media story, data breaches or leaks (36%) and misuse of data by the government, companies, or individuals (10%) were the subjects mentioned most frequently (see Figure 7). These themes closely resemble the most frequently selected fears relating to data use (see Figure 4). Frequently mentioned negative media stories included the Police Service of Northern Ireland data breach, and the Electoral Commission data breach. Both stories were widely reported in August 2023 while Wave 3 fieldwork was being conducted. Where positive news stories were recalled by respondents, these often emphasised the role of data in medicine and research. For example, respondents recalled stories about the ‘NHS using patient data to help medical research’ and ‘Collecting data within the NHS about a person to get a better picture of what’s wrong with them’. These responses often mention words such as “protection” and “analysis”, and organisations such as the Google, NHS, and legal institutions. Figure 7: Public recall of data-related news stories (Showing all themes mentioned by those who recall seeing a news story about data in the past six months) Q10. In a couple of sentences, please could you briefly tell us what the story you saw about data was about? Base: All CAWI respondents who say they have seen a news story about data in the last 6 months: August 2023 (Wave 3), n=2014 (coded responses to an open text question). 5.4 Perception of accountability of data actors Despite growing concerns about data misuse and a perceived lack of control over the use of personal data, a growing proportion of the public agree that data actors are being held to account. Just under half of the public (45%) agrees that organisations that misuse data are held accountable, an increase from 41% in Wave 2, but just over a third (35%) still disagrees. Younger respondents (50% of 18-34s and 48% of 35-54s) and those from lower socio-economic backgrounds (C2DE, 46%) are more likely to agree that organisations are held to account in the instance of data misuse, compared with those aged 55+ (38%) and respondents from higher socio-economic grades (ABC1, 43%). Figure 8: Attitudes towards the statement “When organisations misuse data, they are held accountable” over time Q12. Summary Table Net Agree: Please indicate how much you agree or disagree with each of the following statements? BASE: All online respondents: June/July 2022 (Wave 2) n=4,320, August 2023 (Wave 3), n=4,225 6. Attitudes towards AI 6.1 Summary The use of large language models is relatively widespread, with a third (34%) of the UK public using chatbots on at least a monthly basis in their personal life and a quarter (24%) doing so for work. Reflecting this, self-reported awareness and understanding of AI among the UK public has increased since last year across most groups in society, including older individuals, those belonging to lower socio-economic grades, and those with lower digital familiarity. The vast majority of the public (95%) report having heard of AI and a considerable proportion (66%) report being able to give at least a partial explanation of what AI is. Alongside the growing understanding and use of AI, there are ongoing anxieties linked to the technology, with ‘scary’, ‘worry’, and ‘unsure’ being the most common feelings expressed about it. 6.2 Use of large language models To understand the behavioural uptake of AI by the public, respondents were provided with a short explanation of large language models (referred to in the survey as ‘chatbots’) before being asked how frequently they had used them in the previous three months. A third of the public (34%) report having used chatbots at least once a month in their day-to-day lives and a quarter (24%) report doing so for work purposes. However, there remains a large proportion of the population that has not used chatbots for either personal (44%) or professional (64%) purposes in the previous quarter. Regression analysis was used to identify the demographic dimensions that predict the behavioural uptake of chatbots. Female respondents, older respondents, and non-graduates were less likely to report frequent usage of chatbots in their personal life compared with male respondents, younger respondents, and graduates respectively. Less frequent usage was also identified among respondents who did not trust large technology companies (Annex, Models 8-9). 6.3 Awareness and understanding of AI Self-reported awareness and understanding of AI[footnote 3] is very high, having increased substantially over the last year. As shown in Figure 9, the vast majority of the public have now heard of AI (95% compared with 89% in Wave 2). Furthermore, two in three respondents (66%) now report that they could provide at least a partial explanation of what AI is, an increase from 56% in Wave 2. The trend of increased awareness and understanding of AI is apparent across nearly all demographic groups, including those with lower baseline levels of awareness. For example, although those from lower socio-economic grades are less likely to be aware of AI (C2DE, 93%) than those from higher socio-economic grades (ABC1, 96%), both those from lower and higher socio-economic grades have seen increases in awareness over the last year (by four percentage points and eight percentage points respectively). While there appears to be an overall increase in awareness and understanding of AI, disparities still remain. For example, those with low digital familiarity are less likely to report being aware of AI than respondents with high or medium digital familiarity (89% compared with 96% and 95% respectively). Similarly, older respondents are less likely than younger respondents to report that they could explain, at least partially, what AI is (74% of those aged 18–34, compared with 66% of those aged 35-54 and 62% of those aged 55+). Figure 9: Awareness of AI over time (Showing % selected each option) Q21. Have you ever heard of the term Artificial Intelligence (AI)? BASE: All online respondents: June/July 2022 (Wave 2) n=4320, August 2023 (Wave 3), n=4225 6.4 Sentiment towards AI Despite the notable increase in understanding of AI, the public’s anxieties associated with AI were brought to the surface when respondents were asked to enter a single word that best captured their feelings about AI. Negative and neutral associations with AI are far more prevalent than positive associations, illustrating the level of concern felt towards AI. ‘Scary’ was by far the most common term provided (n=423), followed by ‘worry’ (n=240). The third most common association expressed a lack of knowledge or feeling of being ‘unsure’ (n=209). The most common unambiguously positive terms provided were ‘excited’ and ‘good’ (both n=53). Responses to this question have been visualised in Figure 10. Figure 10: Word cloud of public sentiment towards AI by UK adults, Wave 3 (visualising the top 50 most often mentioned words) Q22. Please type in one word that best represents how you feel about ‘Artificial Intelligence’. Base: All online respondents to say they have heard of AI in August 2023 (Wave 3) and to leave a valid response, n=3453 To an extent, results from Wave 2 and Wave 3 are similar, with ‘scary’ and its synonyms being the most commonly cited word in both waves, while ‘worry’ and ‘unsure’ also continue to be common choices. The principal difference is that in Wave 2 ‘scary’, while the most commonly chosen word, was most prevalent by a margin of less than 100. In Wave 3, ‘scary’ is the most prevalent word by a margin of over 180. In addition, references to the word ‘robot’ are less frequent in Wave 3 compared with Wave 2 (ranked 2nd in Wave 2 compared with 5th in Wave 3). Figure 11: Word cloud of public sentiment towards AI by UK adults, Wave 2 (visualising the top 50 most often mentioned words) Q22. Please type in one word that best represents how you feel about ‘Artificial Intelligence’. Base: All online respondents to say they have heard of AI in June/July 2022 (Wave 2) and to leave a valid response, n=3132 Analysis demonstrates that those with positive expectations about the impact of AI show openness to AI’s potential, while those with negative expectations about the impact of AI mainly associate AI with fear. We compared the frequency of the 50 most common terms written by those who believe AI will either positively or negatively impact society[footnote 4]. What best distinguishes the two groups is not the presence or absence of fear and worry, but the variety of attitudes held by each group. While those who think that AI will have an overall positive impact on society frequently wrote ‘scary’ (n=53), this appeared in relatively equal measure alongside positive words such as ‘interest’ (n=42), ‘future’ (n=37) and ‘excited’ (n=31). In contrast, fear dominates the responses of those who suggest that AI will have an overall negative impact on society. More than twice the number of people selected ‘scary’ (n=140) as did the next most common term (worry, n=62). 7. Perceived impact of AI 7.1 Summary While the majority of the public has a neutral view of AI’s likely impact on society, levels of pessimism have increased since last year. The public largely expects AI to have a positive impact on streamlining everyday tasks and enhancing key public services such as healthcare, policing, and education. However, the perceived impact of AI on the labour market and how fairly people are treated is more contentious, with the most common concerns being job displacement and de-skilling leading to diminished human creativity and problem-solving skills. 7.2 The impact of AI on society The public has mixed attitudes towards AI’s future impact on society, with relatively small proportions adopting extreme positive or negative viewpoints. Figure 12 details how respondents rated the overall impact that AI will have on society on a scale from 0 (very negative) to 10 (very positive). Over half (58%) expect the societal impact of AI to be neutral (score 4-7), while 14% predict a positive impact (score 8-10) and a quarter (25%) predict a negative impact (score 0-3). The proportion of those who expect AI to have a negative impact on society has increased by five percentage points (from 20%) since Wave 2. Figure 12: The perceived impact of AI on society over time Q23b. On a scale from 0-10 where 0 = very negative impact and 10 = very positive impact, based on your current knowledge and understanding, what impact do you think Artificial Intelligence (AI) will have overall on society? BASE: All online respondents: June/July 2022 (Wave 2), n=3,838, August 2023 (Wave 3), n=4,008 Despite the relative neutrality of opinions concerning the future impact of AI on society, differences between groups do emerge. Results from regression analysis indicate that female respondents, non-graduates, and respondents who are unable to explain AI are less likely to predict a positive impact of AI compared with male respondents, graduates, and respondents who felt able to provide some explanation of AI (Annex, Model 10). 7.3 Expected positive and negative impacts of AI When asked about the degree to which they expect the introduction of AI to yield positive or negative outcomes across different situations, respondents reveal diverse expectations (see Figure 13). Just over half think that AI will positively influence the ease of day-to-day tasks (52%) and healthcare for themselves and their family (51%). A substantial proportion think AI will have a positive impact on crime prevention and detection (44%) and education (39%). However, more negative views are expressed when it comes to wider societal impacts. More than two in five (43%) foresee a negative influence of AI on job opportunities for people like them, with only around a quarter anticipating either a positive (22%) or neutral (27%) impact. In addition, 31% of UK adults say AI will have a negative impact on how fairly people are treated in society, compared with 22% expecting a positive impact and 38% foreseeing a neutral impact. Figure 13: Expected impact of AI across different situations Q25b. Summary Table: To what extent do you think the use of Artificial Intelligence will have a positive or negative impact for the following types of situations? BASE: All online respondents: August 2023 (Wave 3), n=4,225 7.4 Anticipated risks from the use of AI When asked about risks AI poses to society, the public do not indicate a single dominant concern, but a variety of potential risks are identified by relatively high proportions of respondents (see Figure 14). Concerns about the impact of AI on the labour market are clear; job displacement due to AI (45%) and the potential loss of human creativity and problem-solving skills (35%) are the two most frequently selected risks. Existential risks, including humans losing control over AI (34%) and AI being used for cyber-crime and terrorism (23%) are also prevalent. In contrast, smaller proportions of the public express concern about potential near-term risks, including AI making decisions that humans can’t understand or explain (23%), and AI being biased and leading to unfair outcomes (14%). There are clear demographic patterns that emerge in the perception of risk. For example, non-graduates (48%) are more likely than their graduate counterparts (41%) to view job displacement as one of the greatest risks of AI. Younger people, aged 18-34, are more likely to stress the risk that AI will negatively impact people’s mental health and wellbeing (16% compared with 10% of those aged 55+) and that biased AI will lead to unfair outcomes (16% compared with 13% of those aged 55+). Figure 14: The greatest risks from using AI in society (Showing % selected each option) Q39. Some people have suggested that there may be risks to society from using Artificial Intelligence (AI). Which of the following, if any, do you think represent the greatest risks from the use of AI in society? Please choose up to three. BASE: All online respondents: August 2023 (Wave 3), n=4,225 7.5 Need for AI governance Respondents were asked to select up to three sectors in which they think it is crucial for governments to carefully regulate AI to avoid negative outcomes for users. Healthcare (29%), the military (27%), and banks and finance (25%) are the most frequently selected sectors. Compared with Wave 2, the proportion of respondents that think AI in healthcare needs careful regulation has decreased (down 4 percentage points from 31% in Wave 2), while the proportion that selects banks and finance has increased (up three percentage points from 22% in Wave 2). Digital familiarity also influences the choices of the public. Those with high digital familiarity are more likely to indicate that self-driving cars (22% compared to 19% with medium and 11% with low familiarity), education (17% compared to 14% and 12%), and hiring and recruitment (14% compared to 10% and 9%) need careful regulation by governments. As shown in Figure 15, the sectors in which regulation is most valued align with the sectors in which the public foresee AI having the greatest positive impact. Together, these findings suggest that, even in sectors where the public is optimistic about the impact of AI, there remains an awareness of the potential risks. In these cases, the public think governance of AI is necessary to realise the benefits, while mitigating risks. Figure 15: The most important areas in AI for the government to carefully manage by areas where AI will have the biggest positive impact over the next decade (axes split along median value) Q32. SUMMARY TOP 3: Which of the following areas do you think is important that governments carefully manage to make sure the use of AI does not lead to negative outcomes for users? Q33b. SUMMARY TOP 3: In which of the following areas, if any, do you think the use of Artificial Intelligence (AI) will have the biggest positive impact in society over the next 10 years? BASE: All online respondents: August 2023 (Wave 3), n=4,225 8. Preferences for how AI is used 8.1 Choice-based experiment (Conjoint) To study people’s preferences for how AI is used, we incorporated a conjoint experiment within the online survey. What follows is a brief description of the conjoint design and its outputs, to aid understanding of the results that follow. For a full description of the method, please see the Methodology section. Respondents were presented with five pairs of scenarios and asked to pick the one they preferred. The scenarios differed from one another in terms of the features of the AI presented. The results of the experiment show which of these features influenced respondents to pick one AI scenario over another. A full list of all features can be found in the Annex. An example of how pairs of scenarios were presented to respondents is illustrated in Figure 16. Figure 16: Example of a possible scenario pairing, as presented to respondents in the conjoint experiment The features of the AI presented were grouped into the following overarching categories, which we call attributes: Attribute 1: AI application. This refers to what the AI would be used to do. An example feature in this attribute group is ‘Identify people who need financial support’ (see Scenario 1 in Figure 14). Attribute 2: The benefit of using the AI, relative to other methods. An example feature in this attribute group is ‘More accurate than alternative approaches’ (see Scenario 1 in Figure 14). Attribute 3: The risk associated with the use of AI and the associated governance mechanism. An example of a feature in this attribute group is ‘people won’t know whether AI is being used’ and ‘it is made clear when AI is being used’ (see Scenario 1 in Figure 14). By analysing which scenarios were chosen by respondents across all pairings, we can produce two key indicators. First, we can assess the relative importance of each attribute category as a driver of the general public’s AI preferences. Second, we can see whether individual AI features make respondents more or less likely to select an AI scenario. In addition, we can assess the impact of risk mitigation strategies on public opinion. Half of respondents saw risk features presented without a governance mechanism (Model One), and the other half of respondents saw both a risk and a governance mechanism designed to mitigate that risk (Model Two), as shown in Figure 14. By comparing the results of Model One with Model Two, we can gauge the impact these governance mechanisms have on the public’s preferences. 8.2 Attribute analysis When respondents express a preference for one AI scenario over another, they mostly do so based on the specific application of the AI. The AI application is responsible for 60.9% of respondent decision-making in Model One, and 63.5% in Model Two. This is more than double the influence over respondent decision-making than can be attributed to risks (24.1% in Model One and 23.8% in Model Two), and more than triple the influence of benefits (14.9% in Model One, and 12.7% in Model Two). The public appear to be much more influenced by what the AI is used for, than they are by associated risks or benefits. The figures presented in Table 3 denote the raw importance of each attribute group when deciding which AI scenario they prefer. However, these scores represent the whole impact of the attribute group, and do not distinguish whether that impact is positive or negative. This information is provided at a more granular level, however, by analysis of the individual features within each attribute category. Table 3: Overview of results for three attributes of AI by Model Model 1 Model 2 Application 60.9% 63.5% Benefits 14.9% 12.7% Risks 24.1% 23.8% CONJOINTQ. Which of these scenarios is your preference for how AI is used? BASE: All online respondents August 2023 (Wave 3) who saw each Model, Model 1 n=2,119, Model 2 n=2,106 8.3 Impact of application on preferences for use of AI Among the specific AI applications tested, AI being used to detect cancer from an x-ray image had the greatest positive impact on respondent preferences. This echoes other findings across the survey. For example, over half of respondents report that the use of AI in healthcare will have a positive impact (51%), and the public are most likely to select healthcare as the use of AI with the biggest positive impact in society over the next 10 years (34%). Using AI to identify people who need financial support to pay their energy bills also has a positive impact on preferences, though this impact is less strong than the application of AI to detect cancer. The application of AI to mark students’ homework and to assess the risk of an individual failing to repay a loan have a particularly negative impacts on preferences. The strength of this negative impact could be attributed to the concerns about fairness seen elsewhere in the survey. For example, three in ten (31%) people see the use of AI as having a negative impact for how fairly people are treated in society. 8.4 Impact of benefits on preferences for use of AI The public appear relatively unmoved by any particular promise that AI may be better than alternative approaches. The tested benefits do differ in whether they have a positive or negative impact. However, the strength of their impact, positive or negative, is so consistently weak that they cannot be viewed as having much weight. One potential explanation for why this might be the case is the formulation ‘This approach could be … than alternative approaches’ (e.g., more accurate, faster, etc.). It may be that people already assume that AI will be better or worse than alternative approaches. For example, it is unlikely that the use case ‘detecting the presence of cancer’ would be viewed so positively, if people did not already assume that AI would be more accurate or faster (or at least as accurate or fast) as a human doctor when executing this task. If this is the case, listing specific benefits fails to have an impact on peoples’ decisions, because people already assume the benefits mentioned are present. 8.5 Impact of risks on preferences for use of AI Much like AI applications, risk features have a complex impact on respondents’ preferences. When presented on their own (as in Model One) some risks have a negative impact. Other risks have little or no impact, and some have a slightly positive impact. Regardless of whether they influence preferences positively or negatively however, when paired with a governance mechanism (in Model Two), most risks have less of an influence on respondents’ decision-making. In Model One, the risk with the greatest negative impact on peoples’ preferences for AI use is the risk that peoples’ personal information could be stolen, with older people and female respondents being slightly more averse to this risk than younger people and male respondents respectively. This concern is reflected in other areas of the survey; 57% of the public express concerns that data will not be held securely and could be hacked or stolen. The risk of bias against certain groups also has a slight negative impact, with that impact being slightly stronger among female respondents compared with male respondents. Meanwhile, the risks that people won’t know whether AI is being used, and that it will be difficult to understand how the technology makes decisions, have a slight positive impact on peoples’ likelihood of picking a scenario. As stated, the impact of risks (whether positive or negative) on preference for AI scenarios is reduced in Model Two, in which the risk is presented alongside a governance mechanism. The changes in the impact of each risk from Model One to Model Two are charted in Figure 17. For example, in Model One, the risk ‘people’s personal information could be stolen’ has a moderate negative impact. In Model Two, where this risk is combined with assurance that steps will be taken to ensure that personal information is safe and secure, the feature’s negative impact reduces. The sole exception to the governance mechanism reducing the impact of the risk can be seen in the risk/governance mechanism pairing ‘It will be difficult to know who is responsible if a mistake happens/a human is always responsible for the decisions made’. In this instance, adding the governance mechanism results in the feature going from having no impact to having a slightly positive impact on peoples’ preferences. Figure 17: Impact of risk and an associated governance mechanism on preferences for use of AI by Model (Showing % change in proportion of times the option is selected) CONJOINTQ. Which of these scenarios is your preference for how AI is used? BASE: All online respondents August 2023 (Wave 3) who saw each Model, Model 1 n=2119, Model 2 n=2106 9. Attitudes of those with very low digital familiarity 9.1 Introduction and summary In addition to a representative sample of UK adults, the tracker survey engages with individuals who have very low digital familiarity through telephone interviews. Interviews with those with very low digital familiarity enable us to ensure that the attitudes and experiences across the full spectrum of digital engagement are captured. We define people as having very low digital familiarity if they either agreed with three out of the five statements below, or said that they did not perform the activities in question: I don’t tend to use email I don’t feel comfortable doing tasks such as online banking I feel more comfortable shopping in person than online I find using online devices such as smartphones difficult I usually get help from family and friends when it comes to the internet Further information regarding the design of the very low digital familiarity sample can be found in the Methodology section. In summary, those with very low digital familiarity perceive they have less control over their personal data compared with the UK population overall, although this gap is gradually narrowing over time. This group increasingly believes organisations are held to account for data misuse and they recognise the positive impact data can have on society and personal benefits. Despite increased familiarity with AI, the majority of individuals with very low digital familiarity maintain a sceptical outlook on AI’s impact on society, largely expecting a negative or neutral outcome. 9.2 Perceptions of data control and accountability Those with very low digital familiarity feel less control over their data and are more likely to reject the notion that data use benefits all societal groups equally compared with the overall UK adult population. Approaching half of those with very low digital familiarity disagree they have control over who uses their data and how (49%), and that all groups in society benefit equally from data use (46%). Both figures are higher than the overall UK population’s views (40% and 32% respectively). In addition, about half of those with very low digital familiarity agree that data collection and analysis is good for society (51%) and that the use of data for creating products or services that benefit them (47%). Some interesting changes are seen over time. The share of those with very low digital familiarity who agree they have control over who uses their data and how has increased since Wave 2 (from 19% in Wave 2 to 35% in Wave 3). Furthermore, this group increasingly agrees that organisations are held accountable for misuse of data (from 38% in Wave 2 to 54% in Wave 3). 9.3 Awareness and understanding of AI Familiarity with AI has risen among those with very low digital familiarity, as with the overall UK population. Just over three quarters (76%) of adults with very low digital familiarity have heard of AI in Wave 3, marking a significant increase since Wave 2 (64%). A third (33%) of those with very low digital familiarity could explain at least partially what AI is, but almost a quarter (24%) have never heard of it. This demonstrates that, despite the increased levels of familiarity with AI among those with very low digital familiarity, there is still a lack of awareness relative to the general population (in which 95% have heard of AI and 5% are unaware). Despite the overall increase in awareness of AI, only a small proportion of adults with very low digital familiarity think AI will have a positive impact on society. Most of those who are aware of AI associate this technology with either a negative (38%) or neutral (42%) societal impact. Perceptions of negative societal impact of AI have risen steeply from 3% in Wave 2 to 38% in Wave 3, while the proportion viewing AI’s impact as neutral has decreased (42% in Wave 3 compared with 68% in Wave 2). 10. Methodology The CDEI’s Public Attitudes to Data and AI Tracker Survey monitors public attitudes towards data and AI over time. This report summarises the third wave (Wave 3) of research and makes comparisons with the first and second wave (Wave 1 and Wave 2). The research uses a mixed-mode data collection approach comprising online interviews (Computer Assisted Web Interviews - CAWI) and a smaller telephone survey (Computer Assisted Telephone Interviews - CATI) to ensure that those low or no digital skills are represented in the data. The Wave 1 CAWI survey ran among the general UK adult population (18+) from 29 November 2021 to 20 December 2021 with a total of 4,250 interviews collected in that time frame. A further 200 CATI interviews with the ‘very low digital familiarity’ sample were conducted between 15 December 2021 and 14 January 2022. The Wave 2 CAWI survey ran among the general UK adult population (18+) from 27 June 2022 to 18 July 2022 with a total of 4,320 interviews collected in that time frame. A further 200 CATI interviews with the ‘very low digital familiarity’ sample were conducted between 1 and 20 July 2022. The Wave 3 CAWI survey ran among the general UK adult population (18+) from 11 to 23 August 2023 with a total of 4,225 interviews collected in that time frame. A further 200 CATI interviews with the ‘very low digital familiarity’ sample were conducted between 15 August and 7 September 2023. Please note that there was a six-month interval between Wave 1 and Wave 2, but a 12-month interval between Wave 2 and Wave 3. Therefore, this report concentrates on the differences between the data from Wave 2 and Wave 3. We welcome any further feedback or questions on our approach at public-attitudes@cdei.gov.uk. 10.1 Sampling and Weighting Representative Online (CAWI) Sample Quotas have been applied to the online sample to ensure that it is representative of the UK adult population, based on age, gender, socio-economic grade, ethnicity, and region. In addition, interlocked quotas on age and ethnicity were used during fieldwork to monitor the spread of age across ethnic groups and ensure a balanced final sample. The online sample was provided by Cint. All the contact data provided is EU General Data Protection Regulation (GDPR) compliant. The online sample was weighted based on official statistics concerning age, gender, ethnicity, region, and socio-economic grade in the UK to correct any imbalances between the survey sample and the population to ensure it is nationally representative. Random Iterative Method (RIM) weighting was used to ensure that the final weighted sample matches the actual population profile. Where possible, the most up to date ONS UK population estimates have been used for both the fieldwork quotas and weighting scheme to ensure a nationally representative sample. 2021 mid-year population estimates were used for age, gender, and region, and the 2011 Census data for socio-economic groups (SEG). 2011 census figures were used for all countries as fieldwork began before the ONS released SEG figures for England and Wales that used the 2021 census. For ethnicity, we combined information from the 2021 Census data available for England, Wales, and Northern Ireland and the 2011 Census for Scotland. The online sample weighting used in Wave 3 was updated to reflect the most up to date population estimates and therefore differs slightly from the weighting scheme used in Wave 1 and Wave 2. Very low digital familiarity (CATI) Sample For Wave 3, 209 respondents with very low digital familiarity were contacted and interviewed via telephone. The named sample list of respondents’ contact details was provided by Datascope. All the contact data provided is GDPR compliant. This telephone sample captures the views of those who have low to no digital skills and are, therefore, likely to be excluded from online surveys. They are likely to be affected by digital issues in different ways to other groups. As the answers respondents give to questions may be impacted by how the question was delivered (e.g., by whether they saw it on a screen, or had it read to them over the phone), any comparisons drawn between the CATI and CAWI samples should be treated with caution. To select those with very low digital familiarity, we asked the below screening question in the Wave 3 telephone survey questionnaire. Respondents needed to agree to the statements or say they do not do the activities for three out of the five statements asked to qualify for the telephone interview. Statements I don’t tend to use email I don’t feel comfortable doing tasks such as online banking I feel more comfortable shopping in person than online I find using online devices such as smartphones difficult I usually get help from family and friends when it comes to using the internet The sample of those with very low digital familiarity was subject to fieldwork quotas and weighted to be representative of the digitally excluded population captured in FCA Financial Lives 2022 survey. The FCA Financial Lives 2022 survey was chosen as the basis to weight data rather than other similar datasets, including ONS Data and Lloyds Digital Skills 2022 report, as the FCA Financial Lives survey includes ethnicity breakdowns in data tables. We are aware that this group is slightly skewed towards ethnic minority (excluding White minority) adults, hence, the inclusion of ethnicity breakdown was of great importance. The quotas used were set on broader bands within key categories of gender, age, ethnicity, employment, UK nations, and regions of England. Random Iterative Method (RIM) weighting was used for this study, such that the final weighted sample matched the actual population profile. Those respondents who prefer not to answer questions on age and ethnicity are weighted to 1.0, and those who do not identify as male or female are also weighted to 1.0. This weighting of the very low digital familiarity (CATI) sample was used for the first time in Wave 2, Wave 1 data have not been weighted. Data from Wave 1 should therefore not be compared to data from Wave 3 and Wave 2. Demographic Profile of the Online (CAWI) sample & very low digital familiarity (CATI) sample The demographic profile of the online and very low digital familiarity (CATI) samples, before and after the weights have been applied, are provided in the following tables. Online (CAWI) sample Unweighted Unweighted Weighted Weighted Gender Female 2274 54% 2173 51% Male 1926 46% 2027 48% Identify in another way 15 <1% 15 <1% Prefer not to say 10 <1% 10 <1% Age NET: 18-34 1152 27% 1175 28% NET: 35-54 1382 33% 1391 33% NET: 55+ 1691 40% 1659 39% Socio-economic classification ABC1 2248 53% 2235 53% C2DE 1977 47% 1990 47% Region Northern Ireland 109 3% 116 3% Scotland 336 8% 351 8% North-West 477 11% 465 11% North-East 174 4% 167 4% Yorkshire & Humberside 362 9% 342 8% Wales 205 5% 199 5% West Midlands 375 9% 374 9% East Midlands 305 7% 307 7% South-West 366 9% 366 9% South-East 561 13% 581 14% Easter 385 9% 396 9% London 570 13% 561 13% NET: England 3575 85% 3559 84% Ethnicity NET: White 3342 79% 3568 84% NET: Mixed 175 4% 330 8% NET: Asian 383 9% 142 3% NET: Black 226 5% 71 2% NET: Other 99 1% 114 2% NET: Ethnic minority (excl. White minority) 883 20% 657 16% Telephone (CATI) sample Unweighted Unweighted Weighted Weighted Gender Female 133 64% 108 51% Male 76 36% 101 49% Age NET: 18-64 52 25% 70 33% NET: 65+ 146 70% 128 61% Region NET: England 160 77% 170 81% NET: Scotland, Wales, Northern Ireland 49 23% 39 19% Ethnicity NET: White 186 89% 168 81% NET: Ethnic minority (excl. White minority) 20 10% 38 18% Wave on wave comparability The same questions were asked in Waves 1, 2, and 3 of the tracker survey to enable comparison between the three time points. For Wave 3, as in Wave 2, we have replaced some questions from previous waves and added or removed some cases-studies and examples from other questions. In future waves we will rotate different items and questions into the survey at different intervals as annual data points are not required for all. Additionally, the question wording has been updated in some instances which are clearly marked in data tables with variable names marked ‘b’. The following notable edits were applied to CAWI and CATI survey questionnaires in Wave 3: New demographic question about highest educational level achieved to date in both the CAWI and CATI survey questionnaires. Q1 and Q14: New additions in Wave 3 include ‘HR and recruitment services’ and ‘Banks and other financial institutions’. Q15b and Q16: New additions in Wave 3 include ‘Cost of living’. ‘COVID-19’ has been excluded from the survey in Wave 3. New questions on AI and data regulation have been added for Wave 3. CAWI Conjoint: Change of conjoint design and content between Wave 2 and Wave 3. CATI: New question asking about the greatest risks for data use in society (Q17b). 10.2 Analysis The data from the CAWI survey has been analysed using a combination of descriptive, conjoint and regression analysis. The CATI data has been analysed using descriptive analysis only, due to its relatively smaller sample size (the conjoint module was not included in the CATI survey). Statistical significance and interpretation When interpreting the figures in this report, please note that only statistically significant differences (at a 95% confidence level) are reported and that the effect of weighting is considered when significance tests are conducted. Significant differences are highlighted in the analytical report and are relative to other directly relevant subgroups (e.g., those identifying as male vs. those identifying as female). Digital familiarity score A proxy score for digital familiarity has been used to break up respondents into three groups, using a score based on self-reported levels of confidence in using technology, and frequency of use of four digital services. The scores have been assigned as follows: Q4b: Respondents are given a score of 3 for each digital service used ‘a lot’, score of 1.5 for used ‘occasionally’, 0 for ‘don’t do at all’, with a maximum of 12 points on this question. Q5: Respondents are given points for every answer; ‘very confident’ = 12 points, ‘somewhat confident’ = 8, ‘not confident’ = 4, ‘not at all confident’ = 0, any other response = 0. The total maximum score one can have from Q4b and Q5 combined is 24. The distribution of scores was then analysed using Jenks method to identify logical divisions between the groups: Low digital familiarity: 0-12.5 (428 respondents) Medium digital familiarity: 13-19 (1630 respondents) High digital familiarity: 19.5-24 (2167 respondents) The CATI survey data has been treated as an extension to this means of dividing the data, providing a ‘very low digital familiarity’ group. Conjoint Analysis Conjoint analysis is a survey-based research approach for measuring the value that individuals place on different features and considerations for decision making. It works by asking respondents to directly compare different combinations of features to determine how they value each one. A conjoint experiment was created to test preferences for four attributes within AI use scenarios. Those attributes were: the AI application the benefits of using an AI approach the risks of using an AI approach Associated governance measures to minimise the risks The difference in responses allows the researcher to understand which attributes and items are driving preference. The experiment tested pairs of AI use scenarios (where the items within attributes varied) asking respondents to select in which scenario they would prefer for AI to be used. Respondents were asked 5 pairs of scenarios; each scenario being allocated based on achieving as even a distribution of combinations as possible across the sample. Each pair could contain some identical characteristics, but the pairs as a whole could not be identical. Even with this methodology not all combinations can feasibly be shown though, this problem is addressed in the analysis phase of the work. The data captured was analysed in Sawtooth using a combination of logistic regression and a hierarchical Bayesian (HB) algorithm. An individual respondent’s data was regressed to create utility scores; these scores can be considered to be the appeal of an attribute within the data sharing scenarios. These utility scores were then used to determine the likelihood of a respondent selecting an attribute or combination of attributes (the propositions displayed in the exercise). The HB algorithm analysed an individual respondent’s utilities for the data sharing scenarios they were shown, compared them to the sample average and then estimated their likely choices for scenarios not shown based on their variation from the sample average. The utility scores were transformed to show the likelihood that an AI usage scenario was selected, with a probability of 50% being the base probability, 100% being chosen every time, and 0% being never selected. For further reading on HB analysis please refer to the Sawtooth website. Regression analysis Regression analysis is used to test associations between different characteristics and responses, for example, to test for associations between demographic characteristics and attitudes toward data. This technique can identify the size and strength of these relationships, while holding all other variables in the model equal, but not cause and effect. Logistic regression is used to test for associations between a single ‘dependent’ variable and multiple ‘independent’ variables. Logistic regression is used as many of the ‘dependent’ variables in this report are survey questions based on Likert scales and not continuous data. Therefore, the data is transformed into a binary variable with two categories, for example ‘agreeing’ with a statement inclusive of ‘strongly agree’ and ‘somewhat agree’, and ‘neutral or not agreeing’ with the statement inclusive of all other responses. Logistic regression provides us with an ‘odds ratio’ (OR). This tells us the odds of someone with a particular characteristic or attitude reporting, for example, that they agree with a statement, compared with someone with another characteristic or attitude, after taking other possible influences into account. For example, regression analysis ran for Wave 3 of this tracker survey showed that non-graduates were less likely (OR = 0.79, meaning 0.79 times as likely) to think that collecting and analysing data is good for society, compared with graduates (Annex, Model 1). A goodness of fit measure, Akaike Information Criterion (AIC), is reported with all models. This can be used to compare models with the same dependent variable and understand which one is the best fit for the data where a smaller AIC indicates a better fit. AIC penalises models for including more variables, therefore, variables that are not found to be statistically significant are removed from the final models iteratively. We tested selected hypotheses using interactive effects on the type of data mentioned in the question and the individual’s demographic characteristics. Interaction effects occur in complex study areas when an independent variable interacts with another independent variable and its relationship with a dependent variable changes as a result. This effect on the dependent variable is non-additive, i.e., the joint effect of two variables interacting is significantly greater or significantly less than the sum of the parts. It is important to understand whether this effect occurs because it tells us how two or more independent variables work together to impact the dependent variable and ensure our interpretation of data is correct. The presence or absence of interactions among independent variables can be revealed with an interaction plot and interaction term effect can be included in an analytic model in order to quantify its significance. When we have statistically significant interaction effects, we have to interpret the main effects considering the interactions. 11. Annex 11.1 Overall attitudes to data Model 1: Collecting and analysing data is good for society Characteristic Odds ratio 95% Confidence level p-value Age (decade) 0.90 0.86, 0.93 <0.001 Education level Graduate – – Non-graduate 0.79 0.69, 0.91 <0.001 Socioeconomic grade ABC1 – – C2DE 0.87 0.76, 1.00 0.046 Ethnicity White – – Asian 1.01 0.80, 1.27 >0.9 Black 1.94 1.43, 2.65 <0.001 Mixed 1.06 0.77, 1.47 0.7 Other 0.82 0.48, 1.39 0.5 Prefer not to say 0.14 0.03, 0.42 0.002 AIC 5,457 Model 2: Data is useful for creating products and services that benefit me Characteristic Odds ratio 95% Confidence level p-value Age (decade) 0.90 0.87, 0.93 <0.001 Regions London – – East Midlands 1.02 0.77, 1.37 0.9 Eastern 1.11 0.85, 1.46 0.4 North-East 1.49 1.04, 2.15 0.029 North-West 1.02 0.79, 1.31 0.9 Northern Ireland 1.05 0.69, 1.61 0.8 Scotland 1.17 0.89, 1.55 0.3 South-East 1.09 0.86, 1.40 0.5 South-West 1.28 0.97, 1.69 0.080 Wales 1.23 0.88, 1.73 0.2 West Midlands 1.30 0.99, 1.71 0.064 Yorkshire & Humberside 1.30 0.99, 1.72 0.059 Socioeconomic grade ABC1 – – C2DE 0.73 0.65, 0.83 <0.001 AIC 5.569 11.2 Chatbot usage Model 8: For personal use, in your day-to-day life Characteristic Odds ratio 95% Confidence level p-value Gender Male – – Female 0.77 0.67, 0.90 <0.001 I identify in another way 0.31 0.07, 1.04 0.082 Prefer not to say 0.56 0.06, 4.21 0.6 Age (decade) 0.69 0.66, 0.73 <0.001 Regions London – – East Midlands 0.74 0.53, 1.05 0.090 Eastern 0.75 0.55, 1.04 0.085 North-East 0.65 0.42, 0.99 0.045 North-West 0.85 0.63, 1.13 0.3 Northern Ireland 0.76 0.47, 1.22 0.3 Scotland 0.74 0.53, 1.02 0.071 South-East 0.85 0.64, 1.14 0.3 South-West 0.82 0.59, 1.14 0.2 Wales 1.00 0.68, 1.47 >0.9 West Midlands 0.80 0.59, 1.08 0.15 Yorkshire & Humberside 1.07 0.78, 1.46 0.7 Education level Graduate – – Non-graduate 0.69 0.59, 0.81 <0.001 Ethnicity White – – Asian 1.27 0.98, 1.65 0.070 Black 1.80 1.30, 2.50 <0.001 Mixed 1.58 1.11, 2.25 0.010 Other 1.49 0.84, 2.66 0.2 Prefer not to say 1.25 0.44, 3.51 0.7 Awareness of AI Can explain AI – – Cannot explain AI 0.76 0.65, 0.89 <0.001 Trust in big tech companies Trust somewhat or a lot – – Do not trust much or at all 0.68 0.59, 0.79 <0.001 AIC 4,416 Model 9: For work or in your job Characteristic Odds ratio 95% Confidence level p-value Gender Male – – Female 0.63 0.53, 0.74 <0.001 I identify in another way 0.69 0.17, 2.33 0.6 Prefer not to say 0.14 0.01, 1.43 0.12 Age (decade) 0.57 0.54, 0.61 <0.001 Regions London – – East Midlands 0.58 0.39, 0.85 0.005 Eastern 0.45 0.30, 0.66 <0.001 North-East 0.55 0.33, 0.89 0.017 North-West 0.83 0.60, 1.14 0.2 Northern Ireland 0.94 0.57, 1.55 0.8 Scotland 0.51 0.35, 0.73 <0.001 South-East 0.67 0.48, 0.92 0.013 South-West 0.58 0.40, 0.85 0.005 Wales 0.79 0.50, 1.22 0.3 West Midlands 0.64 0.46, 0.90 0.010 Yorkshire & Humberside 0.87 0.61, 1.22 0.4 Education level Graduate – – Non-graduate 0.60 0.50, 0.71 <0.001 Ethnicity White – – Asian 1.56 1.19, 2.05 0.001 Black 1.88 1.35, 2.62 <0.001 Mixed 1.63 1.13, 2.34 0.009 Other 2.54 1.40, 4.71 0.003 Prefer not to say 0.34 0.08, 1.07 0.10 Trust in big tech companies Trust somewhat or a lot – – Do not trust much or at all 0.65 0.55, 0.78 <0.001 AIC 3,473 11.3 Attitudes on impact of AI in society Model 10: Perceived impact of Artificial Intelligence (AI) on society overall Characteristic Odds ratio 95% Confidence level p-value Gender Male – – Female 0.56 0.48, 0.65 <0.001 I identify in another way 0.29 0.08, 0.99 0.055 Prefer not to say 0.00 >0.9 Regions London – – East Midlands 0.64 0.45, 0.91 0.013 Eastern 0.90 0.64, 1.25 0.5 North-East 1.03 0.68, 1.58 0.9 North-West 0.74 0.54, 1.00 0.048 Northern Ireland 0.83 0.50, 1.37 0.5 Scotland 0.96 0.69, 1.34 0.8 South-East 0.87 0.65, 1.17 0.4 South-West 0.70 0.50, 0.98 0.038 Wales 1.10 0.74, 1.65 0.6 West Midlands 1.04 0.76, 1.44 0.8 Yorkshire & Humberside 0.82 0.60, 1.14 0.2 Education level Graduate – – Non-graduate 0.77 0.65, 0.90 <0.001 Ethnicity White – – Asian 1.59 1.21, 2.09 <0.001 Black 2.38 1.66, 3.46 <0.001 Mixed 1.36 0.95, 1.97 0.10 Other 0.63 0.33, 1.19 0.2 Prefer not to say 0.67 0.24, 1.82 0.4 Awareness of AI Can explain AI – – Cannot explain AI 0.50 0.43, 0.59 <0.001 AIC 4,072 11.4 Conjoint table Model 1 Model 2 Significance Level: 95% a b Total 2119 2106 Application: Artificial Intelligence (AI) would be used to… Mark student’s homework 33.2 34.2 Assess eligibility for welfare benefits 44.5 46.2 a Detect the presence of cancer from an x-ray scan 76.7 74.6 b Assess the risk of failing to repay a loan 32.8 35.5 a Screen CVs to select candidates for job 40.4 42.1 a Identify people who need financial support to pay their energy bills 61.4 58.3 b Benefits: This approach could be … than alternative approaches More accurate 49.3 49.9 a Faster 48.6 49.4 a Cheaper 47.6 48.3 a Less likely to unfairly discriminate 54.5 52.4 b Model 1: Risks / Model 2: Risks with associated governance: However, there may be a risk that… / but the system will take steps to ensure that… People’s personal information could be stolen / personal information is safe and secure 38.2 46.2 a It will be difficult to understand how the technology makes decisions / the reasons for its decisions can be explained 56.0 47.9 b The decision made by the technology will be difficult to challenge / people can choose to appeal the decisions that are made 52.3 51.4 b It will be difficult to know who is responsible if a mistake happens / a human is always responsible for the decisions made 50.8 56.8 a The technology will be biased against certain groups / bias is identified and reduced 43.3 44.7 a People won’t know whether Artificial Intelligence is being used / it is made clear when Artificial Intelligence is being used 59.8 53.0 b Overview Application 60.9% 63.5% a Benefits 14.9% 12.7% b Risks 24.1% 23.8% NB: Where scores have been identified as significantly different (represented by a bold letter: a or b), this indicates a statistically significant difference compared to the other Model at 95% confidence level. Throughout the report, only differences that are statistically significant (at a 95% confidence level) are reported. Due to the large sample size, even small differences can be statistically significant. ↩ In this report, we categorise respondents into low, medium, and high digital familiarity groups based on a proxy score derived from their self-reported confidence in using technology and frequency of using four digital services. ↩ It is important to note that awareness and understanding of AI was self-reported and should therefore not be treated as an objective measure. ↩ Positive impact is defined as those who answer 8-10 at Q23b, while negative impact is defined as those who answer 0-3 at Q23b. Q23b is as follows: ‘On a scale from 0-10 where 0 = very negative impact and 10 = very positive impact, based on your current knowledge and understanding, what impact do you think Artificial Intelligence (AI) will have overall on society?’ ↩ Back to top Is this page useful? Maybe Yes this page is useful No this page is not useful Thank you for your feedback Report a problem with this page Help us improve GOV.UK Don’t include personal or financial information like your National Insurance number or credit card details. This field is for robots only. Please leave blank What were you doing? What went wrong? Send Cancel Help us improve GOV.UK To help us improve GOV.UK, we’d like to know more about your visit today. Please fill in this survey (opens in a new tab). Cancel Services and information Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions Government activity Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved Support links Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service All content is available under the Open Government Licence v3.0, except where otherwise stated © Crown copyright"
"
		Analyzing University Students’ Attitude and Behavior Toward AI Using the Extended Unified Theory of Acceptance and Use of Technology Model
							| American Journal of Applied Statistics and Economics
			",https://journals.e-palli.com/home/index.php/ajase/article/view/2510,未知,2024-11-21 17:04:01,en-US,AI AND attitude,google,"Analyzing University Students’ Attitude and Behavior Toward AI Using the Extended Unified Theory of Acceptance and Use of Technology Model | American Journal of Applied Statistics and Economics Skip to main content Skip to main navigation menu Skip to site footer Open Menu American Journal of Applied Statistics and Economics Home About About the Journal Call For Paper Author's Guideline Publication Ethics Privacy Statement Copyright and Licensing Review Policy Open Access Policy Archiving Policy Contact Editorial Team Editorial Team Join the Editorial Team Submissions Submissions Submission Guidelines Manuscript Template Issues Current Archives APC Search E-PalliPublishers Home About us About E-Palli List of Journals Article Processing Charges (APC) Authors Guidelines Publication Ethics Privacy Statement Journals Engineering & Technology Engineering & Technology American Journal of Agricultural Science, Engineering, and Technology (AJASET) American Journal of Smart Technology and Solutions (AJSTS) American Journal of IR 4.0 and Beyond (AJIRB) American Journal of Innovation in Science and Engineering (AJISE) American Journal of Geospatial Technology (AJGT) American Journal of Education and Technology (AJET) American Journal of Food Science and Technology (AJFST) International Journal of Metaverse (IJM) Agricultural Science Agricultural Science American Journal of Agricultural Science, Engineering, and Technology (AJASET) American Journal of Aquaculture and Animal Science (AJAAS) American Journal of Life Science and Innovation (AJLSI) American Journal of Food Science and Technology (AJFST) American Journal of Bioscience and Bioinformatics (AJBB) American Journal of Plant Pathology (AJPP) International Journal of Precision Farming (IJPF) Environment & Climate Environment & Climate American Journal of Geospatial Technology (AJGT) American Journal of Environment and Climate (AJEC) American Journal of Energy and Natural Resources (AJENR) American Journal of Environmental Economics (AJEE) International Journal of Sustainable Rural Development (IJSRD) American Journal of Environmental Experience Design (AJEXD) International Journal of Forestry and Ecosystem (IJFE) Journal of Wildlife Conservation (JWC) Journal of Anthropology and Development (JAD) Business & Economics Business & Economics American Journal of Economics and Business Innovation (AJEBI) American Journal of Applied Statistics and Economics (AJASE) American Journal of Environmental Economics (AJEE) American Journal of Financial Technology and Innovation (AJFTI) Arts & Social Science Arts & Social Science American Journal of Social Development and Entrepreneurship (AJSDE) American Journal of Arts and Human Science (AJAHS) American Journal of Youth and Women Empowerment (AJYWE) American Journal of Society and Law (AJSL) American Journal of Applied Statistics and Economics (AJASE) American Journal of Education and Technology (AJET) American Journal of Development Studies (AJDS) American Journal of Tourism and Hospitality (AJTH) Journal of Arts and Natural Science (JANS) Journal of Political Science and International Relationship (JPSIR) Journal of Natural Language and Linguistics (JNLL) Journal of International Relations and Peace (JIRP) History and Cultural Innovation (HCI) Journal of Student and Education (JSE) Journal of Rural Sociology, Microfinance, and Poverty Studies (JRSMPS) Multidisciplinary Multidisciplinary American Journal of Multidisciplinary Research and Innovation (AJMRI) American Journal of Interdisciplinary Research and Innovation (AJIRI) American Journal of Applied Statistics and Economics (AJASE) Applied Research and Innovation (ARI) Journal of Innovative Research (JIR) Medical Science & Others Medical Science & Others American Journal of Medical Science and Innovation (AJMSI) American Journal of Applied Statistics and Economics (AJASE) American Journal of Chemistry and Pharmacy (AJCP) American Journal of Physical Education and Health Science (AJPEHS) American Journal of Human Psychology (AJHP) International Journal of Veterinary Medicine and Animal Science (IJVMAS) International Journal of Rural and Urban Development (IJRUD) Journal of Insect and Pest Management (JIPM) Journal of Policy and Planning (JPP) Submission Webinar Contact Register Login Home / Archives / Vol. 3 No. 1 (2024): American Journal of Applied Statistics and Economics / Research Articles Analyzing University Students’ Attitude and Behavior Toward AI Using the Extended Unified Theory of Acceptance and Use of Technology Model Authors Brandon Nacua Obenza University of Mindanao, Davao City, Philippines https://orcid.org/0000-0001-6893-1782 John Harry S. Caballo University of Mindanao, Davao City, Philippines Ria Bianca R. Caangay Ateneo De Davao University, Davao City, Philippines Trisha Eunice C. Makigod University of Mindanao, Davao City, Philippines Sharldawn M. Almocera University of Mindanao, Davao City, Philippines John Lawrence M. Bayno University of Mindanao, Davao City, Philippines Joseph Jr. R. Camposano University of Mindanao, Davao City, Philippines Sandy Jean G. Cena University of Mindanao, Davao City, Philippines Judy Ann Kyll Garcia University of Mindanao, Davao City, Philippines Bea Faye M. Labajo University of Mindanao, Davao City, Philippines Athena Grace Tua University of Mindanao, Davao City, Philippines DOI: https://doi.org/10.54536/ajase.v3i1.2510 Keywords: Attitude Toward Artificial Intelligence, Behavioral Toward AI, UTAUT Model, Partial Least Square Structural Equation Modeling (PLS-SEM), Philippines Abstract This quantitative study using Partial Least Square Structural Equation Modeling (PLS-SEM) examined a structural model of the attitudes and behaviors of university students toward AI in higher education. The results obtained using SmartPLS 4.0 indicate that the constructs exhibit validity and reliability (λ ≥ 0.708, α=0.767-0.948, AVE=0.584-0.777, HTMT=< 3.3). Further, the analysis of the hypothesized extended Unified Theory of Acceptance and Use (UTAUT) model reveals that AI Awareness significantly impacts Attitude toward AI (β = 0.156, p = 0.003) and Behavioral Intention to Use AI (BIU) (β = 0.337, p < 0.001). AI Trust also significantly influences Attitude toward AI (β = 0.366, p < 0.001) and BIU-AI (β = 0.173, p = 0.007). Additionally, Attitude toward AI is a strong predictor of BIU-AI (β = 0.457, p < 0.001). Social Influence significantly affects Attitude toward AI (β = 0.21, p < 0.001), while Effort Expectancy and Performance Expectancy do not show significant effects in this context. The link between Facilitating Conditions and BIU-AI is also insignificant. The model explained a substantial portion of the variance in attitude (R2 =0.612) and behavior (R2 =0.710). Fit indices indicate good model fit, and predictive relevance metrics were satisfactory. Downloads References Alam, A. (2021, December). Should robots replace teachers? Mobilisation of AI and learning analytics in education. In 2021 International Conference on Advances in Computing, Communication, and Control (ICAC3) (pp. 1-12). IEEE. https://doi.org/10.1109/ICAC353642.2021.9697300. Alzahrani, L. (2023). Analyzing students’ attitudes and behavior toward artificial intelligence technologies in higher education. International Journal of Recent Technology and Engineering (IJRTE), 11(6), 65-73. Bagozzi, R. P., & Yi, Y. (1988). On the evaluation of structural equation models. Journal of the Academy of Marketing Science, 16, 74–94. Bagozzi, R. P., & Yi, Y. (1988). On the evaluation of structural equation models. Journal of the Academy of Marketing Science, 16, 74–94. Barton, D., Woetzel, J., Seong, J., & Tian, Q. (2017). Artificial intelligence: Implications for China (Discussion Paper). Retrieved from McKinsey&Company website: http://dln.jaipuria.ac.in:8080/jspui/bitstream/123456789/1888/1/MGI-Artificial-intelligence-implications-for-China.pdf Becker, J., Ringle, C. M., Sarstedt, M., & Völckner, F. (2014). How collinearity affects mixture regression results. Marketing Letters, 26(4), 643–659. https://doi.org/10.1007/s11002-014-9299-9 Beig, S., & Qasim, S. H. (2023). Attitude towards artificial intelligence: Change in the educational era. International Journal of Creative Research Thoughts, 11(8), 718–b721. http://ijcrt.org/viewfull.php?&p_id=IJCRT2308192 Berdiyorova, I., Akhtamova, P., & Ganiev, I.M. (2021). Artificial intelligence in various industries. Proceedings from International scientific-practical conference, 2021 March 25-26 (pp. 186-193). Chatterjee, S., & Bhattacharjee, K. K. (2020). Adoption of artificial intelligence in higher education: a quantitative analysis using structural equation modeling. Education and Information Technologies, 25(5), 3443–3463. https://doi.org/10.1007/s10639-020-10159-7 Chen, M. Siu-Yung, M., Chai, C.S., Zheng, C., & Park, M.Y. (2021). A pilot study of students’ behavioral intention to use AI for language learning in higher education. Proceedings of International Symposium on Educational Technology (ISET) Tokai Nagoya Japan (pp. 182-184). https://doi.org/10.1109/ISET52350.2021.00045. Choung, H., David, P., & Ross, A. (2022). Trust in AI and its role in the acceptance of AI technologies. International Journal of Human-Computer Interaction, 39(9), 1727–1739. https://doi.org/10.1080/10447318.2022.2050543 Cockburn, I.M., Henderson, R., & Stern, S. (2018). The impact of artificial intelligence on innovation (Working Paper 24449). https://www.nber.org/system/files/working_papers/w24449/w24449.pdf Cook, J. (2023, November 21). How can we make AI adoption more human? https://www.linkedin.com/pulse/how-can-we-make-ai-adoption-more-human-jo-cook-zl0sf/ Creswell, J. W., & Creswell, J. D. (2023). Research design: “Qualitative, Quantitative, and Mixed Methods Approaches” (6th ed.). SAGE Publications. Crompton, H., & Burke, D. (2023). Artificial intelligence in higher education: the state of the field. International Journal of Educational Technology in Higher Education, 20 (22). https://doi.org/10.1186/s41239-023-00392-8 Emon, M. M. H., Hassan, F., Nahid, M. H., & Rattanawiboonsom, V. (2023). Predicting Adoption Intention of Artificial Intelligence CHATGPT. The AIUB Journal of Science and Engineering, 22(2), 189–199. https://doi.org/10.53799/ajse.v22i2.797 Farhi, F., Jeljeli, R., Aburezeq, I., Dweikat, F.F., Al-shami, S.A. & Slamene, R. (2023). Analyzing the students’ views, concerns, and perceived ethics about chat GPT usage. Computers and Education: Artificial Intelligence, 5, 1-8. https://doi.org/10.1016/j.caeai.2023.100180. Fornell CG and Larcker DF. (1981) Evaluating Structural Equation Models with Unobservable Variables and Measurement Error. Journal of Marketing Research 18(1), 39-50. Gado, S., Kempen, R., Lingelbach, K., & Bipp, T. (2022). Artificial intelligence in psychology: How can we enable psychology students to accept and use artificial intelligence? Psychology Learning & Teaching, 21(1), 37-56. https://doi.org/10.1177/14757257211037149 Geetha, R. & BhanuSree Reddy, D. (2018). Recruitment through artificial intelligence: A conceptual study. International Journal of Mechanical Engineering and Technology, 9(7), 63–70. http://www.iaeme.com/IJMET/issues.asp?JType=IJMET&VType=9&IType=7 Ghotbi, N., Ho, T., & Mantello, P. (2022). The attitude of college students towards ethical issues of artificial intelligence in an international university in Japan. AI & Society, 37(2), 1-8. https://doi.org/10.1007/s00146-021-01168-2 Gold, A. H., & Malhotra, A. H. (2001). Title of the article. Journal of Management Information Systems, 18, 185-214. Hair JF, Hult GTM, Ringle CM, et al. (2017a) A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM), Thousand Oaks, CA: Sage. Hair, J. F., Risher, J. J., Sarstedt, M., & Ringle, C. M. (2019). When to use and how to report the results of PLS-SEM. European Business Review, 31(1), 2–24. https://doi.org//10.1108/EBR-11-2018-0203 Hair, J., Hult, G. T. M., Ringle, C., & Sarstedt, M. (2014). A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM). Los Angeles: SAGE Publications, Incorporated. Hair, J., Hult, G. T. M., Ringle, C., & Sarstedt, M. (2014). A Primer on Partial Least Squares Structural Equation Modeling (PLS-SEM). Los Angeles: SAGE Publications, Incorporated. Hall, J. & Pesenti, J. (2017). Growing the artificial intelligence industry in the UK. GOV.UK. https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk Hamid, M. R. A., Sami, W., & Sidek, M. H. M. (2017). Discriminant Validity Assessment: Use of Fornell & Larcker criterion versus HTMT Criterion. Journal of Physics: Conference Series, 890, 012163. https://doi.org/10.1088/1742-6596/890/1/012163 Hassani, H., Silva, E.S., Unger, S., Ta jMazinani, M., & Mac Feely, S. (2020). Artificial intelligence (AI) or intelligence augmentation (IA): What is the future? AI 2020, pp. 1, 143–155. https://doi.org/10.3390/ai1020008 Henseler, J., Ringle, C. M., & Sinkovics, R. R. (2009). The use of partial least squares path modeling in international marketing. Journal of the Academy of Marketing Science, 20, 227–319. Hilale, N. (2021). The evolution of artificial intelligence (AI) and its impact on women: how it nurtures discrimination towards women and strengthens gender inequality. International Journal of Human Rights, 1(2), 141–150. http://www.humanrights.periodikos.com.br/article/61489565a9539526b5418543 Hulland, J. (1999). Title of the Article. Strategic Management Journal, pp. 20, 195–204. Humble, N., Mozelius, P. The threat, hype, and promise of artificial intelligence in education. Discover Artificial Intelligence, 2(22). https://doi.org/10.1007/s44163-022-00039-z Intiser, R., Nahid, M. H., Anwar, M. A., & Nahar, R. (2023). Adoption of AI-powered web-based English writing assistance software: An Exploratory Study. AIUB Journal of Business and Economics, 20(1), 90–101. https://ajbe.aiub.edu/index.php/ajbe/article/view/194 Isaac, O., Abdullah, Z., Ramayah, T. and Mutahar, A.M. (2017). “Internet usage, user satisfaction, task-technology fit, and performance impact among public sector employees in Yemen,” International Journal of Information and Learning Technology, 34(3), 210–241. https://doi.org/10.1108/IJILT-11-2016-0051 Jarrett, A, Choo, K.-K. (2021). The impact of automation and artificial intelligence on digital forensics. WIREs Forensic Science, pp. 1–17. https://doi.org/10.1002/wfs2.1418 Jindal, H., Kumar, D., Ishika, Kumar, S., & Kumar, R. (2021). Role of artificial intelligence in the distinct sector: a study. Asian Journal of Computer Science and Technology, 10(1), 18–28. https://doi.org/10.51983/ajcst-2021.10.1.2696 Kairu, C. (2020). Students’ attitude towards the use of artificial intelligence and machine learning to measure classroom engagement activities. In Association for the Advancement of Computing in Education (AACE) (Ed.). Proceedings of EdMedia + Innovate Learning, 23 June 2020 (pp. 793–802). https://www.learntechlib.org/primary/p/217382/. Kandoth, S. ., & Kushe Shekhar, S. . (2022). Social influence and intention to use AI: the role of personal innovativeness and perceived trust using the parallel mediation model. Forum Scientiae Oeconomia, 10(3), 131–150. https://doi.org/10.23762/FSO_VOL10_NO3_7 Kavitha, V., & Lohani, R. (2019). A critical study on the use of artificial intelligence, e-learning technology, and tools to enhance the learner’s experience. Cluster Computing, 22, 6985–6989. https://doi.org/10.1007/s10586-018-2017-2 Kaya, F., Aydin, F., Schepman, A., Rodway, P., Yetişensoy, O., & Demir Kaya, M. (2022). The roles of personality traits, AI anxiety, and demographic factors in attitudes towards artificial intelligence. International Journal of Human–Computer Interaction. https:// doi.org/10.1080/10447318.2022.2151730 Khanagar, S., Al-ehaideb A., Maganur, P., Vishwanathaiah, S., Patil, S., Baeshen, H., S., S., & Bhandi, S. (2021). Developments, application, and performance of artificial intelligence in dentistry – A systematic review. Journal of Dental Sciences, 16(1), 508–522. https://doi.org/10.1016/j.jds.2020.06.019. Kim, J. M. (2017). Study on intention and attitude of using artificial intelligence technology in healthcare. Journal of Convergence for Information Technology, 7(4), 53-60. https://doi.org/10.22156/CS4SMB.2017.7.4.053 Kline, R. B. (2011). Principles and Practice of Structural Equation Modeling (3rd ed.). New York: The Guilford Press. Kock N and Hadaya P. (2018). Minimum Sample Size Estimation in PLS-SEM: The Inverse Square Root and Gamma-Exponential Methods. Information Systems Journal 28(1), 227- 261. Kushmar, L.V., Vornachev, A.O Korobova.I.O., & Kaida,N.O. (2022). Artificial Intelligence in Language Learning: What Are We Afraid of? Arab World English Journal (AWEJ) Special Issue on CALL, (8), 262-273. https://dx.doi.org/10.24093/awej/call8.18 Liehner, G.L., Biermann, H., & Hick, A., Brauner, P. & Ziefle, M. (2023). Perceptions, attitudes, and trust towards artificial intelligence — an assessment of the public opinion. Artificial Intelligence and Social Computing, 72, 32–41. https://doi.org/10.54941/ahfe1003271 Lin, H.C., Ho, C.F., & Yang, H. (2021). Understanding the adoption of artificial intelligence-enabled language e-learning system: an empirical study of UTAUT model. Home International Journal of Mobile Learning and Organisation, 16(1), 79-94. https://www.inderscienceonline.com/doi/epdf/10.1504/IJMLO.2022.119966 Loble, L., Creenaune, T., & Hayes, J. (2017). Future frontiers education for an AI world. Melbourne University Press. Lu, H., Li, Y., Chen, M., Kim, Hyoungseop, K., & Serikawa, S. (2017). Brain intelligence: Go beyond artificial intelligence. Mobile Networks and Applications, 23, 368–375. https://doi.org/10.1007/s11036-017-0932-8 Marrone, R., Taddeo, V., & Hill, G. (2022). Creativity and artificial intelligence—a student perspective. Journal of Intelligence, 10(3), 1-11. https://doi.org/10.3390/jintelligence10030065 Mason, C. H., & Perreault, W. D. (1991). Collinearity, power, and interpretation of multiple regression analysis. Journal of Marketing Research, 28(3), 268–280. (PDF) How Collinearity Affects Mixture Regression Results. Mintz, Y. & Brodie, R. (2019). Introduction to artificial intelligence in medicine. Minimally Invasive Therapy & Allied Technologies, 28(2), 73-81. https://doi.org/10.1080/13645706.2019.1575882 Mohamed, S.S.A. & Alian, E.M.I. (2023). Students’ attitudes toward using a chatbot in EFL Learning. Arab World English Journal (AWEJ), 14(3), 15–27. https://dx.doi.org/10.24093/awej/vol14no3.2 Obenza, B. N., Salvahan, A., Rios, A. N., Solo, A., Alburo, R. A., & Gabila, R. J. (2023b). University Students’ Perception and Use of ChatGPT Generative Artificial Intelligence (AI) in Higher Education. International Journal of Human Computing Studies, 5(12), 5–18. https://doi.org/10.5281/zenodo.10360697 Obenza, B. N., Baguio, J. S. I. E., Bardago, K. M. W., Granado, L. B., Loreco, K. C. A., Matugas, L. P., Talaboc, D. J., Zayas, R. K. D. D., Caballo, J. H. S., & Caangay, R. B. R. (2023a). The Mediating Effect of AI Trust on AI Self-Efficacy and Attitude Toward AI of College Students. International Journal of Metaverse, 2(1), 1–10. https://doi.org/10.54536/ijm.v2i1.2286 Obenza, B. N., Go, L. E., Francisco, J. A. M., Buit, E. E. T., Mariano, F. V. B., Cuizon Jr, H. L., Cagabhion, A. J. D., & Agbulos, K. A. J. L. (2024). The Nexus between Cognitive Absorption and AI Literacy of College Students as Moderated by Sex. American Journal of Smart Technology and Solutions, 3(1), 32–39. https://doi.org/10.54536/ajsts.v3i1.2603 Olhede, S. C., & Wolfe, P. J. (2018). The AI Spring of 2018. Significance, 15(3), 6–7. https://doi.org/10.1111/j.1740-9713.2018.01140.x Pande, K., Sonawane, S., Jadhava, V., & Malia, M. (2023). Artificial intelligence: exploring the attitude of secondary students. Journal of e-learning and knowledge society, 19(3), 43-48. https://www.je-lks.org/ojs/index.php/Je-LKS_EN/article/view/1135865 Paul, D., Sanap, G., Shenoy, S., Kalyane, D., Kalia, K., & Tekade, R.K. (2021). Artificial intelligence in drug discovery and development. Drug Discov, 26(1), 80-93. doi: 10.1016/j.drudis.2020.10.010. Pedró, F., Subosa, M., Rivas, A., & Valverde, P. (2019). Artificial intelligence in education: challenges and opportunities for sustainable development. United Nations Educational, Scientific and Cultural Organization, 7, 1-48. https://unesdoc.unesco.org/ark:/48223/pf0000366994 Ringle CM, Wende S and Becker J-M. (2015) SmartPLS 3. Bönningstedt: SmartPLS. Romero-Rodriguez, J.M., Ramirez-Montoya, M.S., Buenestado-Fernández, M. & Lara-Lara, F. (2023). Use of ChatGPT at university as a tool for complex thinking: Students’ perceived usefulness. Journal of New Approaches in Education Research, 12(2), 323-339. https://doi.org/10.7821/naer.2023.7.1458 Roy, R., Babakerkhell, M.D., Mukherjee, S., Pal, D., & Funilkul, S. (2022). Evaluating the intention for the adoption of artificial intelligence-based robots in the university to educate the students. IEEE Access, 10, 125666-125678. doi: 10.1109/ACCESS.2022.3225555. Saravanan, K., Sreedevi, E., & Subhamathi, V. (2017). A Review of Artificial Intelligence Systems. International Journal of Advanced Research in Computer Science, 8(9), 418–421. DOI 10.26483/ijarcs.v8i9.5095 Sarstedt M, Ringle CM and Hair JF. (2017a) Partial Least Squares Structural Equation Modeling. In: Homburg C, Klarmann M and Vomberg A (eds) Handbook of Market Research. Heidelberg: Springer. Sarstedt M, Ringle CM, Cheah J-H, et al. (2019b). Structural Model Robustness Checks in PLS-SEM. Tourism Economics is forthcoming. Schepman, A. & Rodway, P. (2023). The general attitude towards artificial intelligence scale (GAAIS): Confirmatory validation and associations with personality, corporate distrust, and general trust. International Journal of Human–Computer Interaction, 39(13), 2724–2741. https://doi.org/10.1080/10447318.2022.2085400 Seo, K., Tang, J., Roll, I., Fels, S., & Yong, D. (2021). The impact of artificial intelligence on learner–instructor interaction in online learning. International Journal of Educational Technology in Higher Education, 18 (54). https://doi.org/10.1186/s41239-021-00292-9 Shao, Z., Yuan, S., & Wang, Y., (2020). Institutional collaboration and competition in artificial intelligence. IEEE Access, 8, 69734-69741. https://doi.org/10.1109/ACCESS.2020.2986383. Skeat, J. & Ziebell, N. (2023). University students are using AI, but not how you think. The University of Melbourne. https://pursuit.unimelb.edu.au/articles/university-students-are-using-ai-but-not-how-you-think Slavov, V., Yotovska, K. & Asenova, A. (2023, March 11-13). Research on the attitudes of high school students toward the application of artificial intelligence in education. 19th International Conference on Mobile Learning 2023, Lisbon Portugal. Suh, W., & Ahn, S. (2022). Development and validation of a scale measuring student attitudes toward artificial intelligence. SAGE Open, 12(2), 215824402211004. https://doi.org/10.1177/21582440221100463 Tahiru, F. (2021). AI in education: A systematic literature review. Journal of Cases on Information Technology (JCIT), 23(1), 1–20. DOI: 10.4018/JCIT.2021010101 Tan, L. & Ran, N. (2022). Applying artificial intelligence technology to analyze the athletes’ training under a sports training monitoring system. International Journal of Humanoid Robotics, 20(06). https://doi.org/ 10.1142/S0219843622500177 Venkatesh, M., Davis, & Davis (2003). User Acceptance of Information Technology: Toward a Unified View. MIS Quarterly, 27(3), 425. Venkatesh, V. & Davis, F.D. (2000). A Theoretical Extension of the Technology Acceptance Model: Four Longitudinal Field Studies. Management Science, 46(2), 186–204. Venkatesh, V., Thong, J. & Xu, X. (2016). Unified Theory of Acceptance and Use of Technology: A Synthesis and the Road Ahead. Journal of the Association for Information Systems, 17(5), 328-376. Welding, L. (2023). Half of college students say using AI on schoolwork is cheating or plagiarism. BestColleges. https://www.bestcolleges.com/research/college-students-ai-tools-survey/ Wold HOA. (1982) Soft Modeling: The Basic Design and Some Extensions. In: Jöreskog KG and Wold HOA (eds) Systems Under Indirect Observations: Part II. Amsterdam: North- Holland, 1–54. Xie, X. & Wang, T. (2023). Artificial intelligence: A help or threat to contemporary education. Should students be forced to think and do their tasks independently? Education and Information Technologies. https://doi.org/10.1007/s10639-023-11947-7 Yadrovskaia, M., Porksheyan, M., Petrova, A., Dudukalova, D., & Bulygin, Y. (2023). About the attitude towards artificial intelligence technologies. E3S Web of Conferences, 376, 05025. https://doi.org/10.1051/e3sconf/202337605025 Zawacki-Richter, O., Marín, V.I., Bond, M. & Gouverneur, F. (2019). Systematic review of research on artificial intelligence applications in higher education – where are the educators? International Journal of Educational Technology in Higher Education, 16 (39). https://doi.org/10.1186/s41239-019-0171-0 Zhang, K. & Aslan, A. B. (2021). AI technologies for education: Recent research & future directions. Computers and Education: Artificial Intelligence, 2. https://doi.org/10.1016/j.caeai.2021.100025. Zhou, Z., Chen, X., Li E., Zeng, L., Luo, K., & Zhang, J. (2019). Edge intelligence: Paving the last mile of artificial intelligence with edge computing. Proceedings of the IEEE, 107(8), 1738-1762. https://doi.org/10.1109/JPROC.2019.2918951 Downloads Download PDF 👁 624 Published 2024-05-13 How to Cite Obenza, B. N., Caballo, J. H. S., Caangay, R. B. R., Makigod, T. E. C., Almocera, S. M., Bayno, J. L. M., Camposano, J. J. R., Cena, S. J. G., Garcia, J. A. K., Labajo, B. F. M., & Tua, A. G. (2024). Analyzing University Students’ Attitude and Behavior Toward AI Using the Extended Unified Theory of Acceptance and Use of Technology Model. American Journal of Applied Statistics and Economics, 3(1), 99–108. https://doi.org/10.54536/ajase.v3i1.2510 More Citation Formats ACM ACS APA ABNT Chicago Harvard IEEE MLA Turabian Vancouver Download Citation Endnote/Zotero/Mendeley (RIS) BibTeX Issue Vol. 3 No. 1 (2024): American Journal of Applied Statistics and Economics Section Research Articles License Copyright (c) 2024 Brandon Nacua Obenza, John Harry S. Caballo, Ria Bianca R. Caangay, Trisha Eunice C. Makigod, Sharldawn M. Almocera, John Lawrence M. Bayno, Joseph Jr. R. Camposano, Sandy Jean G. Cena, Judy Ann Kyll Garcia, Bea Faye M. Labajo, Athena Grace Tua This work is licensed under a Creative Commons Attribution 4.0 International License. Subscribe for newsletter Make a Submission Make a Submission manuscript-template Manuscript Template Information For Readers For Authors For Librarians Keywords indexing Indexed in Facebook Connect on Facebook Current Issue E-Palli Limited Liability Company (LLC) is registered in Delaware, United States of America, provides publication solutions for journals, conference proceedings, books, and special issues. E-palli publishes articles in a number of double-blind peer-reviewed international journals that emphasize research, development, application, and innovation. This OJS site and its metadata are licensed under a Creative Commons Attribution 4.0 International Licence E-Palli Home Article Processing Charges About us Contact us Authors Guideline List of New Journals List of Journals List of Indexing Privacy Statement Publication Ethics Copyright © 2024, E-Palli LLC"
Insights 2024 | Attitudes toward AI | Elsevier ,https://www.elsevier.com/insights/attitudes-toward-ai,未知,2024-11-21 17:04:03,en-us,AI AND attitude,google,"Insights 2024 | Attitudes toward AI | Elsevier Skip to main contentUnfortunately we don't fully support your browser. If you have the option to, please upgrade to a newer version or use Mozilla Firefox, Microsoft Edge, Google Chrome, or Safari 14 or newer. If you are unable to, and need support, please send us your feedback.We'd appreciate your feedback.Tell us what you think! opens in new tab/windowAcademic & GovernmentAcademic & GovernmentHealthHealthIndustryIndustryElsevier ConnectInsightsAboutAboutCustomer supportSupportPublish with usOpen SearchLocation SelectorShow MenuHomeInsightsAttitudes toward AIInsights 2024: Attitudes toward AIDiscover what researchers and clinicians around the world think about the use of AI in their work.Read the full report(opens in new tab/window)The reportKey findingsFocus on CliniciansFocus on ResearchersFocus on IndustryMore informationSlideSlideThe rapid evolution of generative artificial intelligence (GenAI) has brought with it a wave of expectations, opportunities and concerns in all sectors. In research and health, the promise of accelerated discovery, increased output and improved patient care is matched by an expressed need for transparency, trust and quality content. Our report Insights 2024: Attitudes toward AI brings together the views of nearly 3,000 researchers and healthcare professionals around the world. Their feedback reveals a clear appetite for adopting AI tools in their work, but also shows differing attitudes among respondents from the world’s top three research-generating countries, the US, China and India.Read the full report(opens in new tab/window)Attitudes on AI: Key findingsClinicians and researchers believe AI tools can helpResearchers and clinicians recognize the growing potential of AI tools, and if they’re not already using them, most expect to do so in the coming two to five years.94% of researchers and 96% of clinicians think AI will help accelerate knowledge discovery87% think it will help increase work quality overall85% of both groups believe AI will help free up time to focus on higher value projectsThe importance of trusted content Respondents were clear that if the benefits of AI tools were to be realized, the tools themselves must be based on high quality, trusted content.71% expect generative AI dependent tools’ results be based on high quality trusted sources onlyIf AI tools are backed by trusted content, quality controls and responsible AI principles, 89% of researchers would use such tools to generate a synthesis of articles, while 94% of clinicians said they would employ AI to assess symptoms and identify conditions or diseasesResearchers and clinicians are wary of misinformationIf organizations are to benefit from AI tools, they will need to understand the factors that build researchers’ and clinicians’ trust in AI tools, and their comfort using them.95% of researchers along with 93% of clinicians believe AI will be used for misinformation86% of researchers and 85% of clinicians believe AI will cause critical errors, while a similar ratio expressed concern about AI leading to weakened critical thinking79% of clinicians and 80% of researchers believe AI will cause disruption to societyView the key findings opens in new tab/windowAI insights at a glanceView the infographic opens in new tab/windowClinician attitudes toward AIIn the latest of our annual reports looking at the future of healthcare, clinicians have growing optimism about the capability of AI to increase their productivity, freeing up their time for high value work. However, this is accompanied by caution about AI’s impact on critical thinking and clinical decision making.Discover Clinician key findings(opens in new tab/window)Clinician of the Future: AI editionKey insights:26% of clinicians have used AI for work purposes96% of clinicians believe AI will help accelerate knowledge discovery88% of clinicians believe AI will help increase work quality overall82% of physicians believe AI has the potential to cause critical errors or mishapsView the clinician infographic opens in new tab/windowResearcher attitudes toward AIIn this focused analysis of attitudes to AI in the world of research, respondents expect AI to have an increased impact on their work, enabling them to find information more quickly and to increase scholarly output. But that expectation is tempered by caution over the integrity of information and a demand for transparency in how AI is used. Discover Researcher key findings(opens in new tab/window)What do researchers have to say?Key insights:37% of researchers have used AI for work purposes94% of researchers believe AI will help accelerate knowledge discovery81% of researchers believe AI has the potential to erode critical thinking skillsView the researcher infographic opens in new tab/windowCorporate researcher attitudes toward AIIn this segment we focus on corporate researchers' attitudes toward AI, including generative AI (GenAI), we cover its attractiveness, perceived impact, the benefits to them and wider society, the degree of transparency necessary to be comfortable using tools that capitalize on the technology, and the challenges they see with AI.Discover corporate researcher key findings(opens in new tab/window)What do corporate researchers have to say?Key insights:96% believe it will help accelerate knowledge discovery95% believe it will help increase their work efficiency71% believe AI (including GenAI) will have a transformative or significant impact on their area of work58% of those aware of AI have used it and 38% have used it for work purposesView the corporate researcher infographic opens in new tab/windowMore informationAt Elsevier, we bring together trusted content, human expertise and responsibly applied AI technologies to help researchers, educators and healthcare professionals worldwide advance discovery, innovation and patient care.Learn more about our approach and AI toolsDatabooks available for downloadAttitudes toward AI databook opens in new tab/windowResearcher: Attitudes toward AI databook opens in new tab/windowClinician of the Future: Attitudes toward AI databook opens in new tab/windowUseful linksSubmit your paperShop Books & JournalsOpen accessView all productsElsevier ConnectAboutAbout ElsevierCareersGlobal Press OfficeAdvertising, reprints & supplementsModern slavery act statementSupportCustomer supportResource centerGlobal | English LinkedIn opens in new tab/window Twitter opens in new tab/window Facebook opens in new tab/window YouTube opens in new tab/window opens in new tab/window opens in new tab/windowCopyright © 2024 Elsevier, its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies.Terms & ConditionsPrivacy policyAccessibilityCookie settings"
"
		How do people feel about AI? | Ada Lovelace Institute	",https://www.adalovelaceinstitute.org/report/public-attitudes-ai/,未知,2024-11-21 17:04:14,en-GB,AI AND attitude,google,"How do people feel about AI? | Ada Lovelace Institute Skip to content Ada Lovelace Institute About Our work Ada in Europe Blog News & events Search for content: Home / Our work / Library Report How do people feel about AI? A nationally representative survey of public attitudes to artificial intelligence in Britain Roshni Modhvadia 6 June 2023 Reading time: 97 minutes How do people feel about AI? (opens in new tab) (PDF 2 MB) Contributing authors Anna Colom Andrew Strait Octavia Field Reid (formerly Reeve) Aidan Peppin Project How do people feel about AI? Research domain Public Participation & Research Keywords Public attitudes Public trust Diversity Chapters Print this page Share this page Browse the findings on our dedicated microsite 'Attitudes to AI' Executive summary 1. How to read this report 2. Introduction 3. Methodology Sample Survey Analysis 4. Key findings 4.1. Awareness and experience of AI uses 4.2. How beneficial do people think AI technologies are, and how concerning? 4.3. Specific benefits and concerns around different AI uses 4.4. Governance and explainability 5. Conclusion 6. Appendix 6.1. Descriptions for each technology use case 6.2. Limitations 6.3. Analysis and additional tables 6.4. Sample sizes Partner information and acknowledgements About The Alan Turing Institute Footnotes Browse the findings on our dedicated microsite 'Attitudes to AI' 'Attitudes to AI' highlights key findings from 'How do people feel about AI?', with interactive charts and bite-size content. Attitudes to AI Executive summary Artificial intelligence (AI) technologies already interact with many aspects of people’s lives. Their rapid development has resulted in increased national attention on AI and surrounding policy. In November 2022, the Ada Lovelace Institute and The Alan Turing Institute conducted a nationally representative survey of over 4,000 adults in Britain, to understand how the public currently experience AI. We asked people about their awareness of, experience with and attitudes towards different uses of AI. This included asking people what they believe are the key advantages and disadvantages, and how they would like to see these technologies regulated and governed. While the term AI appears frequently in public discourse, it can be difficult to define and is often poorly understood, particularly as it encompasses a wide range of technologies that are used in different contexts and for distinct purposes. There is no single definition of AI, and the public may see the term applied in a wide variety of settings. Making matters even more challenging is the fast pace of AI development. OpenAI’s ChatGPT was released two weeks after we began our fieldwork. The widespread media coverage of generative AI – AI that can generate content such as images, videos, audio and text – has probably already impacted public discourse, and this survey therefore reflects the attitudes of the British public before the surge of interest in this topic. The multifaceted and continually evolving nature of AI can present a challenge for public attitudes research, as it can be difficult to ask people meaningfully how they feel about a complex topic which may evoke different interpretations. Taking this into account, we focused on asking people about specific technologies that make use of AI and we gave people clear descriptions of each. We asked the British public about their attitudes towards and experiences with 17 different uses of AI. These uses ranged from applications that are visible and commonplace, such as facial recognition for unlocking mobile phones and targeted advertising on social media; to those which are less visible, such as assessing eligibility for jobs or welfare benefits; and applications often associated with more futuristic visions of AI, such as driverless cars and robotic care assistants. For each specific use of AI, people were given the opportunity to express their perceptions of the benefits and their concerns about the technology, recognising that people may see potential benefit and concern simultaneously. We also offered people the chance to tell us how they thought each technology might yield both benefits and risks. Additionally, respondents were asked more general questions about their preferences for AI governance and regulation, including how explainable they would like AI decision-making to be. Broadly, our findings highlight the complex and nuanced views that people in Britain have about the many different uses of AI across public and personal life. People’s awareness varies greatly across the different technologies we asked about, with the highest levels of awareness reported for everyday applications, such as facial recognition for unlocking mobile phones, and applications that are less commonplace but have received media attention, such as driverless cars. Public awareness is lowest for less visible technologies, such as AI for assessing eligibility for welfare or risk in healthcare outcomes. Key findings relating to public attitudes across these technologies are summarised below. Key findings: For the majority of AI uses that we asked about, people had broadly positive views, but expressed concerns about some uses. Many people think that several uses of AI are generally beneficial, particularly for technologies related to health, science and security. For 11 of the 17 AI uses we asked about, most people say they are somewhat or very beneficial. The use of AI for detecting the risk of cancer is seen as beneficial by nine in 10 people. The public also express concern over some uses of AI. For six of the 17 uses, over 50% find them somewhat or very concerning. People are most concerned about advanced robotics such as driverless cars (72%) and autonomous weapons (71%). People’s perceived benefit levels outweigh concerns for 10 of the 17 technologies, while concerns outweigh benefits for five of the 17. For two technologies, benefits and concerns are evenly balanced. Digging deeper into people’s perceptions of AI shows that the British public hold highly nuanced views on the specific advantages and disadvantages associated with different uses of AI. For example, while nine out of 10 British adults find the use of AI for cancer detection to be broadly beneficial, over half of British adults (56%) are concerned about relying too heavily on this technology rather than on professional judgements, and 47% are concerned about the difficulty in knowing who is responsible for mistakes when using this technology. People most commonly think that speed, efficiency and improving accessibility are the main advantages of AI across a range of uses. For example, 70% feel speeding up processing at border control is a benefit of facial recognition technology. However, people also note concerns relating to the potential for AI to replace professional judgements, not being able to account for individual circumstances, and a lack of transparency and accountability in decision-making. For example almost two-thirds (64%) are concerned that workplaces will rely too heavily on AI for recruitment compared to professional judgements. Additionally, for technologies like smart speakers and targeted social media advertisements, people are concerned about personal data being shared. Over half (57%) are concerned that smart speakers will gather personal information that could be shared with third parties while 68% are concerned about this for targeted social media adverts. The public want regulation of AI technologies, though this differs by age. The majority of people in Britain support regulation of AI. When asked what would make them more comfortable with AI, 62% said they would like to see laws and regulations guiding the use of AI technologies. In line with our findings showing concerns around accountability, 59% said that they would like clear procedures in place for appealing to a human against an AI decision. When asked about who should be responsible for ensuring that AI is used safely, people most commonly choose an independent regulator, with 41% in favour. Support for this differs somewhat by age, with 18–24-year-olds most likely to say companies developing AI should be responsible for ensuring it is used safely (43% in favour), while only 17% of people aged over 55 support this. People say it is important for them to understand how AI decisions are made, even if making a system explainable reduces its accuracy. For example, a complex system may be more accurate, but may therefore be more difficult to explain. When considering whether explainability is more or less important than accuracy, the most common response is that humans, not computers, should make ultimate decisions and be able to explain them (selected by 31%). This sentiment is expressed most strongly by people aged 45 and over. Younger adults (18–44) are more likely to say that an explanation should only be given in some circumstances, even if that reduces accuracy. Taken together, this research makes an important contribution to what we know about public attitudes to AI and provides a detailed picture of the ways in which the British public perceive issues surrounding the many diverse applications of AI. We hope that the research will be useful in helping researchers, developers and policymakers understand and respond to public expectations about the benefits and risks that these technologies may pose, as well as public demand for how these technologies should be governed. 1. How to read this report If you’re a policymaker or regulator concerned with AI technologies: The report highlights the nuance in the perceived benefits and concerns that adults in Britain identify across a range of AI uses. Section 4.2 presents an overview of the perceived benefits and concerns; and Section 4.3 provides more detail on the specific benefits and concerns for each type of technology. Section 4.4 identifies a widely shared expectation for independent regulation that involves explainability and redress. It includes more detail on age differences and expectations of responsibility by different stakeholders. If you’re a developer or designer building AI-driven technologies, or an organisation or body using them or planning to incorporate them: Section 4.4 includes findings related to the expectations and trust the public have for different stakeholders, including private companies and government, and the views from the public on who is responsible for ensuring AI is used safely. Sections 4.2 and 4.3 cover people’s perceived benefits and concerns for different AI uses, with insights on expectations around capabilities and risk. If you’re a researcher, civil society organisation, public participation practitioner or member of the public interested in technology and society: Section 3 includes an overview of the survey methodology. There is more detail in the appendices and the separate technical report.[1] In Appendix 6.1, we include the descriptions of each AI use that we shared with respondents before asking about their awareness and experience of the uses; and about their view of the potential benefits and concerns. Section 4.1 includes an overview of people’s awareness and experience of different AI uses. An overview of overall net benefits and concerns for each technology can be found in Section 4.2. Section 4.3 includes specific perceived benefits and concerns about particular technologies. 2. Introduction Artificial intelligence (AI) technology, and its widespread use in many aspects of public and private life, is developing at a rapid pace. It is therefore crucial to understand how people experience the many applications of AI, including their awareness of these technologies, their concerns, the perceived benefits, and how attitudes differ across demographic groups. To effectively inform the design of policy responses, it is also important to understand people’s views on how these technologies should be governed and regulated. To answer these questions, The Alan Turing Institute and the Ada Lovelace Institute partnered to conduct a new, nationally representative random sample survey of the British public’s attitudes towards, and experiences of, AI. While previous surveys have tackled related questions, there remain several gaps in our understanding of public attitudes to AI. For example, other work has tended to ask about a single definition of AI or has only covered specific uses, meaning that findings regarding positive or negative sentiment toward AI are broad and somewhat ambiguous. Additionally, few large-scale studies elicit people’s preferences for how AI technologies should be regulated, or how explainable a decision made by an AI system should be. Asking people about their views on AI in general can be difficult because the term is hard to define and often poorly understood. Previous surveys have tended to find that people’s knowledge of AI is low, and that few are able to define the term. Only 13% of respondents in a 2022 Public Attitudes to Data and AI tracker survey,[2] and 10% in a 2017 Royal Society survey reported being able to give a full explanation of AI.[3] However, the limited evidence available to date suggests that people tend to be aware of some specific applications of AI, including in healthcare, job application screening, driverless cars, and military uses.[4] [5] [6] [7] [8] With these considerations in mind, we sought to examine attitudes towards a large and varied set of AI uses in society. We wanted to include routine uses that people may not typically think of as AI, and that are often excluded from other studies, such as targeted advertising and smart speakers, as well as uses more commonly associated with the term, such as advanced robotics. Importantly, we aimed to capture the potential complexity of the public’s views. Previous studies suggest that people’s attitudes to AI are nuanced and vary according to specific uses and across countries.[9] For example, people tend to be more supportive of the use of AI where it enhances human decision-makers, such as in healthcare settings,[10] but are more negative where it is seen as replacing human decision-making, such as in cases of criminal justice and driverless cars.[11] We therefore sought to delve deeper into some of the factors underlying these differences, offering people the chance to express both benefits and concerns about uses of AI, recognising that people may simultaneously see positives and negatives in these technologies. We also wanted to understand what people think about the specific benefits and risks associated with different AI uses. Other surveys have found that people report feeling concerned about the potential risks associated with AI, rather than feeling optimistic about the benefits. For example, less than half of the US public believe AI technologies will ‘improve things over the current situation’, and in particular they express high concern about the potential for AI to increase inequality.[12] To build on these findings, we offered people the chance to express how they thought each technology might yield benefits and risks by selecting from a range of possibilities designed to reflect overall themes including accuracy, speed, bias, accountability, data security, job security and more. Our aim was to acknowledge that people may have nuanced views of all the possible benefits and concerns surrounding AI uses, rather than simply measuring positive or negative sentiment, or attitudes to only a few potential risks. To effectively inform policy responses to public concerns surrounding the development and use of AI, it is crucial to understand attitudes towards its governance and regulation. Previous research shows some support for independent or government regulation of AI, with a 2019 UK Department for Business, Energy and Industrial Strategy (BEIS) report showing 33% favour an independent AI regulator, and 22% favour a government regulator.[13] The same report showed that the UK public are not confident that UK data protection regulations can adapt to new technologies, expressing concerns over adequate regulation in the face of a fast-changing landscape. Additionally, citizens’ juries have found that people prioritise the explainability of an AI system over its accuracy,[14] and other work offers important resources and guidelines for aiding AI explainability.[15] However, there is currently little available evidence about explainability preferences from a large-scale and recent sample. Through the results of this survey, we provide a detailed picture of how the British public perceive issues surrounding the many diverse applications of AI. We hope that the research will be useful for informing researchers, developers and policymakers about the concerns and benefits that the public associate with AI, thereby helping to maximise the potential benefits of AI. 3. Methodology In this chapter we provide a summary of the key aspects of the study’s methodology. A technical report[16] containing full details of the methodological approach including how we designed our questions for the study can be accessed separately.[17] Sample The sample was drawn from the Kantar Public Voice random probability panel.[18] This is a standing panel of people who have been recruited to take part in surveys using random sampling methods. At the time the survey was conducted, it comprised 24,673 active panel members who were resident in Great Britain and aged 18 or over. This subset of panel members was stratified by sex/age group, highest educational level and region, before a systematic random sample was drawn. We undertook fieldwork in November and December 2022, and issued the survey in three stages: a soft launch with a random subsample of 500 panel members, a launch with the remainder of the main panel members, and a final launch with reserve panel members. A total of 4,010 respondents completed the survey and passed standard data quality checks.[19] The majority of respondents completed the questionnaire online, while 252 were interviewed by telephone either because they do not use the internet or because this was their preference. Respondents were aged between 18 and 94. Unweighted, a total of 1,911 (48%) identified as male, and 2,096 (52%) as female, with no sex recorded for three participants. The majority (3,544; 88%) of respondents were white; 261 (7%) were Asian or Asian British; 90 (2%) were Black, African, Caribbean or Black British; and 103 (3%) were mixed, multiple or other ethnicities; with no ethnicity recorded for 12 participants.[20] The data was weighted based on official statistics to match the demographic profile of the population (see technical report).[21] However, with a sample size of 4,010, it is not possible to provide robust estimates of differences across minority ethnic groups, so these are not reported here. Survey We told respondents that the questions focus on people’s attitudes towards new technologies involving artificial intelligence (AI), and presented the following definition of AI to them: AI is a term that describes the use of computers and digital technology to perform complex tasks commonly thought to require intelligence. AI systems typically analyse large amounts of data to take actions and achieve specific goals, sometimes autonomously (without human direction). Respondents then answered some general questions about attitudes to new technologies and how confident they feel using computers for different tasks. They were then asked questions about their awareness of and experience with specific uses of AI; how beneficial and concerning they perceive each use to be; and about the key risks and benefits associated with each. The specific technologies we asked about were: facial recognition (uses were unlocking a mobile phone or other device, border control, and in policing and surveillance) assessing eligibility (uses were for social welfare and for job applications) assessing risk (uses were risk of developing cancer from a scan and loan repayments) targeted online advertising (for consumer products and political adverts) virtual assistants (uses were smart speakers and healthcare chatbots) robotics (uses were robotic vacuum cleaners, robotic care assistants, driverless cars and autonomous weapons) simulations (uses were simulating the effects of climate change and virtual reality for educational purposes). These 17 AI uses were chosen based on emerging policy priorities and increased usage in public life. See Section 4.1 or Appendix 6.1 for the descriptions of each use. See the technical report [22] for information about our questionnaire design. To keep the duration of the survey to an average of 20 minutes, we employed a modular questionnaire structure. Each person responded to questions about nine of the 17 different AI uses. All participants were asked about facial recognition for unlocking a mobile phone and then responded to one of the two remaining uses of facial recognition. They were then asked about one of the two uses for the other technologies, other than robotics, for which there were four uses. For robotics, each participant considered either robotic vacuum cleaners or robotic care assistants, and then either driverless cars or autonomous weapons. After responding to questions for each specific AI use, participants answered three general questions about AI governance, regulation and explainability. The survey was predominantly made up of close-ended questions, with respondents being asked to choose from a list of predetermined answers. Analysis We analysed the data between January 2023 and March 2023, using descriptive analyses for all survey variables followed-up with chi-square testing of differences across specific demographic groups. We then used regression analyses to understand relationships between demographic and attitudinal variables, and perceived benefit of specific technologies (see Appendix 6.3 for further information). We analysed the data using the statistical programming language R, and used a 95% confidence level to assess statistically significant results. Analysis scripts and the full survey dataset can be accessed on the Ada Lovelace Institute GitHub site.[23] In this report, we generalise from a nationally representative sample of the population of Great Britain to refer to the ‘British public’ (sometimes shortened to ‘the public’) or ‘people in Britain’ (sometimes shortened to ‘people’) throughout. This phrasing does not refer to British nationals, but rather to people living in Great Britain at the time the survey was conducted. 4. Key findings We asked about the uses of AI listed below. Detailed definitions for each technology can be found in Appendix 6.1. Facial recognition… … to unlock a mobile phone … at border control … for policing and surveillance Assess eligibility… … for welfare benefits … for a job Determine risk… … of cancer from a scan … of repaying a loan Targeted advertisements online… … for consumer products … for political parties Virtual assistant technologies… … smart speakers … virtual assistants for healthcare Robotics… … robotic vacuum cleaners … robotic care assistants … driverless cars …autonomous weapons 4.1. Awareness and experience of AI uses To understand people’s awareness of and experience with each of the AI technologies included, participants were asked to indicate whether they had heard of each technology before and their self-reported personal experience with each. The question on personal experience was not included for autonomous weapons, driverless cars, robotic care assistants and simulation technologies for advancing climate change research, where direct experience would be unlikely for most respondents. Overall, awareness of and experience with AI technologies varies substantially according to the specific use. Awareness of AI technologies is mixed. For 10 of the 17 technologies we asked about, over 50% of the British public say they have heard of them before. Awareness is highest for the use of facial recognition for unlocking mobile phones, with 93% having heard of this before. People are also largely aware of driverless cars (92%) and robotic vacuum cleaners (89%). People are least aware of the use of AI for assessing eligibility for welfare benefits, with just 19% having heard of this before. People are also less aware of robotic care assistants (32%), using AI to detect risk of cancer from a scan (34%), and using AI to assess eligibility for jobs or risks relating to loan repayments (both 35%). It is important to note that people’s awareness of technologies for assessing risk and eligibility is relatively low. Some of these technologies are already being used in public services,[24] and these results show that people may be largely unaware of the technologies that help make decisions which directly impact their lives. Awareness of AI technologies differs somewhat according to age, with people aged 75 and over less likely to indicate they have heard of the use of facial recognition for unlocking mobile phones (69% reported being aware, compared to 95% of under 75s), border control (61% reported being aware, compared to 72% of under 75s), or for consumer social media adverts (68% reported being aware, compared to 89% of under 75s). Our findings about people’s awareness of AI technologies align with those from other studies, which highlight gaps in awareness of AI that are less visible in day-to-day life or the media. For example, a Centre for Data Ethics and Innovation (CDEI) 2022 mixed-methods study [25] found that the public have high levels of awareness of more visible uses of AI, such as recommendation systems, and futuristic associations of AI based on media images such as robotics. In contrast, the same study found low levels of awareness of AI in technologies that are part of wider societal systems’, such as the prioritisation of social housing. People report mixed levels of personal experience with AI technologies. Over 50% of the public report personal experience with four of the 13 technologies we asked about. People report most experience with targeted online adverts for consumer products (with 81% reporting some or a lot of experience), smart speakers (with 64% reporting some or a lot of experience), and facial recognition for unlocking mobile phones and at border control (with 62% and 59% respectively reporting some or a lot of experience). People report least experience with AI for determining risk of cancer from a scan (8%), for calculating welfare eligibility (11%) and with facial recognition for police surveillance (12%). Experience with some of the technologies differs according to age. People aged 75 and over report less experience with facial recognition to unlock mobile phones (23% report having some or a lot of experience compared to 67% of under 75s), facial recognition at border control (32% report having some or a lot of experience compared to 62% of under 75s), and social media advertisements for consumer products (51% vs 84%) and political parties (18% report having some or a lot of experience compared to 52% of under 75s). Figure 1 shows level of awareness for each of the 17 AI uses, and Figure 2 shows how much personal experience people report having with the 13 AI uses for which experience level was asked. 4.2. How beneficial do people think AI technologies are, and how concerning? To find out about overall attitudes towards different AI technologies, for each technology they were asked about, respondents indicated the extent to which they think the technology will be beneficial, and the extent to which they are concerned about the technology. The extent to which AI is perceived as beneficial or as concerning varies greatly according to the specific use. The British public tend to perceive facial recognition technologies, virtual and robotic assistants, and technologies having health or science applications as very or somewhat beneficial. A majority says facial recognition for unlocking mobile phones, at border control and for police surveillance is somewhat or very beneficial. In addition, over half also say that virtual assistants, both smart speakers and healthcare assistants; simulations to advance knowledge in both climate change research and in education; risk assessments for cancer and loan repayments; and robotics for vacuum cleaners and care assistants are beneficial. AI uses with the highest percentage of people indicating ‘very’ or ‘somewhat’ beneficial are cancer risk detection (88% think beneficial) and facial recognition for border control and police surveillance (87% and 86% respectively think beneficial). These attitudes resonate with previous research, which found that people are positive about the role of AI in improving the efficiency of day-to-day tasks, the quality of healthcare, and the ability to save money on goods and services.[26] Figure 3 shows how beneficial people believe each use of AI to be. The British public are most concerned about AI uses that are associated with advanced robotics, advertising and employment. More than half of British adults are somewhat or very concerned about the use of robotics for driverless cars and autonomous weapons, the use of targeted advertising online for both political and consumer adverts, for calculating job eligibility and for virtual healthcare assistants. These findings complement those from previous studies that indicate concern around the use of AI in contexts that replace humans, such as driverless cars,[27] and in advertising.[28] Figure 4 shows the level of concern people have about each use of AI. The proportion of the public selecting ‘don’t know’ in response to how concerned they are about each AI use is relatively small, suggesting little ambivalence or resignation towards AI across different uses. The British public do not have a single uniform view of AI – rather, there are mixed views about the extent to which AI technologies are seen as beneficial and concerning depending on the type of technology. To further understand these views, we created net benefit scores by subtracting the extent to which each respondent indicated the AI use was concerning from the extent to which they indicated the AI use was beneficial. Positive scores indicate that perceived benefit outweighs concern, negative scores indicate that concern outweighs perceived benefit and scores of zero indicate equal levels of concern and perceived benefit. More detail on this analysis can be found in Appendix 6.3. Benefit level outweighs concern for 10 of the 17 technologies. These are: cancer risk detection; simulations for climate change research and education; robotic vacuum cleaners; smart speakers; assessing risk of repaying a loan; robotic care assistants; and facial recognition for unlocking mobile phones, border control and police surveillance. These findings add to the Ada Lovelace Institute’s 2019 research into attitudes towards facial recognition, where findings showed that most people support the use of facial recognition technology where there is demonstrable public benefit.[29] Concern outweighs benefit level for five of the 17 technologies. These are: autonomous weapons; driverless cars; targeted social media advertising for consumer products and political ads; and AI for assessing job eligibility. Some technologies are seen as more divisive overall, with equal levels of concern and perceived benefit reported. This is the case for virtual healthcare assistants, and welfare eligibility technology. Figure 5 shows mean net benefit scores for each technology. 4.2.1. Individual and group level differences in perceptions of net benefits We analysed whether perceived net benefits for each AI technology differed according to differences in the sample such as sex, age, education level, and how aware, informed or interested people are in new technologies. The public think differently about facial recognition technologies depending on their level of education, how informed they feel about new technologies, and their age. People who feel more informed about technologies or who hold degree-level qualifications are significantly less likely than those who feel less informed or do not hold degree-level qualifications to believe that the benefits of facial recognition technologies outweigh the concerns. People aged 65 and over are significantly more likely than those under 65 to believe that the benefits of facial recognition technologies outweigh the concerns. Awareness of a technology is not always a significant predictor of whether or not people perceive it to be more beneficial than concerning. For uses of AI in science, health, education and robotics, being aware of the technology is associated with perceiving it to be more beneficial than concerning. These include: virtual healthcare assistants, robotic care assistants, robotic vacuum cleaners, autonomous weapons, cancer risk prediction, and simulations for climate change and education. However, awareness can also exacerbate concerns. Being aware of the use of targeted social media advertising (both for consumer and political ads) is associated with concern outweighing perceived benefits. Those who feel more informed about technology are also less likely to see targeted advertising on social media for consumer products as beneficial, compared with those who feel less informed. Appendix 6.3 provides more information about the analyses outlined in this section, including further results showing the effects of demographic and attitudinal differences on perceived net benefit for each technology. Appendix 6.3 also includes a figure showing how the perceived net benefits for each AI technology differ according to differences in sex, age, education level, and how aware, informed or interested people are with new technologies. These findings support existing research from the Ada Lovelace Institute into public attitudes around data, suggesting that public concerns should not simply be dismissed as reflecting a lack of awareness or understanding of AI technologies, and further that raising awareness alone will not necessarily increase public trust in these systems.[30] More qualitative and deliberative research is needed to understand the trade-offs people make between specific benefits and concerns. The nuanced impact of awareness about attitudes towards AI technologies is evident in the range of specific benefits and concerns people select relating to each one technology, described in the next section. 4.3. Specific benefits and concerns around different AI uses To further understand how people view the possible benefits and concerns surrounding different uses of AI, we asked respondents to select specific ways they believe each technology to be beneficial and concerning from multiple choice lists. The benefits and concerns included in each list were created to reflect common themes, such as speed and accuracy, bias and accountability, though each list was specific to each technology (see full survey for all benefits and concerns listed for each technology). Participants could select as many statements from each list as they felt applied, with ‘something else’, ‘none of the above’, and ‘don’t know’ options also given for each. Overall, people most commonly identify benefits related to speed, efficiency and accessibility, and most commonly express concerns related to overreliance on technologies over professional human judgement, being unable to account for personal circumstances, and a lack of transparency and accountability in decision-making processes. However, the specific benefits and concerns most commonly selected vary across technologies. The following sections describe the specific benefits and concerns that people chose for each AI use. We cluster these by categories of technologies for risk and eligibility assessments, facial recognition technologies, robotics, virtual assistants, targeted online advertising, and simulations for science and education. Tables 1–12 show the three most commonly chosen benefits and concerns for each technology. A full list of benefits and concerns presented to participants and the percentage of people selecting each can be found in Appendix 6.3.4. 4.3.1. Risk and eligibility assessments We asked about the following uses of assessing eligibility and risk using AI: to calculate eligibility for jobs, to assess eligibility for welfare benefits, to predict the risk of developing cancer from a scan, and to predict the risk of not repaying a loan. The public’s most commonly chosen benefit for risk and eligibility assessments is speed (for example, ‘applying for a loan will be faster and easier’). Just under half, 43%, think speed is a benefit of using AI to assess eligibility for welfare benefits, 49% for job recruitment, and 52% for assessing risk of repaying a loan. An overwhelming majority of 82% think that earlier detection of cancer is a key advantage in using AI to predict the risk of cancer from a scan, a consensus not reached in any other technologies. In addition to speed, reduction of human bias and error are seen as key benefits of technologies in this group. For the use of AI in recruiting for jobs and for assessing risk of repaying a loan, the technologies being less likely than humans to ‘discriminate against some groups of people in society’ is the second most commonly selected benefit, selected by 41% and 39% respectively. Reduction in ‘human error’ is the second most commonly selected benefit for the use of AI in determining risk of cancer from scans and for assessing eligibility for welfare benefits, selected by 53% and 38% respectively. The technologies being more accurate than human professionals overall, however, is not selected as a key benefit of most uses of AI in this group. Less than one third of people in Britain perceive this to be a key benefit for the use of AI in determining risk for the repayment of loans (29% selected), determining eligibility for welfare benefits (22% selected) and determining eligibility for jobs (13% selected). An exception to this pattern is in the use of AI to determine risk of cancer from scans, where 42% of people perceive a key benefit as improved accuracy over professionals. The most common concerns the British public have about using AI for these eligibility and risk assessments include the technology being less able than a human to account for individual circumstances, overreliance on technologies over professional judgement, and a lack of transparency about how decisions are made. These concerns are particularly high in relation to the use of AI in job recruitment processes with 64% saying they think that professionals will ‘rely too heavily on their technology rather than their professional judgements’; 61% saying that the technology will be ‘less able than employers and recruiters to take account of individual circumstances’; and 52% saying that ‘it will be more difficult to understand how decisions about job application assessments are reached’. These concerns add to findings from CDEI’s latest research into public expectations around AI governance, where people felt it was important to have a clear understanding of the criteria AI uses to make decisions in the case of job recruitment and to have the ability to challenge such decisions.[31] The British public express repeated concerns around a lack of human oversight in AI technologies, even for the use of AI to determine cancer risk from a scan – a technology that is seen as largely beneficial. As seen in the previous section, AI for predicting risk of cancer from a scan is perceived to be one of the most beneficial technologies in the survey. Yet, over half of British adults (56%) still express concern about relying too heavily on this technology rather than professional judgements, while 47% are concerned that if the technology made a mistake it would be difficult to know who is responsible. These attitudes suggest that the public see value in human oversight in AI for cancer risk detection, even when this use of AI is perceived as largely positive. 4.3.2. Facial recognition We asked the British public about the following uses of facial recognition technologies: its use for unlocking mobile phones, for policing and surveillance and facial recognition use at border control. Most of the British public feel speed is the main benefit offered by facial recognition technologies. Over half, 61%, of people say ‘it is faster to unlock a phone or personal device’ in relation to phone unlocking, 77% say ‘the technology will make it faster and easier to identify wanted criminals and missing persons’ in relation to policing and surveillance and 70% identify ‘processing people at border control will be faster’ as a benefit in relation to border control. Although half of the public perceive accuracy to be a substantial benefit of these technologies, half have concerns around these technologies making mistakes. On the one hand, the technology being more accurate than professionals is the second most selected benefit for the use of facial recognition in policing and surveillance (chosen by 55% of people) and the use of facial recognition at border control (chosen by 50% of people). On the other hand, the most commonly selected concern for policing and surveillance is false accusations (54% of people worry that ‘if the technology makes a mistake it will lead to innocent people being wrongly accused’); while for border control, the most selected concern is related to accountability (‘if the technology makes a mistake, it will be difficult to know who is responsible for what went wrong’). Therefore, while speed is seen by a majority as a benefit, there are a range of concerns that are mentioned by approximately half of people over the use of facial recognition for border control and police surveillance. A survey conducted by the Ada Lovelace Institute in 2019 found that a majority supported facial recognition technology when there was a demonstrable public benefit and appropriate safeguards in place.[32] Very few people identify concerns about the use of facial recognition in policing, surveillance and border control as discriminatory technologies. However, there may be socio-demographic differences around these concerns. The responses suggest that Black people, students and those with no formal qualifications might be more concerned about the discriminatory potential of these technologies. However, it is important to note that our sample sizes for various subgroups are too small to be statistically significant, and we need to follow up these indicative findings through other research methods. More research is also needed to understand the lived experiences of different groups and concerns about how these technologies can impact, or can be perceived to impact, people in different ways. 4.3.3. Robotics In the case of robotics, specific benefits vary depending on the area in which the AI is applied, with accessibility and speed being the most common benefits. Accessibility is the most commonly selected benefit for robotic technologies that can make day-to-day activities easier for people who otherwise might not be physically able to do them (driverless cars and vacuum cleaners), highlighting positive perceptions, and potentially high expectations, around AI making tasks easier for all of society. People are concerned about a lack of human interaction in AI technologies, the potential overreliance on the technology at the expense of human judgement and issues of who to hold accountable when the technology makes a mistake. As with benefits, concerns also vary depending on where robotics are applied. For robotic care assistants, people note significant advantages relating to efficiency (that is, faster, and more accurate). However, people are most worried about the potential loss of human interaction (78% worry that ‘patients will miss out on the human interaction they would otherwise get from human carers’), suggesting that people do not want AI-powered technologies to replace human-to-human care. This is consistent with findings from the Public Attitudes to Science survey in 2019, which found that people were concerned that the use of AI and robotics in healthcare would reduce human interaction, and that the public were open to the idea of the use of this technology to support, rather than replace, a doctor.[33] Nearly half of people identify concerns relating to the technology leading to job cuts to human caregiving professionals (46%), and that it would be difficult to assign responsibility for what went wrong if the robot care assistant made a mistake (45%). In the case of driverless cars, the most selected concerns relate to: lack of reliability (62% chose ‘the technology will not always work, making the cars unreliable’); accountability for mistakes (59% chose ‘if the technology makes a mistake, it will be difficult to know who is responsible for what went wrong’); and lack of clarity on how decisions were made (51% chose ‘it will be more difficult to understand how the car makes decisions compared to a human driver’). Similarly, people’s concerns about autonomous weapons centre on overreliance on the technology (selected by 54%) and lack of clarity on who would be responsible if the technology made a mistake (selected by 53%). 4.3.4. Virtual assistants In relation to virtual assistants, we asked specifically about smart speakers and about the use of virtual assistants in healthcare. The British public most commonly chose accessibility and speed as benefits in relation to virtual assistants, a similar finding to the benefits chosen for robotics. Accessibility (‘The technology will allow people with difficulty using devices to access features more easily’) is the most selected benefit of smart speakers, selected by 71% of people. To a lesser extent, accessibility is also the top benefit mentioned in relation to virtual health assistants (53% chose ‘The technology will be easier for some groups of people in society to use, such as those who have difficulty leaving their home’). Speed is the second most selected benefit for both technologies. Over half, 60%, of people selected speed as a benefit for smart speakers, while 50% selected it for virtual mental health assistants. People are most concerned about the gathering and sharing of personal data for smart speakers. This is also a common concern across other technologies that are more visible and commonplace in day-to-day lives, such as the use of facial recognition for unlocking mobile phones, and targeted online social media advertisements. Over half (57%) of the British public selected ‘the technology will gather personal information which could be shared with third parties’ as a concern. This concern aligns with previous research into attitudes towards the use of personal data, where data security and privacy were felt to be the greatest risk for data use in society.[34] This concern is particularly salient among those who are more generally concerned by smart speakers, where the top two concerns relate to personal information. In this group, 79% are concerned that their personal information could be shared with third parties and 68% are concerned their personal information is less safe and secure. These concerns suggest that people see data security as more significant for AI technologies that are designed for more personal use, particularly in spaces like home or work. The biggest concern in relation to virtual assistants in healthcare relates to the potential difficulty for some people to use it, and the technology not being able to account for individual differences. Almost two thirds of the British public (64%) identify difficulty in use (‘some people may find it difficult to use the technology’) as a concern in relation to virtual assistants in healthcare, which is higher than the 53% who mention accessibility as a benefit. This concern reiterates the value people place on AI technologies working for all members of society. Another major concern raised around virtual assistants in healthcare is that the technology may not account for individual circumstances as well as human healthcare professionals (63%). Those with experience of virtual assistants in healthcare are more likely than those without to report concerns around the technology being more inaccurate than humans. Concerns include: suggesting diagnosis and treatment options; the difficulty of assigning who is accountable when the technology makes mistakes; and the technology being less effective for some members of society. However, those with experience of these technologies are also more likely to report benefits relating to accessibility, helping the health system save money, personal information being secure and the technology being less likely than healthcare professionals to discriminate against some groups of people in society. 4.3.5. Targeted online advertising While discovery of new and relevant content is the most mentioned benefit for the use of consumer or political targeted online advertising, the public identify invasions of privacy and personal information being shared with third parties as the most prevalent concerns, highlighting a tension between personalisation of content and privacy. Half of the public (50%) chose ‘it will help people discover new products that might be of interest to them’ as a benefit in relation to targeted online consumer advertisement, while only one third (33%) select this as a benefit for targeted online political advertisement (‘It will help people discover new political representatives who might be of interest to them’). Similar proportions for both technologies mention the relevance of ads as a benefit for consumer targeted advertising (53%) and for political ads (32%). However, as seen in previous sections, people are highly concerned about these uses of AI. Over two thirds of people (69%) identify invading privacy as a concern for targeted online consumer advertisements, while 51% identify this for political advertisements. Similarly, 68% selected ‘the technology will gather personal information which could be shared with third parties’ as a concern for consumer adverts while 48% selected this concern for political adverts. This suggests that while the public might find social media advertising more helpful in discovering relevant content, especially for consumer adverts, they are also less trusting of what is done with their personal information. This resonates with the findings from an online study on online advertising in the UK and France which found that most participants were concerned about how their browsing activity was being used even when they saw some of the benefits related to discovery. The study concluded that participants wanted their data, and their ability to choose how it is used, to be respected and to be able to ‘practically, meaningfully, and simply curate their own advertising experience’.[35] 4.3.6. Simulations We asked about two uses of AI simulations for advancing knowledge, one relating to the use of AI for climate change research and another around the use of virtual reality for educational purposes. The public see the main benefits of simulations for science and education as making it faster and easier to enhance knowledge and understanding, as well as enabling a greater number of people to learn or benefit from research. However, the public are concerned about inequalities in access to the technology, meaning not everyone will benefit. When asked about the use of new simulation technologies to advance climate change research, around two thirds of people said: that they would ‘make it faster and easier for scientists and governments to predict climate change effects’ (64%); that it would ‘predict issues across a wider range of regions and countries, meaning more people will experience the benefits of climate research’ (64%); and that it would ‘allow more people to understand the possible effects of climate change’ (63%). In relation to the use of simulation technologies like virtual reality for education, the potential to ‘increase the quality of education by providing more immersive experiences’ (66%), and its potential to ‘allow more people to learn about history and culture’ (60%) are the most selected benefits (Table 11). Overall, the public choose few concerns in relation to AI for climate change research. People don’t express many specific concerns about the use of simulation technologies for advancing climate change research. Over one third (36%) selected the risk that ‘the technology will predict issues in some regions better than others, meaning that some people do not experience the benefits of these technologies’. After this concern, however, the most selected answer is ‘None of these’ (26%), followed by 21% who selected inaccuracy as a concern. The public are most concerned about inequalities in access and control over narratives in education in relation to the development of virtual reality for education. Over half (51%) of British adults are concerned that ‘some people will not be able to learn about history and culture in this way as they will not have access to the technology’ in the development of virtual reality for education. This concern is followed by giving control over to technology developers on ‘what people learn about history or culture’ which is selected by 46% of people. 4.4. Governance and explainability 4.4.1. Explainability To understand how explainable the British public think a decision made by an AI system should be when explainability trades off with accuracy, we first informed participants that: ‘Many AI systems are used with the aim of making decisions faster and more accurately than is possible for a human. However, it may not always be possible to explain to a person how an AI system made a decision.’ We then asked people which of the following statements best reflects their personal opinion: Making the most accurate AI decision is more important than providing an explanation. In some circumstances an explanation should be given, even if that makes the AI decision less accurate. An explanation should always be given, even if that makes all AI decisions less accurate. Humans, not computers, should always make the decisions and be able to explain them to the people affected. When there are trade-offs between the explainability and accuracy of AI technologies, the British public value the former over the latter: it is important for people to understand how decisions driven by AI are made. Figure 6 shows that only 10% of the public feel that ‘making the most accurate AI decision is more important than providing an explanation’, whereas a majority choose options that reflect a need for explaining decisions. Specifically, almost one third (31%) indicate that humans should always make the decisions (and be able to explain them), followed by 26% who think that ‘sometimes an explanation should be given, even if it reduces accuracy’ and another 22% who choose ‘an explanation should always be given, even if it reduces accuracy’. People’s preferences for explainable AI decisions dovetail with the importance of transparency and accountability demonstrated by people’s specific concerns about each technology (described in Section 4.3). Here, for all technologies[36] (except for driverless cars and virtual health assistants) the proportion of concerns mentioning ‘it is unclear how decisions are made’ is higher than mentions of ‘inaccuracy’. People’s preferences for explainability over accuracy change across age groups. Older people choose explainability and human involvement over accuracy to a greater extent than younger people. For those aged 18–44, ‘sometimes an explanation should be given even if it reduces accuracy’ was the most popular response (Figure 7). At the youngest end of the age spectrum (18–24) ‘humans should always make the decisions and be able to explain them’ is the least popular response, whereas this becomes the first choice from 45+ and above and the highest for respondents aged 65+. 4.4.2. Governance and regulation To find out about people’s views on the regulation of AI, we asked people to indicate what (if anything) would make them more comfortable with AI technologies being used. Participants could select as many they felt applied from a list of seven possible options. Public attitudes suggest a need for regulation that involves redress and the ability to contest AI-powered decisions. People most commonly indicated that ‘laws and regulations that prohibit certain uses of technologies and guide the use of all AI technologies’ would increase their comfort with the use of AI, with 62% in favour. People are also largely supportive of ‘clear procedures for appealing to a human against an AI decision’ (selected by 59%). Adding to the concerns expressed about data security and accountability, 56% of the public want to make sure that ‘personal information is kept safe and secure’ and 54% want ‘clear explanations of how AI works’. Figure 8 shows the proportion of people selecting each option when asked what, if anything, would make them more comfortable with AI technologies being used. We also asked participants who they think should be most responsible for ensuring AI is used safely from a list of seven potential actors. People could select up to two options. The British public want regulation of AI technologies. ‘An independent regulator’ is the most popular choice for governance of AI. Figure 9 shows 41% of people feel that ‘An independent regulator’ should be responsible for the governance of AI, the most popular choice of the seven presented. Patterns of preferred governance do not change notably depending on whether people feel well informed about new technologies or not. Results add to a PublicFirst poll conducted in March 2023 with 2,000 UK adult respondents which found that 62% of respondents supported the creation of a new government regulatory agency, similar to the Medicines and Healthcare Products Regulatory Agency (MHRA), to regulate the use of new AI models.[37] People’s preferences for the governance of AI changes across age groups. While people overall most commonly select ‘an independent regulator’, Figure 10 shows 43% of 18–24-year-olds think that the ‘companies developing the technology’ should be most responsible for ensuring AI is used safely. In contrast, only 17% of people over 55 select this option. This could reflect more in-depth experiences by young people with different technologies and associated risks, and therefore demands for more responsibility on developers. Especially since young people also report the highest exposure to technology driven problems such as online harms’.[38] That 18–24-year-olds most commonly say that the companies developing the technologies should be responsible for ensuring AI is used safely raises questions about private companies’ corporate responsibility alongside regulation. To understand people’s concerns about who develops AI technologies, we asked people how concerned, if at all, they feel about different actors producing AI technologies. We asked this in the context of hospitals asking an outside organisation to produce AI technologies that predict the risk of developing cancer from a scan, and the Department for Work and Pensions (DWP) asking an outside organisation to produce AI technologies for assessing eligibility for welfare benefits. We asked people how concerned they are about each of the following groups producing AI in each context: private companies not-for-profit organisations (e.g. charities) another governmental body or department universities/academic researchers. For both the use of AI in predicting cancer from a scan, and assessing eligibility for welfare benefits, the British public are most concerned by private companies developing the technologies and least concerned by universities and academic researchers developing the technologies For the development of AI which may be used to assist the Department for Work and Pensions in assessing eligibility for welfare benefits, the public are most concerned about private companies developing the technology, with 66% being somewhat or very concerned. Just over half, 51%, of people are somewhat or very concerned about another governmental body or department developing the technology, and 46% somewhat or very concerned about not-for-profit organisations developing the technology. People are generally least concerned about universities or academic researchers developing this technology, with 43% being somewhat or very concerned. While this is the lowest percentage of concern compared to other stakeholders, this is still a sizable proportion of people expressing concern, which suggests the need for more trusted stakeholders to also be transparent about their role and approach to developing technologies. Regarding the development of AI that may help healthcare professionals predict the risk of cancer from a scan, there is a very similar pattern of concerns over who develops the technology. People are most concerned with private companies developing the technology with 61% being somewhat or very concerned, followed by a governmental body (44%). People are less concerned with not-for-profit organisations and universities or academic researchers developing the technology. Overall level of concern about developers was lower for technologies that predict risk of cancer than technologies which help assess eligibility for welfare. Figure 11 shows the extent to which people feel concerned by the following actors developing new technologies to assess eligibility for welfare benefits and predict the risk of developing cancer: private companies, governmental bodies, not-for-profit organisations and universities/academic researchers. While we asked about concerns over the development of a specific technology rather than overall trust, our findings resonate with results from the second wave of a CDEI survey on public attitudes towards AI, which found that on average, respondents most trusted the NHS and academic researchers to use data safely, while trust in government, big tech companies and social media companies was lower.[39] 5. Conclusion This report provides new insights into the British public’s attitudes towards different AI-powered technologies and AI governance. It comes at a time when governments, private companies, civil society and the public are grappling with the rapid pace of development of AI and its potential impacts across many areas of life. A key contribution of this survey is that it highlights complex and nuanced views from the public across different AI applications and uses. People identify specific concerns about technologies even when they see them as overall more beneficial than concerning, and acknowledge potential benefits about particular technologies even when they also express concern. The public are aware of the use of AI in many visible, commonplace technologies, such as the use of facial recognition for unlocking phones, or the use of targeted advertising in social media. However, awareness of AI technologies used in public services with potential high impact on people, like the use of AI for welfare benefits eligibility, is low. The public typically see advantages of several uses of AI as improving efficiency, and accessibility. However, people worry about the security of their personal data, the replacement of professional human judgements, and the implications for accountability and transparency in decision-making. While applications of AI in health, science, education and security are overall perceived positively, applications in advanced robotics and targeted advertising online are viewed as more concerning. There is a strong desire among the public for independent regulation, more information on how AI systems make decisions, and the ability to challenge decisions made by AI. Younger adults also tend to place responsibility on the companies developing AI to ensure that the technologies are used safely. Future work will benefit from understanding how different groups of people in society are impacted differently by various uses of AI. However, this study highlights important considerations for policymakers and developers of AI technologies and how they can help ensure AI technologies work for people and society: Policymakers and developers of AI systems must work to support public awareness and enhance transparency surrounding the use of less visible applications of AI used in the public domain. This is particularly true for areas that have significant impacts on people’s lives, such as in assessments for benefits, financial support or employment. The findings show that the public expect many AI technologies to bring improvements to their lives, particularly around speed, efficiency and accessibility. It is important for policymakers and developers of these technologies to meet public expectations, work to strengthen public trust in AI further, and therefore help to maximise the benefits that AI has the potential to bring. While people are positive about some of the perceived benefits of AI, they also express concerns, particularly around transparency, accountability, and loss of human judgement. As people’s interaction with AI increases across many areas of life, it is crucial for policymakers and developers of AI to listen to public concerns and work towards solutions for alleviating them. People call for regulation of AI and would like to see an independent regulator in place, along with clear procedures for appealing against AI decisions. Policymakers working on AI regulatory regimes should consider the establishment of an independent regulatory body of AI technologies and ensure that the public have opportunities to seek redress if AI systems fail or make a mistake. People in older age groups are particularly concerned about the explainability of AI decisions and lack of human involvement in decision-making. It is important for policymakers and civil society organisations to work to ensure older members of society in particular do not feel alienated by the increasing use of AI in many decision-making processes. Lastly, policymakers must acknowledge that the public have complex and nuanced views about uses of AI, depending on what the technology is used for. Debates or policies will need to go beyond general assumptions or one-size-fits-all approaches to meet the demands and expectations from the public. 6. Appendix 6.1. Descriptions for each technology use case The following definitions were provided to survey respondents: Facial recognition Facial recognition technologies are AI technologies that can compare and match human faces from digital images or videos against those stored elsewhere. The technology works by first being trained on many images, learning to pick out distinctive details about people’s faces. These details, such as distance between the eyes or shape of the chin, are converted into a face-print, similar to a fingerprint. Mobile phone One use of facial recognition technology is for unlocking mobile phones and other personal devices. Such devices use this technology by scanning the face of the person attempting to unlock the phone through the camera, then comparing it against a saved face-print of the phone’s owner. Border control Another use of facial recognition technology is to assist with border control. ‘eGates’ at many international airports use facial recognition technologies to attempt to automatically verify travellers’ identities by comparing the image on their passport with an image of their face taken by a camera at the gate. If the technology verifies the person’s identity, the eGate will open and let them through, otherwise they will be sent to a human border control officer. Police surveillance Another use of facial recognition technology is in policing and surveillance. Some police forces in Britain and elsewhere use this technology to compare video footage from CCTV cameras against face databases of people of interest, such as criminal suspects, missing persons, victims of crime or possible witnesses. Eligibility Some organisations use AI technologies to help them decide whether someone is eligible for the programmes or services they offer. These AI technologies draw on data from previous eligibility decisions to assess the eligibility of a new applicant. The recommendations of the technology are then used by the organisation to make the decision. Welfare eligibility AI technologies that assess eligibility are sometimes used to determine a person’s eligibility for welfare benefits, such as Universal Credit, Jobseeker’s Allowance or Disability Living Allowance. Here, AI technologies are trained on lots of data about previous applicants for similar benefits, such as their employment history and disability status, learning patterns about which features are associated with particular decisions. Many applications will only be considered for the benefit once the computer has marked them as eligible. Job eligibility One use of AI technologies for assessing eligibility is for reviewing people’s job applications. The technology will look at a person’s job application or CV and automatically determine if they are eligible for a job. Here, AI technologies are trained on lots of data from decisions about previous applicants for similar roles, learning patterns about which features are associated with particular hiring outcomes. Many employers who use this technology will only read the applications that the computer has marked as an eligible match for the role. Risk AI technologies may be used by organisations to predict the risk of something happening. When predicting the risk, these AI technologies draw on a wide range of data about the outcomes of many people to calculate the risk for an individual. The recommendations these technologies make are then used by organisations to make decisions. Cancer risk One use of AI technologies for calculating risk is for assessing a medical scan to identify a person’s risk of developing some types of cancer. Here, AI technologies are trained on many scans from past patients, learning patterns about which features are associated with particular diagnoses and health outcomes. The technology can then give a doctor a prediction of the likelihood that a new patient will develop a particular cancer based on their scan. Loan repayment risk One use of AI technologies for calculating risk is to assess how likely a person is to repay a loan, including a mortgage. Here, AI technologies are trained on data about how well past customers have kept up with repayments, learning which characteristics make them likely or unlikely to repay. When a new customer applies for a loan, the technology will assess a range of information about that person and compare it to the information it has been trained on. It will then make a prediction to the bank about how likely the new customer will be able to repay the loan. Targeted online advertising Targeted advertising on the internet tailors adverts to a specific user. These kinds of ads are commonly found on social media, online news sites, and video and music streaming platforms. The technology uses lots of data generated by tracking people’s activities online to learn about people’s characteristics, attitudes and interests. The technology then uses this data to generate adverts tailored to each user. Targeted social media advertising for consumer products Targeted adverts on social media are sometimes used by companies to suggest consumer products such as clothes, gadgets and food. These ads are targeted at people according to their personal characteristics and previous behaviour on social media. They are intended to encourage people to buy particular products. Targeted social media advertising for political parties Targeted adverts on social media are sometimes used by political parties to suggest political content to users. These ads are targeted at people according to their personal characteristics and previous behaviour on social media. They are intended to encourage people to support a specific political party. Virtual assistant technologies Virtual assistant technologies are devices or software that are designed to assist people with tasks like finding information online or helping to arrange appointments. The technologies can often respond to voice or text commands from a human. The technologies work by being ‘trained’ on lots of information about how people communicate through language, learning to match certain words and phrases to actions that they have been designed to carry out. Virtual assistant smart speakers One example of a virtual assistant technology is a smart speaker. These technologies are small computers that are connected to the internet and which can respond to voice commands to do things such as, turn appliances in the home on and off, answer questions about any topic, set reminders, or play music. Virtual assistants in healthcare One example of a virtual assistant is for assessing information about a person’s health. These AI technologies aim to respond to healthcare queries online, including about appointments or current symptoms. The technologies are able to automatically suggest a possible diagnosis or advise treatment. For more serious illnesses, the technologies may suggest a person seeks further medical advice, for example by booking a GP appointment or by going to hospital. Robotics Robotic technologies are computer-assisted machines which can interact with the physical world automatically, sometimes without the need for a human operator. These technologies use large amounts of data generated by machines, humans and sensors in the physical world to ‘learn to’ carry out tasks that would previously have been carried out by humans. Robotic vacuum cleaners One example of robotic technologies are robotic vacuum cleaners, sometimes called a ‘smart’ vacuum cleaner. This is a vacuum cleaner that can clean floors independently, without any human involvement. Robotic vacuum cleaners use sensors and motors to automatically move around a room while being able to detect obstacles, stairs and walls. Robotic care assistants One example of robotic technologies are robotic care assistants. These technologies are being developed to help carry out physical tasks in care settings such as hospitals and nursing homes. Robotic care assistants are designed to support specific tasks, such as helping patients with mobility issues to get in and out of bed, to pick up objects, or with personal tasks such as washing and dressing. When these technologies are used, a human care assistant will be on-call if needed. Driverless cars Another use of robotic technologies is for driverless cars. These are vehicles that are designed to travel on roads with other cars, lorries and vans, but which drive themselves automatically without needing a human driver. Driverless cars can detect obstacles, pedestrians, other drivers and road layouts by assessing their physical surroundings using sensors and comparing this information to large amounts of data about different driving environments. Autonomous weapons Another use of robotic technologies is for autonomous weapon systems used by the military. These include missile systems, drones and submarines that, once launched, can automatically identify, select or attack targets without further human intervention. These technologies decide when to act by assessing their physical surroundings using sensors and comparing this information to large amounts of data about different combat environments. Advancing knowledge through simulations New computer technologies are being developed to advance human knowledge about the past and the future. These technologies work by taking large amounts of data that we already have, and using this to create realistic simulations about how things were in the past, or how they might be in the future. These ‘simulation technologies’ aim to allow people to study and learn about places and events that would otherwise be impossible or difficult to directly experience. Climate change research One example of using new simulation technologies for advancing knowledge is for research about climate change. New simulation technologies can analyse large amounts of past data in order to simulate the future impacts of climate change in particular areas. This data could come from weather and environmental data, pollution data, and data on energy usage from individual homes. For example, these technologies can help scientists and governments to predict the likelihood of a significant flood occurring in a particular region over the next 10 years, along with how the flood may impact agriculture and health. Virtual reality for culture and education One example of using new simulation technologies for advancing knowledge is the development of virtual reality for education. Here, a person can wear a virtual reality headset at home or school that will show them a three-dimensional simulation of a museum or historical site, using a range of data about the museum or historical site. These technologies are designed to allow people to learn more about history or culture through games, videos and other immersive experiences. 6.2. Limitations While this study benefits from including a large, random probability sample representative of the population of Great Britain, the work is limited by several features which we address here. As discussed in the methodology, the sample size of our survey is not sufficiently large to provide robust estimates for different minority ethnic groups. We also do not have representation from Northern Ireland in our survey, meaning findings from this report cannot be generalised to the United Kingdom. We recognise the complexity of AI as a subject matter of our survey, and although we contextualised all the uses of AI we included in the survey, we were still not able to capture the granularity of some of these uses. For example, we asked about autonomous weapons in the broad sense, but acknowledge that attitudes may vary depending on whether they are framed as in use by participant’s own nation or other nations, or whether the system is for defensive or offensive uses. We also asked respondents about awareness and experience with uses of AI, but cannot gather from the survey alone what type of experience they have had with each technology. For example, in the case of AI to assess job eligibility, we do not know whether experience relates to using these services to recruit or to using these services when applying for a job. The list of concerns and benefits we presented respondents with, though grounded in literature surrounding AI, is also not exhaustive. While we left an open-text option for all benefit and concerns questions, very few respondents filled these in. It was important to keep these questions short due to time restrictions for the survey overall, and therefore the benefits and concerns presented in this report are not definitive across the uses of AI we surveyed. We asked generally about feelings towards the governance and regulation of AI technologies as a whole rather than for specific uses of AI. As discussed in the report, AI is complex and difficult to define, and our findings show that attitudes towards AI are nuanced and vary depending on the application of AI. Future research should look at public attitudes to regulating specific technologies. Finally, although we used both online and offline methods, we recognise that we still may not have reached those that are truly digitally excluded in Great Britain, those with restrictions on their leisure time, and those with additional requirements that may have made participating in this survey challenging. Therefore the ability to generalise our findings is limited. Overall, we acknowledge that a survey alone cannot be a perfect representation of public attitudes. Attitudes may change depending on time and context and include trade-offs across different groups in the population and across different technologies that are difficult to explore using this method. This survey was designed and in the field in November 2022, just before generative AI and large language models like ChatGPT became a widely covered media topic. It is probable that these advances have already impacted public discourse towards some AI technologies since our survey. Therefore there is a need for rich qualitative research to follow up the insights we have presented here. 6.3. Analysis and additional tables In this section we provide more detail on the analysis conducted to understand in which cases perceived benefits of each AI use outweighed concerns and vice versa (Section 6.3.1.), and the regression analysis conducted to understand differences on the extent to which different groups are more likely to see technologies as more or less beneficial (Section 6.3.2.). Section 6.3.3. includes detail on the type of analysis conducted to derive some of the attitudinal variables used. Section 6.3.4. provides tables with the full list of specific benefits and concerns, and the percentage of respondents selecting these, for all 17 of the technologies included in the survey. 6.3.1. Net benefit analysis A mean net benefit score was calculated for each technology by subtracting the benefit score from the concern score. When net benefit scores were negative, concern outweighed benefit. When scores were positive, benefit outweighed concern. Scores of zero indicated equal concern and benefit. The benefit and concern variables were coded in the following ways: To what extent do you think that the use of [AI technology] will be beneficial? ‘Very beneficial’ was re-coded as 3 ‘Fairly beneficial’ was 2 ‘Not very beneficial’ was 1 ‘Not at all beneficial’ was 0. To what extent are you concerned about the use of [AI technology]? ‘Very concerned’ was re-coded as 3 ‘Somewhat concerned’ was 2 ‘Not very concerned’ was 1 ‘Not at all concerned’ was 0. ‘Prefer not to say’ and ‘don’t know’ options were re-coded as missing values. 6.3.2. Regression analysis To understand how demographics and attitudinal variables are related to the perceived net benefits of AI, we fitted linear regression models for each individual AI technology using the same set of predictor variables. The dependent variable in each model is ‘net benefit’, calculated as described above. The independent variables in the models were: Age (65 and older compared to younger than 65), this coding is chosen because it represents the main age difference across AI uses) Sex (male compared to female) Education (having a degree compared to not having a degree) Social class (NS-SEC 1-3 compared to NS-SEC 4-7) Awareness of the technology (aware compared to not aware) Experience with using the technology (experience compared to no experience) Tech interested (self-reported interest in technology) Tech informed (self-reported informedness about technology) Digital literacy (high compared to low) Comfort with technology (high compared to low) Figure 12 presents the results for all 17 regressions in a single plot. Each square in the plot represents the expected change in net benefit for a unit increase in the corresponding independent variable on the vertical axis, controlling for all other variables included in the model. Statistically significant coefficients (p < 0.05) are shown in pink, while green coefficients denote non-significant coefficients. Coefficient estimates higher than 0 indicate a higher net benefit and conversely coefficients lower than 0 are associated with lower net benefit (or higher concern) on a particular variable. Taking the age variable as an example, people aged over 65 are significantly more likely to see simulation in climate change, predicting cancer risk, all three facial recognition technologies and autonomous weapons as net beneficial. On the other hand, this age group is significantly more likely to see consumer and political social media advertising, job eligibility and driverless cars as net concerning. There are no significant differences between age groups for the remaining AI uses. Figure 12 illustrates how patterns of perceived net benefit vary substantially across demographic groups and attitudinal indicators. Only ‘comfort with technology’ shows a consistent relationship, with people who are more comfortable with technology significantly more likely to see net benefits across all 17 AI uses. Being a graduate, on the other hand, is associated with expressing net concerns on most AI uses, although several are non-significant and one is in the opposite direction (graduates are more likely to see autonomous weapons as net beneficial). Sex shows a near equal mix of positive, negative and non-significant associations across use cases. These results reinforce the conclusion from the descriptive analyses; public perceptions of AI are complex and highly nuanced, varying according to the specific technology and the context in which it is used. 6.3.3. Principal component analysis The independent variables ‘digital literacy’ and ‘comfort with technology’ are summary measures of multiple items produced using principal component analysis. The ‘digital literacy’ measure is based on eight survey questions each covering the level of confidence in different information technology skills, ranging from using the internet for finding information to setting up an online account to buy goods (see Table 13). ‘Comfort with technology’ is a measure derived from seven questions which cover attitudes towards new technologies and their impact on society, for example, whether the respondent finds it easy to keep up with new technologies or whether AI is making society better (see Table 14). The summary score for each measure is taken as the first principal component in a principal component analysis. Tables 15 and 16 include the factor loadings for each measure from the principal component analysis. Table 13: Digital literacy scale (response options 1-4 recoded from least to most confident) Variable name Question wording DIG_LIT_1 Use the internet to find information that helps you solve problems DIG_LIT_2 Attach documents to an email and share it DIG_LIT_3 Create documents using word processing applications (e.g. a CV or a letter) DIG_LIT_4 Set up an email account DIG_LIT_5 Organise information and content using files and folders (either on a device, across multiple devices, or on the Cloud) DIG_LIT_6 Recognise and avoid suspicious links in emails, websites, social media messages and pop-ups DIG_LIT_7 Pay for things online DIG_LIT_8 Set up an online account that enables you to buy goods and services (e.g. Amazon account, eBay, John Lewis) Table 14: ‘Comfort with technology’ scale (response options scale 1-11, using a slider question approach) Variable name Question wording TECHSELF_1 TECHSELF. Do not seek out new technologies or gadgets…When new technologies or gadgets are introduced, like to try them’ TECHSELF_2 TECHSELF. Overall, new technologies make quality of life worse…Overall, new technologies improve quality of life’ TECHSELF_3 TECHSELF. Find it difficult to keep up to date with new technologies…Find it easy to keep up to date with new technologies TECHSELF_4 TECHSELF. Do not like my online activity being tracked…Fine with my online activity being tracked TECHSELF_5 TECHSELF. So long as the technology works, don’t need to know how it works…Knowing how new technologies work is important TECHSOCIAL_1 Are changing society too quickly…Are changing society at a good pace TECHSOCIAL_2 Are making society worse…Are making society better Table 15: Digital literacy: Principal Component Analysis Factor Loadings Variable name Component 1 Component 2 DIG_LIT_1 0.7861 0.2621 DIG_LIT_2 0.8691 -0.1814 DIG_LIT_3 0.8126 -0.4233 DIG_LIT_4 0.8343 -0.0224 DIG_LIT_5 0.8112 -0.3837 DIG_LIT_6 0.7034 0.2331 DIG_LIT_7 0.8126 0.3514 DIG_LIT_8 0.8609 0.2043 Table 16: Comfort with technology: Principal Component Analysis Factor Loadings Variable name Component 1 Component 2 TECHSELF_1 0.8297 0.3404 TECHSELF_2 0.8210 0.0611 TECHSELF_3 0.8357 0.3023 TECHSELF_4 0.5226 -0.4599 TECHSELF_5 0.6570 0.4723 TECHSOCIAL_1 0.7584 -0.4245 TECHSOCIAL_2 0.7234 -0.4607 6.3.4. Full list of specific benefits and concerns chosen for each technology Table 17: Full list of specific benefits and percentage of respondents selecting for all 17 technologies Technology Benefit option Percentage selecting Cancer risk prediction The technology will enable earlier detection of cancer, allowing earlier monitoring or treatment 82% There will be less human error when predicting people’s risk of developing cancer 53% The technology will be more accurate than a human doctor at predicting the risk of developing cancer 42% The technology will reduce discrimination in healthcare 32% People’s personal information will be more safe and secure 11% Something else (please specify) 2% None of these 3% Don’t know 6% Job eligibility Reviewing applications will be faster and easier for employers and recruiters 49% The technology will be more accurate than employers and recruiters at reviewing applications 13% There will be less human error in determining eligibility for a job 22% The technology will be less likely than employers and recruiters to discriminate against some groups of people in society 41% The technology will save money usually spent on human resources 32% People’s personal information will be more safe and secure 10% Something else (please specify) 1% None of these 13% Don’t know 10% Loan repayment risk Applying for a loan will be faster and easier 52% The technology will be more accurate than banking professionals at predicting the risk of repaying a loan 29% There will be less human error in decisions 37% The technology will be less likely than banking professionals to discriminate against some groups of people in society 39% The technology will save money usually spent on human resources 31% People’s personal information will be more safe and secure 11% Something else (please specify) 0% None of these 8% Don’t know 12% Prefer not to say 0% Welfare eligibility Determining eligibility for benefits will be faster and easier 43% The technology will be more accurate than fare officers at determining eligibility for benefits 22% There will be less human error in determining eligibility for benefits 38% The technology will be less likely than fare officers to discriminate against some groups of people in society 37% The technology will save money usually spent on human resources 35% People’s personal information will be more safe and secure 14% Something else (please specify) 0% None of these 12% Don’t know 14% Facial recognition at border control Processing people at border control will be faster 70% People will not have to answer personal questions sometimes asked by border control officers 32% The technology will be more accurate than border control officers at detecting people who do not have the right to enter 50% The technology will be less likely than border control officers to discriminate against some groups of people in society 40% People’s personal information will be more safe and secure 18% The technology will save money usually spent on human resources 42% Something else (please specify) 1% None of these 4% Don’t know 3% Facial recognition for mobile phone unlocking It is faster to unlock a phone or personal device 61% People’s personal information will be more safe and secure 53% Something else (please specify) 1% None of these 8% Don’t know 6% Facial recognition for policing and surveillance The technology will make it faster and easier to identify wanted criminals and missing persons 77% The technology will be more accurate than police officers and staff at identifying wanted criminals and missing persons 55% The technology will be less likely than police officers and staff to discriminate against some groups of people when identifying criminal suspects 41% The technology will save money usually spent on human resources 46% People’s personal information will be more safe and secure 11% Something else (please specify) 0% None of these 3% Don’t know 4% Autonomous weapons The technologies will enable faster military response to threats 50% The technologies will preserve the lives of some soldiers 54% The technologies will be more accurate than human soldiers at identifying targets 34% The technologies will be less likely than human soldiers to target people based on particular characteristics 26% The technologies will lead to fewer civilians being harmed or killed 36% The technology with save money usually spent on human resources 22% Something else (please specify) 1% None of these 9% Don’t know 15% Prefer not to say 0% Driverless cars It will make travel by car easier 30% It will free up time to do other things while driving like working, sleeping or watching a movie 30% Driverless cars will drive with more accuracy and precision than human drivers 32% Driverless cars will be less likely to cause accidents than human drivers 32% It will make travel by car easier for disabled people or for people who have difficulty driving 63% The technology will save money usually spent on human drivers 19% Something else (please specify) 1% None of these 17% Don’t know 6% Robotic care assistant The technology will make caregiving tasks easier and faster 47% The technology will be able to do tasks such as lifting patients out of bed more accurately than caregiving professionals 45% The technology will be less likely than caregiving professionals to discriminate against some grou of people in society 37% The technology will save money usually spent on human resources 34% Something else (please specify) 0% None of these 12% Don’t know 11% Will benefit the care workers 0% Robotic vacuum cleaner The technology will do the vacuuming, saving people time 68% The technology will be more accurate than a human at vacuuming 12% It will make vacuuming possible for people who have difficulty doing manual tasks 84% Something else (please specify) 1% None of these 3% Don’t know 3% Smart speaker The technology will allow people to carry out tasks faster and more easily 60% The technology will allow people with difficulty using devices to access features more easily 71% People’s personal information will be more safe and secure 5% People will be able to find information more accurately 39% Something else (please specify) 0% None of these 7% Don’t know 6% Virtual healthcare assistant It is a faster way for people to get help for their health and symptoms than speaking to a healthcare professional 50% The technology will be more accurate than a healthcare professional at suggesting a diagnosis and treatment options 13% The technology will be less likely than healthcare professionals to discriminate against some groups of people in society 31% The technology will be easier for some groups of people in society to use, such as those who have difficulty leaving their home 53% The technology will save money usually spent on human resources 35% People’s personal information will be more safe and secure 8% Something else (please specify) 1% None of these 9% Don’t know 9% Targeted online consumer ads People will be able to find products online faster and more easily 39% The adverts people see online will be more relevant to them than adverts that are not targeted 53% It will help people discover new products that might be of interest to them 50% Something else (please specify) 0% None of these 17% Don’t know 3% Targeted online political ads People will be able to find political information online faster and more easily 35% The political adverts that people see online will be more relevant to them than political adverts that are not targeted 32% It will help people discover new political representatives who might be of interest to them 33% It will increase the diversity of political perspectives that people engage with 22% Something else (please specify) 0% None of these 22% Don’t know 12% Simulations for climate change research The technology will be more accurate than scientists and government researchers alone at predicting climate change effects 41% The technology will make it faster and easier for scientists and governments to predict climate change effects 64% The technology will predict issues across a wider range of regions and countries, meaning more people will experience the benefits of climate research 64% This technology will allow more people to understand the possible effects of climate change 63% Something else (please specify) 1% None of these 6% Don’t know 12% Simulations for education People will gain a more accurate understanding of historical events and how people lived in the past 57% The technology will make it easier and faster to learn about history and culture 58% The technology will increase the quality of education by providing more immersive experiences 66% The technology will allow more people to learn about history and culture 60% Something else (please specify) 1% None of these 6% Don’t know 10% Prefer not to say 0% Table 18: Full list of specific concerns and percentage of respondents selecting for all 17 technologies Technology Concern option Percentage selecting Cancer risk prediction The technology will be unreliable and cause delays to predicting a risk of cancer 17% The technology will gather personal information which could be shared with third parties 24% People’s personal information will be less safe and secure 13% The technology will not be as accurate as a human doctor at predicting the risk of developing cancer 19% The technology will be less effective for some groups of people in society than others, leading to more discrimination in healthcare 17% Doctors will rely too heavily on the technology rather than their professional judgements 56% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 47% It will be more difficult to understand how decisions about potential health outcomes are reached 32% Something else (please specify) 1% None of these 10% Don’t know 7% Job eligibility The technology will be unreliable and cause delays to assessing job applications 19% The technology will not be as accurate as employers and recruiters at reviewing job applications 39% The technology will be less able than employers and recruiters to take account of individual circumstances 61% The technology will be more likely than employers and recruiters to discriminate against some groups of people in society 19% The technology will gather personal information which could be shared with third parties 32% People’s personal information will be less safe and secure 19% It will lead to job cuts. For example, for trained recruitment staff 34% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 40% Employers and recruiters will rely too heavily on the technology rather than their professional judgements 64% It will be more difficult to understand how decisions about job application assessments are reached 52% Something else (please specify) 1% None of these 3% Don’t know 7% Loan repayment risk The technology will be unreliable and cause delays to assessing loan applications 18% The technology will gather personal information which could be shared with third parties 37% People’s personal information will be less safe and secure 21% Banking professionals may rely too heavily on the technology rather than their professional judgements 51% The technology will not be as accurate as banking professionals at predicting the risk of repaying a loan 21% The technology will be more likely than banking professionals to discriminate against some groups of people in society 16% It will be more difficult to understand how decisions about loan applications are reached 49% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 43% It will lead to job cuts. For example, for trained banking professionals 33% The technology will be less able than banking professionals to take account of individual circumstances 52% Something else (please specify) 1% None of these 4% Don’t know 8% Prefer not to say 0% Welfare eligibility The technology will be unreliable and will cause delays to allocating benefits 24% The technology will not be as accurate as welfare officers at determining eligibility for benefits 29% The technology will be more likely than welfare officers to discriminate against some groups of people in society 13% The technology will gather personal information which could be shared with third parties 32% People’s personal information will be less safe and secure 19% It will lead to job cuts. For example, for trained welfare officers 35% It will be more difficult to understand how decisions about allocating benefits are reached 45% Welfare officers will rely too heavily on the technology rather than their professional judgements 47% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 47% The technology will be less able than welfare officers to take account of individual circumstances 55% Something else (please specify) 1% None of these 5% Don’t know 10% Prefer not to say 0% Facial recognition at border control The technology will be unreliable and cause delays when it breaks down 44% The technology will not be as accurate as border control officers at detecting people who do not have the right to enter 20% The technology will gather personal information which could be shared with third parties 29% People’s personal information will be less safe and secure 15% The technology will be more likely than border control officers to discriminate against some groups of people in society 10% Border control officers will rely too heavily on the technology rather than their professional judgements 41% Some people might find it difficult to use the technology 42% It will lead to job cuts. For example, for trained border control officers 47% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 47% It will be more difficult to understand how decisions are reached 26% Something else (please specify) 1% None of these 6% Don’t know 4% Facial recognition for mobile phone unlocking The technology will be unreliable, making it take longer to unlock your phone or personal device 21% The technology will gather personal information which could be shared with third parties 40% The technology will make it easier for other people to unlock your phone or personal device 23% People’s personal information will be more safe and secure 19% Some people may find it difficult to use the technology 41% The technology will be less effective for some groups of people in society than others 33% Something else (please specify) 1% None of these 12% Don’t know 3% Facial recognition for policing and surveillance The technology will be unreliable and will cause delays identifying wanted criminals and missing persons 15% The technology will not be as accurate as police officers and staff at identifying wanted criminals and missing persons 13% If the technology makes a mistake it will lead to innocent people being wrongly accused 54% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 48% The technology will be more likely than police officers and staff to discriminate against some groups of people in society 15% The technology will gather personal information which could be shared with third parties 38% People’s personal information will be less safe and secure 21% It will lead to job cuts. For example, for trained police officers and staff 30% Police officers and staff will rely too heavily on the technology rather than their professional judgements 46% Something else (please specify) 1% None of these 8% Don’t know 4% Autonomous weapons The technologies will be unreliable and may miss or not fire at targets 41% The technologies will lead to more civilians being harmed or killed 33% The technologies will not be as accurate at identifying targets as human soldiers 29% The technologies will be more likely than human soldiers to target people based on particular characteristics 22% Defence staff will rely too heavily on the technologies rather than their professional judgements 54% It will lead to job cuts. For example, for trained defence staff 25% If the technologies make a mistake, it will be difficult to know who is responsible for what went wrong 53% It is more difficult to understand how military decisions are reached 39% The technologies will lead to more soldiers being harmed or killed 21% Something else (please specify) 2% None of these 5% Don’t know 11% Prefer not to say 0% Driverless cars The technology will not always work, making the cars unreliable 62% Getting to places will take longer as the cars will be overly cautious 25% Driverless cars will not be as accurate or precise as humans are at driving 38% The technology will gather personal information which could be shared with third parties 22% The technology will be less effective for some groups of people in society than others 26% Some people may find it difficult to use the technology 46% It will lead to job cuts. For example, for truck drivers, taxi drivers, delivery drivers 44% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 59% It will be more difficult to understand how the car makes decisions compared to a human driver 51% Driverless cars will be more likely to cause accidents than human drivers 36% Something else (please specify) 2% None of these 4% Don’t know 2% Prefer not to say 0% Robotic care assistant The technology will be unreliable and cause delays to urgent caregiving tasks 34% The technology will not be able do tasks such as lifting patients out of bed as accurately as caregiving professionals 37% The technology will be less effective for some groups of people in society than others 33% It will lead to job cuts. For example, for trained caregiving professionals 46% The technology will not be safe, it could hurt people 41% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 45% The technology will gather personal information which could be shared with third parties 20% Patients will miss out on the human interaction they would otherwise get from human carers 78% Something else (please specify) 1% None of these 3% Don’t know 7% Prefer not to say 0% Technology may miss subtle signs when assisting patients 1% Robotic vacuum cleaner The technology will be unreliable and not always work, for example, the motion sensors will not detect steps or surface change 45% The technology will not be as accurate as a human at vacuuming 42% The technology will be a safety hazard, you might trip on them 40% The technology will gather personal information which could be shared with third parties 12% People’s personal data will be less safe and secure 9% Some people may find it difficult to use the technology 39% The technology will be less effective for some groups of people in society than others 18% Something else (please specify) 1% None of these 14% Don’t know 5% Smart speaker The technology will be unreliable and cause delays to doing tasks 18% The technology will not always give accurate responses 51% The technology will be less effective for some groups of people in society than others 32% Some people may find it difficult to use the technology 44% The technology will gather personal information which could be shared with third parties 57% People’s personal information will be less safe and secure 41% Something else (please specify) 0% None of these 7% Don’t know 6% Virtual healthcare assistant The technology will be unreliable and cause delays to getting help 31% The technology will not be as accurate as a healthcare professional at suggesting a diagnosis and treatment options 51% The technology will be less able than healthcare professionals to take account of individual circumstances 63% The technology will be less effective for some groups of people in society than others 38% Some people may find it difficult to use the technology 64% The technology will gather personal information which could be shared with third parties 35% People’s personal information will be less safe and secure 24% It will lead to job cuts. For example, for trained healthcare professionals 38% If the technology makes a mistake, it will be difficult to know who is responsible for what went wrong 49% It will be more difficult to understand how decisions about diagnoses and treatments are reached 47% Something else (please specify) 2% None of these 2% Don’t know 6% Targeted online consumer ads The technology will be inaccurate and will show people adverts that are not relevant to them 29% The technology will gather personal information which could be shared with third parties 68% People’s personal information will be less safe and secure 50% The technology invades people’s privacy 69% Something else (please specify) 2% None of these 6% Don’t know 4% Targeted online political ads The technology will be inaccurate and will show people political adverts that are not relevant to them 33% The technology will gather personal information which could be shared with third parties 48% People’s personal information will be less safe and secure 29% It will reduce the diversity of political perspectives that people engage with 46% The technology invades people’s privacy 51% Something else (please specify) 1% None of these 5% Don’t know 9% Simulations for climate change research The technology will be unreliable, making it harder to predict the impacts of climate change and extreme weather 17% The technology will not be as accurate as scientists and government researchers alone at predicting climate change events 21% The technology will gather personal information which could be shared with third parties 13% The technology will predict issues in some regions better than others, meaning that some people do not experience the benefits of these technologies 36% Something else (please specify) 1% None of these 26% Don’t know 18% Simulations for education research Some people will not be able to learn about history and culture in this way as they will not have access to the technology 51% People will gain a less accurate understanding of historical events and how people lived in the past 17% The technology will gather personal information which could be shared with third parties 18% The technology will be unreliable, making it harder to learn about history and culture 11% The technology will allow those developing the technology to control what people learn about history or culture 46% Something else (please specify) 1% None of these 15% Don’t know 11% Prefer not to say 0% 6.4. Sample sizes Table 19: Weighted and unweighted sample size of respondents for each technology Technology Unweighted sample size Weighted sample size Facial recognition – Unlocking mobile phones 4,010 4,002 Facial recognition – Police surveillance 1,993 1,987 Facial recognition – Border control 2,017 2,015 Risk and eligibility – Welfare 2,015 2,012 Risk and eligibility – Loan repayment 1,999 1,991 Risk and eligibility – Job eligibility 1,995 1,990 Risk and eligibility – Cancer risk 2,011 2,011 Smart speaker – Virtual assistant 2,028 2,011 Smart speaker – Virtual healthcare assistant 1,982 1,991 Robotics – Robotic care assistant 1,985 1,973 Robotics – Robotic vacuum cleaner 2,025 2,029 Robotics – Driverless cars 1,992 2,021 Robotics – Autonomous weapons 2,018 1,981 Social media targeted advertising – Consumer ads 2,010 2,002 Social media targeted advertising – Political ads 2,000 2,000 Simulations – Climate change 2,036 2,015 Simulations – Education 1,974 1,987 Table 20: Weighted and unweighted sample size of respondents by various socio-demographic variables Demographic Unweighted sample size Weighted sample size Survey format Online 3,757 3,647 Telephone 253 355 Region England 3,520 3,461 Scotland 303 345 Wales 187 196 Age band 18–24 years 341 408 25–34 years 709 682 35–44 years 741 654 45–54 years 692 666 55–64 years 696 645 65–74 years 513 517 75+ years 318 431 Socio-economic status SEC1, 2 1,642 1,477 SEC3 555 494 SEC4 262 298 SEC5 165 173 SEC6, 7 634 664 SEC8 122 145 Students 201 209 NA 429 543 Education level Degree level qualification(s) 1,562 1,407 No academic or vocational qualifications 283 443 Non-degree level qualifications 2,155 2,139 NA 10 13 Ethnic group Asian or Asian British 261 296 Black British, Caribbean or African 90 103 White 3,544 3,476 Any other ethnic group 103 116 NA 12 12 Sex Female 2,096 2,037 Male 1,911 1,961 NA 3 4.4 Partner information and acknowledgements This report was co-authored by The Alan Turing Institute (Professor Helen Margetts, Dr Florence Enock, Miranda Cross) and the Ada Lovelace Institute (Aidan Peppin, Roshni Modhvadia, Anna Colom, Andrew Strait, Octavia Reeve) with substantial input from LSE’s Methodology Department (Professor Patrick Sturgis, Katya Kostadintcheva, Oriol Bosch-Jover). We’d like to also thank Kantar for their contributions in designing the survey and collecting the data. This project was made possible by a grant from The Alan Turing Institute and the Arts and Humanities Research Council (AHRC). About The Alan Turing Institute The Alan Turing Institute is the national institute for data science and artificial intelligence (AI). Established in 2015, we are named in honour of Alan Turing, whose pioneering work in theoretical and applied mathematics, engineering and computing laid the foundations for the modern-day fields of data science and AI. Headquartered at the British Library in London, we partner with organisations across government, industry, academia and the third sector to undertake world-class research that benefits society. Footnotes [1] Kantar, ‘Technical Report: How Do People Feel about AI?’ (GitHub, Ada Lovelace Institute 2023) <https://github.com/AdaLovelaceInstitute> [2] Centre for Data Ethics and Innovation, ‘Public Attitudes to Data and AI: Tracker Survey (Wave 2)’ (2022) <https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-2> [3] The Royal Society and Ipsos MORI, ‘Public Views of Machine Learning’ (2017) <https://royalsociety.org/topics-policy/projects/machine-learning> [4] ibid. [5] BEIS, ‘Public Attitudes to Science’ (Department for Business, Energy and Industrial Strategy/Kantar Public 2019) <https://www.kantar.com/uk-public-attitudes-to-science> [6] Centre for Data Ethics and Innovation (n 1). [7] Ada Lovelace Institute, ‘Beyond Face Value: Public Attitudes to Facial Recognition Technology’ (2019) <https://www.adalovelaceinstitute.org/report/beyond-face-value-public-attitudes-to-facial-recognition-technology> [8] Baobao Zhang, ‘Public Opinion Toward Artificial Intelligence’ (Open Science Framework, 2021) preprint <https://osf.io/284sm>. [9] European Commission. Directorate General for Communication. Citizens’ Knowledge, Perceptions, Values and Expectations of Science. (2021) <https://data.europa.eu/doi/10.2775/071577>. [10] BEIS (n 5). [11] Centre for Data Ethics and Innovation (n 2) 2; The Royal Society and Ipsos MORI (n 3). [12] Lee Rainie and others, ‘AI and Human Enhancement: Americans´Openness Is Tempered by a Range of Concerns’ (Pew Research Center, 2022) <https://www.pewresearch.org/internet/2022/03/17/how-americans-think-about-artificial-intelligence> [13] BEIS (n 5). [14] Sabine N van der Veer and others, ‘Trading off Accuracy and Explainability in AI Decision-Making: Findings from 2 Citizens’ Juries’ (2021) 28 Journal of the American Medical Informatics Association 2128 <https://academic.oup.com/jamia/article/28/10/2128/6333351> [15] The Alan Turing Institute, ‘Project ExplAIN’ (2023) <https://www.turing.ac.uk/news/project-explain>. [16] Kantar (n 1). [17] ibid. [18] Kantar, ‘Public Voice’ (2022) <https://www.kantar.com/uki/expertise/policy-society/public-evidence/public-voice>. [19] The technical report specifies a total of 4,012 but one 16-year-old was removed from the dataset as the survey was for adults aged 18+, while another provided their sex as ‘other’ so was removed on account of being the only participant identifying in this way and therefore having a very large weighting. Further information available in the limitations section. [20] While participants indicated more specific ethnic identities at the time of recruitment to the Public Voice panel, we combine them into these broader categories for providing an overview of the sample. [21] Kantar (n 1). [22] Ada Lovelace Institute (n 1). [23] Ada Lovelace Institute (n 1). [24] Lina Dencik and others, ‘Data Scores as Governance: Investigating Uses of Citizen Scoring in Public Services’ (Data Justice Lab, 2018) https://datajusticelab.org/data-scores-as-governance. [25] Britain Thinks and CDEI, ‘AI Governance’ (2022) <https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance>. [26] Centre for Data Ethics and Innovation (n 2). [27] The Royal Society and Ipsos MORI (n 3). [28] BEIS, ‘BEIS Public Attitudes Tracker: Artificial Intelligence Summer 2022, UK’ (Department for Business, Energy and Industrial Strategy 2022) <https://www.gov.uk/government/statistics/beis-public-attitudes-tracker-summer-2022>. [29] Ada Lovelace Institute (n 7). [30] Ada Lovelace Institute, ‘Who Cares What the Public Think?’ (2022) <https://www.adalovelaceinstitute.org/evidence-review/public-attitudes-data-regulation/> accessed 12 December 2022. [31] Britain Thinks and CDEI (n 25). [32] Ada Lovelace Institute (n 7). [33] BEIS (n 5). [34] Centre for Data Ethics and Innovation (n 2). [35] European Interactive Digital Advertising Alliance, ‘Your Online Voices’ (2022) <https://edaa.eu/your-online-voices-your-voice-your-choice> [36] For which both ‘inaccuracy; and ‘unclear how decisions are made’ were potential given concerns to choose from. [37] Jonathan Dupont, Seb Wride and Vinous Ali, ‘What Does the Public Think about AI?’ (Public First 2023) <https://publicfirst.co.uk/ai/> [38] Florence Enock and others, ‘Tracking Experiences of Online Harms and Attitudes Towards Online Safety Interventions: Findings from a Large-Scale, Nationally Representative Survey of the British Public’ (2023)] SSRN Electronic Journal <https://www.ssrn.com/abstract=4416355> [39] Centre for Data Ethics and Innovation (n 2). Partner The Alan Turing Institute Author Roshni Modhvadia Related content Event How do people feel about AI? A nationally representative survey of the British public Virtual event 2:00pm – 3:00pm, 6 June 2023 AI policy Public attitudes Completed project How do people feel about AI? A nationally representative survey of the British public. Public attitudes Public Participation & Research ... Public trust Evidence review Who cares what the public think? Aidan Peppin UK public attitudes to regulating data and data-driven technologies 5 May 2022 Data governance Evidence review ... Public attitudes Public Participation & Research Public trust Social value of data Sign up for updates The Ada Lovelace Institute is an independent research institute with a mission to ensure data and AI work for people and society. Subscribe to our newsletter You can opt out at any time through a link on the bottom of the newsletter email. Read our privacy policy for more information. Main menu About Our people Jobs Who we work with Impact Contact Our work Research domains Projects Library Ada in Europe Blog News & events News Events Contact us 100 St John Street London, EC1M 4EH +44 (0)207 631 0566 Email us Press office +44 (0)207 323 6274 Part of the Nuffield Foundation Contents of this website are shared under CC-BY 4.0 license, unless stated otherwise. This means you can share and adapt content freely, as long as you credit the Ada Lovelace Institute and named authors. Quick Links Terms & conditions Digital best practice Sitemap © 2024 Nuffield Foundation Design and development by Soapbox Connect with us"
"University Students&rsquo; Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes",https://www.mdpi.com/2227-7102/14/9/988,未知,2024-11-21 17:04:17,en,AI AND attitude,google,"University Students&rsquo; Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes Next Article in Journal South African Teachers’ Insights on Improving the Sensory Classroom Teacher Questionnaire (SCTQ) for Inclusive Education and ADHD Support Previous Article in Journal Beyond the Classroom: Integrating the ORID Model for In-Depth Reflection and Assessment in Service-Learning Journals Active Journals Find a Journal Journal Proposal Proceedings Series Topics Information For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers Open Access Policy Institutional Open Access Program Special Issues Guidelines Editorial Process Research and Publication Ethics Article Processing Charges Awards Testimonials Editing Services Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series About Overview Contact Careers News Press Blog Sign In / Sign Up Notice You can make submissions to other journals here. clear Notice You are accessing a machine-readable page. In order to be human-readable, please install an RSS reader. Continue Cancel clear All articles published by MDPI are made immediately available worldwide under an open access license. No special permission is required to reuse all or part of the article published by MDPI, including figures and tables. For articles published under an open access Creative Common CC BY license, any part of the article may be reused without permission provided that the original article is clearly cited. For more information, please refer to https://www.mdpi.com/openaccess. Feature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications. Feature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers. Editor’s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal. Original Submission Date Received: . <div id=""no-javascript""> You seem to have javascript disabled. Please note that many of the page functionalities won't work as expected without javascript enabled. </div> clear zoom_out_map search menu Journals Active Journals Find a Journal Journal Proposal Proceedings Series Topics Information For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers Open Access Policy Institutional Open Access Program Special Issues Guidelines Editorial Process Research and Publication Ethics Article Processing Charges Awards Testimonials Editing Services Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series About Overview Contact Careers News Press Blog Sign In / Sign Up Submit Search for Articles: Title / Keyword Author / Affiliation / Email Journal All Journals Acoustics Acta Microbiologica Hellenica (AMH) Actuators Administrative Sciences Adolescents Advances in Respiratory Medicine (ARM) Aerobiology Aerospace Agriculture AgriEngineering Agrochemicals Agronomy AI Air Algorithms Allergies Alloys Analytica Analytics Anatomia Anesthesia Research Animals Antibiotics Antibodies Antioxidants Applied Biosciences Applied Mechanics Applied Microbiology Applied Nano Applied Sciences Applied System Innovation (ASI) AppliedChem AppliedMath Aquaculture Journal Architecture Arthropoda Arts Astronomy Atmosphere Atoms Audiology Research Automation Axioms Bacteria Batteries Behavioral Sciences Beverages Big Data and Cognitive Computing (BDCC) BioChem Bioengineering Biologics Biology Biology and Life Sciences Forum Biomass Biomechanics BioMed Biomedicines BioMedInformatics Biomimetics Biomolecules Biophysica Biosensors BioTech Birds Blockchains Brain Sciences Buildings Businesses C Cancers Cardiogenetics Catalysts Cells Ceramics Challenges ChemEngineering Chemistry Chemistry Proceedings Chemosensors Children Chips CivilEng Clean Technologies (Clean Technol.) Climate Clinical and Translational Neuroscience (CTN) Clinical Bioenergetics Clinics and Practice Clocks & Sleep Coasts Coatings Colloids and Interfaces Colorants Commodities Complications Compounds Computation Computer Sciences & Mathematics Forum Computers Condensed Matter Conservation Construction Materials Corrosion and Materials Degradation (CMD) Cosmetics COVID Crops Cryo Cryptography Crystals Current Issues in Molecular Biology (CIMB) Current Oncology Dairy Data Dentistry Journal Dermato Dermatopathology Designs Diabetology Diagnostics Dietetics Digital Disabilities Diseases Diversity DNA Drones Drugs and Drug Candidates (DDC) Dynamics Earth Ecologies Econometrics Economies Education Sciences Electricity Electrochem Electronic Materials Electronics Emergency Care and Medicine Encyclopedia Endocrines Energies Energy Storage and Applications (ESA) Eng Engineering Proceedings Entropy Environmental Sciences Proceedings Environments Epidemiologia Epigenomes European Burn Journal (EBJ) European Journal of Investigation in Health, Psychology and Education (EJIHPE) Fermentation Fibers FinTech Fire Fishes Fluids Foods Forecasting Forensic Sciences Forests Fossil Studies Foundations Fractal and Fractional (Fractal Fract) Fuels Future Future Internet Future Pharmacology Future Transportation Galaxies Games Gases Gastroenterology Insights Gastrointestinal Disorders Gastronomy Gels Genealogy Genes Geographies GeoHazards Geomatics Geometry Geosciences Geotechnics Geriatrics Glacies Gout, Urate, and Crystal Deposition Disease (GUCDD) Grasses Hardware Healthcare Hearts Hemato Hematology Reports Heritage Histories Horticulturae Hospitals Humanities Humans Hydrobiology Hydrogen Hydrology Hygiene Immuno Infectious Disease Reports Informatics Information Infrastructures Inorganics Insects Instruments Intelligent Infrastructure and Construction International Journal of Environmental Research and Public Health (IJERPH) International Journal of Financial Studies (IJFS) International Journal of Molecular Sciences (IJMS) International Journal of Neonatal Screening (IJNS) International Journal of Plant Biology (IJPB) International Journal of Topology International Journal of Translational Medicine (IJTM) International Journal of Turbomachinery, Propulsion and Power (IJTPP) International Medical Education (IME) Inventions IoT ISPRS International Journal of Geo-Information (IJGI) J Journal of Ageing and Longevity (JAL) Journal of Cardiovascular Development and Disease (JCDD) Journal of Clinical & Translational Ophthalmology (JCTO) Journal of Clinical Medicine (JCM) Journal of Composites Science (J. Compos. Sci.) Journal of Cybersecurity and Privacy (JCP) Journal of Dementia and Alzheimer's Disease (JDAD) Journal of Developmental Biology (JDB) Journal of Experimental and Theoretical Analyses (JETA) Journal of Functional Biomaterials (JFB) Journal of Functional Morphology and Kinesiology (JFMK) Journal of Fungi (JoF) Journal of Imaging (J. Imaging) Journal of Intelligence (J. Intell.) Journal of Low Power Electronics and Applications (JLPEA) Journal of Manufacturing and Materials Processing (JMMP) Journal of Marine Science and Engineering (JMSE) Journal of Market Access & Health Policy (JMAHP) Journal of Molecular Pathology (JMP) Journal of Nanotheranostics (JNT) Journal of Nuclear Engineering (JNE) Journal of Otorhinolaryngology, Hearing and Balance Medicine (JOHBM) Journal of Parks Journal of Personalized Medicine (JPM) Journal of Pharmaceutical and BioTech Industry (JPBI) Journal of Respiration (JoR) Journal of Risk and Financial Management (JRFM) Journal of Sensor and Actuator Networks (JSAN) Journal of the Oman Medical Association (JOMA) Journal of Theoretical and Applied Electronic Commerce Research (JTAER) Journal of Vascular Diseases (JVD) Journal of Xenobiotics (JoX) Journal of Zoological and Botanical Gardens (JZBG) Journalism and Media Kidney and Dialysis Kinases and Phosphatases Knowledge LabMed Laboratories Land Languages Laws Life Limnological Review Lipidology Liquids Literature Livers Logics Logistics Lubricants Lymphatics Machine Learning and Knowledge Extraction (MAKE) Machines Macromol Magnetism Magnetochemistry Marine Drugs Materials Materials Proceedings Mathematical and Computational Applications (MCA) Mathematics Medical Sciences Medical Sciences Forum Medicina Medicines Membranes Merits Metabolites Metals Meteorology Methane Methods and Protocols (MPs) Metrics Metrology Micro Microbiology Research Micromachines Microorganisms Microplastics Minerals Mining Modelling Modern Mathematical Physics Molbank Molecules Multimodal Technologies and Interaction (MTI) Muscles Nanoenergy Advances Nanomanufacturing Nanomaterials NDT Network Neuroglia Neurology International NeuroSci Nitrogen Non-Coding RNA (ncRNA) Nursing Reports Nutraceuticals Nutrients Obesities Oceans Onco Optics Oral Organics Organoids Osteology Oxygen Parasitologia Particles Pathogens Pathophysiology Pediatric Reports Pets Pharmaceuticals Pharmaceutics Pharmacoepidemiology Pharmacy Philosophies Photochem Photonics Phycology Physchem Physical Sciences Forum Physics Physiologia Plants Plasma Platforms Pollutants Polymers Polysaccharides Populations Poultry Powders Proceedings Processes Prosthesis Proteomes Psychiatry International Psychoactives Psychology International Publications Quantum Beam Science (QuBS) Quantum Reports Quaternary Radiation Reactions Real Estate Receptors Recycling Regional Science and Environmental Economics (RSEE) Religions Remote Sensing Reports Reproductive Medicine (Reprod. Med.) Resources Rheumato Risks Robotics Ruminants Safety Sci Scientia Pharmaceutica (Sci. Pharm.) Sclerosis Seeds Sensors Separations Sexes Signals Sinusitis Smart Cities Social Sciences Société Internationale d’Urologie Journal (SIUJ) Societies Software Soil Systems Solar Solids Spectroscopy Journal Sports Standards Stats Stresses Surfaces Surgeries Surgical Techniques Development Sustainability Sustainable Chemistry Symmetry SynBio Systems Targets Taxonomy Technologies Telecom Textiles Thalassemia Reports Therapeutics Thermo Time and Space Tomography Tourism and Hospitality Toxics Toxins Transplantology Trauma Care Trends in Higher Education Tropical Medicine and Infectious Disease (TropicalMed) Universe Urban Science Uro Vaccines Vehicles Venereology Veterinary Sciences Vibration Virtual Worlds Viruses Vision Waste Water Wild Wind Women World World Electric Vehicle Journal (WEVJ) Youth Zoonotic Diseases Education Sciences Article Type All Article Types Article Review Communication Editorial Abstract Book Review Brief Communication Brief Report Case Report Clinicopathological Challenge Comment Commentary Concept Paper Conference Report Correction Creative Data Descriptor Discussion Entry Essay Expression of Concern Extended Abstract Field Guide Guidelines Hypothesis Interesting Images Letter New Book Received Obituary Opinion Perspective Proceeding Paper Project Report Protocol Registered Report Reply Retraction Short Note Study Protocol Systematic Review Technical Note Tutorial Viewpoint All Article Types Advanced Search Section All SectionsCurriculum and InstructionEarly Childhood EducationEducation and PsychologyHigher EducationLanguage and Literacy EducationSpecial and Inclusive EducationSTEM EducationTeacher EducationTechnology Enhanced EducationGeneral All Sections Special Issue All Special Issues(Re)membering How We Liberate: Black Women Educators’ Identities, Educational Praxis, and InfluenceA Familycentric Approach to Schooling: What It Is, What It Takes, What It Looks LikeAccommodating All Students: Instruction through a Social Justice LensActive Methodologies and Educative Resources Mediated by TechnologyActive Teaching and Learning: Educational Trends and PracticesAddressing Mental Health and Well-Being in Higher Education: Collaborative Insights from Education, Psychology, and the Learning SciencesAddressing Mental Health and Well-Being in Higher Education: Insights from Educational PsychologyAdvancements in Educational and Information Technology: Bridging the Gap in LearningAdvances and Novel Methods for Education in the Era of Industry 4.0Advances in Education and Training for Disaster Risk Reduction and ManagementAdvances in Educational Interventions for Autistic IndividualsAdvances in Gifted and Talented and Creativity ResearchAdvances in Learning and Teaching in Medical EducationAdvances in Media EducationAdvances of Augmented and Mixed Reality in EducationAdvancing Vocational Education: Exploring Innovative Approaches for Lifelong Skills EnhancementAge-Friendly Media Literacy Education for Older PeopleAI and Digital Learning in Education and Research TrainingAn Educational Approach to LandscapeArt and Design Education for Equity and InclusionArt Education: Past, Present and FuturesArtificial Intelligence (AI) and EducationArtificial Intelligence and EducationArtificial Intelligence in Higher Education: Innovation, Cognition and LearningArtificial Intelligence Promoted Education ReformAssessment for Learning: The Added Value of Educational MonitoringAssessment in Medical Education during and after COVID-19Authentic LearningBiology EducationBlended Learning: A Global PerspectiveBoundary Objects and Practices in Educational ContextsBreaking Barriers: Educational Leadership and Policies for Diversity and InclusionCareer Development Learning for Higher Education StudentsChallenges and Future Trends of Distance LearningChallenges and Future Trends of Sustainable Environmental EducationChallenges and Innovations in Teaching Foreign Languages in the Digital AgeChallenges in Language Education in the 21 CenturyChallenges to Improving Outcomes in K-12 and Postsecondary Systems: Acknowledging the Elephants in the RoomCivics and Citizenship in Its Global ContextClassroom and School Learning EnvironmentsClassroom Assessment Literacy: Exploring Teachers’ Knowledge and Skills for AssessmentCognitive Development in Early Childhood EducationCollaborative and Resilience-Oriented Practices and Teacher WellbeingCollaborative Learning with Technology—Frontiers and EvidenceComparing Classroom and Online LearningCompetencies for Sustainable Development in University Education: Challenges and OpportunitiesComputational Thinking, Programming and Robotics in Educational ContextsComputer Science and Engineering Education for Pre-collegiate Students and TeachersComputer-based Learning in Higher EducationConcept Mapping and EducationConceptions, Perceptions and Intersections of Equity Gaps in EducationConsequential Assessment of Student LearningConstruction and Built Environment Education in A Digital ContextContemporary Arts Education Through the Lens of Social Justice: Insights from Research and PracticeContemporary Issues in Music Education: International PerspectivesContemporary Research in Sport, Physical Activity, and Physical EducationContemporary Teacher Education: A Global PerspectiveCooperative/Collaborative LearningCreating the Future of Teaching and Teacher Education – Innovative Disruption as a Catalyst for ReformCreativity and EducationCreativity, Problem-based Learning, and (STEM) Education PracaticesCritical Issues in Educational TechnologyCritical Issues in Mathematics EducationCritical Language PedagogyCritical Mathematics Education: Bridging Theory, Practice, and Social ChangeCritical Multicultural Education: Working at the Intersections of Resistance, Restorative Justice, and Revolutionary ChangeCritical Pedagogy and Literacy for Rethinking the Pandemic EnvironmentCritical Thinking: Bridging a Successful Transition between University and Labour MarketCross-Cultural Education: Building Bridges and Breaking BarriersCultivating Creativity and Innovation in Music EducationCultivating Professional Teachers for Science EducationCultural Influences on Classroom InteractionCulturally Responsive Leadership in Education: Engaging Social Justice and Equity for ImprovementCulture in Language Education: Is This an Outdated Issue?Culture of Diversity and Interculturality in Education TodayCurrent Challenges and New Perspectives on Physical EducationCurrent Issues and Trends in Higher EducationCurrent Perspectives on Social and Emotional Learning and Development – COVID-19Current Trends and Practices in Business and Law Education: Lessons from the Pandemic and Ways ForwardCurrent Trends in Game-Based LearningCybersecurity and Digital EducationDemocracy and Education at 100Democracy, Justice, and Human Rights EducationDeveloping Teachers: A Necessary Condition for Quality RetentionDeveloping the Teacher Workforce: Motivation before, during, and after Teacher TrainingDevelopment and Learning: Embracing the Arrival of Artificial Intelligence in the Early YearsDidactics of Physical Education and SportDifferent Approaches in Mathematics Teacher EducationDigital Competence of Educators: Opportunities and ChallengesDigital Education: Theory, Method and PracticeDigital Learning in Open and Flexible EnvironmentsDigital Learning InnovationDigital Learning Innovation in Formal and Non-formal Learning Institution: Current Trends and PracticesDigital Literacy Environments and Reading ComprehensionDigitization and Lifelong Learning Sustainability: A Global PerspectiveDisability, Inequity, and Opportunity amidst the COVID-19 Pandemic: Lessons Learned and Future Directions for Family–Professional InteractionsDispelling Myths about MathematicsE-learning in Engineering Education: Challenges and SolutionsE-learning Trends and Opportunitiese-Vocabularies and e-LearningEarly Childhood EducationEcocentric EducationEconomic EducationEconomics of Education and Prospective Developments in Higher EducationEducating for Positive CreativityEducating for Sustainability TransitionsEducating Informal EducatorsEducation and Development for Racial and Minority StudentsEducation for Social Transformation: Initiatives and Challenges in the Contexts of Globalization and the Sustainable Development GoalsEducation for Sustainable Development and Teaching: Challenges, Practice and ResearchEducation for the Professions in Times of Change Education Improvement Promoting Human Capabilities Development in Post-Neoliberal PeriodEducation in TranslationEducation Policy: Analysis and Practice for Persisting Educational IssuesEducational Effectiveness and Improvement - Research, Policy and Practice from the UK, the USA, China and across the WorldEducational Equity Gap and Artificial Intelligence in STEM SubjectsEducational Equity: Cultural and Ethnic Diversity in SchoolsEducational Innovation with Information and Communication TechnologiesEducational JusticeEducational Leadership and Management for Quality: Past, Present and FutureEducational Leadership in School ImprovementEducational Leadership in Turbulent TimesEducational Leadership, Management and Policy in International EducationEducational Leadership: A Global PerspectiveEducational Leadership: International Perspectives and Global InnovationsEducational Research and Innovation in the First Global Catastrophe of the 21st Century: Committed to EducationEducational Research in the Era of 2030 Agenda for Sustainable DevelopmentEducational Technologies, Teacher Training and Competencial Development Based on Innovative Emerging PedagogiesEffective Teaching and Learning Strategies for EFL LearnersEffects of Learning Environments on Student OutcomeseLearning: Exploring Digital Futures in the 21st CenturyElementary Reading Instruction: Effective Practices for Successful Literacy AcquisitionEmerging Issues and Advancing Research on Dental Education and TrainingEmerging Issues in Maritime Education and TrainingEmerging Pedagogies for Integrating AI in EducationEmerging Technologies in EducationEmerging Trends in Educational Leadership: Crisis-Resilience and Future Proofing SchoolingEmotions and Education: Analysis of the Benefits and Risks of Human DevelopmentEmotions in Second/Foreign Language Teaching and Learning: Applications, Implications, and ImpactsEmpowering Lifelong Learning: Innovative Approaches and Best Practices in Adult EducationEmpowering Teacher Professionalization with Digital CompetencesEmpowering the Next Generation: Environmental Education for a Sustainable FutureEmpowering the Next Generation: Fostering Physical Education through Effective PedagogyEngaging Children in Math: Game-Based and Playful Approaches to LearningEngineering Education and Technological / Professional LearningEnhancing Educational Leadership and Management for Effective Education Policy ImplementationEnhancing STEM Education through Collaborative Learning ApproachesEnhancing Teacher Education: Innovations and Challenges in Professional Development and TrainingEntrepreneurship EducationEntrepreneurship Education: Challenged and ChallengingEntrepreneurship Education: Moving Forward for the Next Generation of Graduate EntrepreneursEnvironmental Education during and Post-COVID-19: Challenges and New OpportunitiesEpistemology and EducationEquity and Inclusion in Physical Education: Challenging Power Structures and Fostering Equal Opportunities for Diverse StudentsEquity-Driven School LeadershipEstablishing Links between Research on Educational Effectiveness and School Improvement: Constraints and PerspectivesEvaluation of Education Programmes and PoliciesEvidence Informed Practice in EducationExcellence Gaps in EducationExercise and Health in the School EnvironmentExperimenting with Online Pedagogical Resources for European Universities (OpenU)Exploring Research-Based Theatre within Contemporary Theatre EducationExploring Teaching and Learning in Physical Education and SportExtended Reality in EducationExtending Learners’ Mathematical Thinking: Venues and VenturesFacing New Challenges for Competencies Acquisition in Higher EducationFermi Problems in Mathematics and Science EducationFinancial Literacy Acquisition: Issues and ImplicationsForeign Language Teaching and LearningForgiveness Education around the World: Considerations, Benefits, and ApproachesFostering Academic Performance in Online LearningFostering Civic Engagement: Empowering Students through Citizenship EducationFostering Educational Equity through Linguistically and Culturally Responsive EducationFostering Mathematical Thinking in Early Years: The Synergy of Inquiry-Based Learning, Play-Based Activities and Parental InvolvementFrom Hi-Tech to Hi-Touch: A Global Perspective of Design Education and PracticeFuture Directions for Gifted and Creative EducationFuturing the Effects of Rapid Technological Change on Inclusivity and Equity in Higher EducationGame-Based LearningGame-Based Learning: Evaluation of Integrating Pedagogical Content and AssessmentGardens as Innovative Learning ContextsGender and Early Childhood Education: Debates and Current ChallengesGender and LeadershipGenerative AI in Education: Current Trends and Future DirectionsGenerative AI in Education: Theoretical Explorations and Practical InvestigationsGenerative-AI-Enhanced Learning Environments and ApplicationsGeography Education Promoting Sustainability—Series 1Global Perspectives on Higher EducationHarmonizing Vocational Education and the Arts: Exploring the Role of Aesthetic Experience in Conceptual DevelopmentHealth Professional Education: Responding to Population Health NeedsHealth Professions Education & Integrated LearningHealthy Habits, Psychological Variables and Learning in Children and AdolescentsHigher Education Governance and Leadership in the Digital EraHistory Curriculum, Geschichtsdidaktik, and the Problem of the NationHow Artificial Intelligence Can Enhance Education: Current Practices and ChallengesICT in Education Contexts of 21 CenturyICTs in Managing Education EnvironmentsIdentifying and Supporting Giftedness and Talent in SchoolsIncluding Sustainable Development Goals (SDGs) Transversally in EducationInclusion and Disability: Perspectives on Theory, Research, and PracticeIncreasing Participation in Higher Education STEM Programs: Practices, Policies, Pedagogies to Disrupt ExclusionIndustrial EducationInfant–Toddler Pedagogy: Strategies and ApproachesInnovation and Challenges in Teaching and Learning Applied SciencesInnovation in Teaching Science and Student Learning AnalyticsInnovation, Didactics, and Education for SustainabilityInnovations and Challenges in Online Postgraduate Education: Preparing the Next Generation of ScholarsInnovations and Contemporary Perspectives in Chemistry EducationInnovations in Educational Leadership Preparation and Development: Promising Approaches and ResultsInnovative and Hybrid Learning SpacesInnovative Approaches to Anatomy Education for Undergraduate Physical Therapy StudentsInnovative Approaches to Understanding Student LearningInnovative Methods in Teaching in Secondary EducationInquiry-Based Chemistry Learning and Teaching in Higher EducationInspiring Engagement through Reading and Writing with Children’s Literature in Initial Teacher EducationIntegration of Educational Technologies to Support Teaching and Learning in Higher Education: Exploring Innovations, Challenges, and Best PracticesInteractive Simulations and Innovative Pedagogy for Conceptual Understanding in Science EducationIntercultural EducationInterdisciplinary Perspectives on Professional Culture in a Changing Environment——Selected Papers from Professional Culture of the Specialist of the Future)International and Comparative Studies in EducationInternational Perspectives on Inclusion in EducationInterprofessional Medical Education and Practice: Global Perspectives in Overcoming the Challenges of Disrupted FutureJustice-Centered Mathematics TeachingLanguage and Literacy Development in Second Language Learners across the CurriculumLeading in Uncertain Times: Eradicating Inequities and Fostering Social Justice to Promote Student SuccessLean in EducationLearner-Centered TeachingLearner–Computer Interaction and Intelligent Tutoring SystemsLearning EnvironmentsLearning for Sustainability: Challenges and Progress of Embedding Sustainability into Teaching and Learning and BeyondLearning Technologies and Interactive DesignsLearning, Its Education and Its Contemporary Theoretical ComplexitiesLeisure in Education: A Multi-Contextual ToolLiteracy, Motivation and Education among Prison PopulationsLong Overdue: Translating Learning Research into Educational PracticeMachine learning in EducationManaging Informal Adult Learning Processes and EducationMassive Open Online CoursesMathematical Modeling in Education with the Use of Digital ToolsMathematics Education: At Home and in the ClassroomMedia Literacy in Lifelong LearningMiddle Grades EducationMigrant Integration in Schools: Policies and PracticesMixed-Methods Perspectives on Social Networks in Education ResearchMobile and Ubiquitous Personalised Technologies and Applications in Heritage Education and Museum Learning EnvironmentsMobile LearningMoral Education and IdentityMoving Forward Together: Diversity, Equity, and Inclusion in Higher EducationMoving Forward: Research to Guide Middle Level EducationMultilingualism in Higher EducationMusic Education: Current Changes, Future TrajectoriesNatural Science and Technology in Early Years EducationNetworked Learning—Expanding and Challenging Theory, Design and PracticeNetworks Applied in Science Education ResearchNew Trends in Educational Technology after the PandemicNew Ways of Seeing Outdoor and Environmental LearningOnline Education: Lessons Learnt and the Way ForwardOnline Learning and Digital Education: Opportunities and ChallengesOnline Practicum and Teacher Education in the Digital SocietyOutdoor Adventure Education: Trends and New DirectionsOutdoors: Playing, Learning and TeachingPathways to Progress and Practice: Leading the Way in STEM Education Innovations and the WorkforcePeer Assessment in K–12 ContextsPerspectives on Educational MeasurementPerspectives on Special EducationPhilosophy of Education Today: Diagnostics, Prognostics, Therapeutics and PandemicsPhilosophy of Education: The Promise of Education and GriefPhysical Education: Teaching and LearningPivoting Healthcare Education Online: Perspectives, Challenges, and ProspectsPlace-based Partnerships and New School Designs to Address Poverty, Social Exclusion, and Social IsolationPlay and Learning in Early Childhood EducationPolicy and Practice in Inclusive EducationPositive Pedagogy in Physical Education and Sport ContextsPost-Compulsory EducationPostsecondary and Tertiary Peer Assisted LearningPower of Literacy: Strategies for Effective Reading InstructionPractice and Policy: Rural and Urban Education ExperiencesPractices and Challenges in Gifted EducationPrefiguring Sustainable Futures through Climate Change EducationProblem-Based Learning in Science Education: Achievements, Pitfalls and Ways ForwardProblem-Based Learning in Science Education: Achievements, Pitfalls and Ways Forward, 2nd EditionProblem-based Pedagogies in Higher EducationProgrammatic Assessment in Education for Health ProfessionsProgress in Geography Education ResearchProject and Design-Based Learning in Electrical and Computer Engineering for K-16 Students and TeachersPromoting Environmental and Sustainability Education in the Era of Climate ChangePromoting Linguistic Diversity in Higher EducationPsychology and EducationQuantitative Research Methods to Support Quality Education and Reduced InequalitiesRacial Equity, Diversity, and Inclusion in Schools: Humanizing Wellness While under AttackReach for the Stars: Enhancing Pedagogy and Technology in Physics and Astronomy EducationReaching Struggling Learners: Teachers' PerspectiveReading FluencyReclaiming and Rethinking Teacher Education: Global Concerns, Challenges and OpportunitiesReform in EducationRegional Disparities in National Education: Origins, Governance, and ConsequencesRegulation and Ethical Practice for Educational ResearchReimagining K-20 Educational Leadership in the 21st CenturyReimagining Science Education: Computational Thinking, Computational Science and Sustainability EducationReligious Education for Civic Renewal: Challenges and ProspectsResearch and Trends in Entrepreneurship EducationResearch Needs in Mathematical Giftedness and CreativityRethinking Education in the Digital AgeRevolutionizing Engineering Education: Innovations and PerspectivesSchool Leadership and School ImprovementSchool Management and EffectivenessSchool Well-Being in the Digital EraSchools' Linguistically Diverse Landscapes: Issues and Innovations in Teaching Multilingual School PopulationsScience Communication in Education: Mapping the Field to Foster the Impact and Sustainability of Education SciencesScience EducationScience Education in a Post-Truth WorldSecond Language Acquisition and Language Education – Bridging the InterfaceSelect Papers from EdMeas23: Sydney Educational Measurement and Assessment Symposium 2023Selected paper from The International Conference on Science and Technology EducationSelected Papers from Eurasian Conference on Educational Innovation 2020Selected Papers from the Eurasian Conference on Educational Innovation 2019Selected Papers from V International Congress on Research on Didactics of Physical EducationSerious Games and Gamification in School EducationShrinking Opportunity and Achievement GapsSmart Education in the Digital SocietySocial and Emotional Learning and Wellbeing in EducationSocietal Culture and Educational/School LeadershipSport, Physical Activity and Physical Education without Boundaries: Fairness and InclusionStorytelling in STEM Disciplines—At the Crossroads of Science and HumanitiesStrategic Academic Research and DevelopmentStrengthening Educational Leadership Preparation and DevelopmentStudent Preferences and Satisfaction: Measurement and OptimizationSuccessful School Leadership: Perceptions and Practice in Multiple CountriesSustainability and Environmental Education across Place, Culture and CommunitySustainability of Digital Game-Based LearningSustainability, Environment and EducationTalent and Education: Approaches, Assessments and InterventionTeacher Education for Islamic Education and SchoolingTeacher Identity from the Perspective of StudentsTeachers, Teacher Education, Professional Learning and Development: A Focus on Pedagogical MobilityTeachers’ Decisions regarding Students’ Transition from Primary to Secondary School: New Insights from International ResearchTeaching and Assessing Mathematics in a Digital WorldTeaching and Learning in Exercise and Environmental PhysiologyTeaching and Learning in Physics: An Inquiry-Based PerspectiveTeaching and Learning in STEM EducationTeaching and Learning Sequences: Design and EffectTeaching and Learning with Digital Resources in Preschool/KindergartenTeaching and Learning with Technology to Promote Educational Inclusion Processes: Opportunities and ChallengesTeaching English to Speakers of Other Languages as a Non-native English SpeakerTeaching Methods in Science Subjects Promoting SustainabilityTechnology Enhanced Learning and Evaluation: A Global PerspectiveTechnology Enhancing the Skills of Students with DisabilitiesTechnology-Enhanced Learning and Teaching: Present and FutureTechnology-enhanced Learning in Media StudiesTechnology-Mediated Active Learning MethodsThe ""Gentle Push"" of Technologies to Change the SchoolThe 2-MEV Model Monitoring Green AttitudesThe Academic Approach to EducationThe Challenges of Teachers in the Digital Age: Teaching despite TechnologyThe Education of d/Deaf and Hard of Hearing Children: Perspectives on Language and Literacy DevelopmentThe Flipped Classroom in Higher Education: Research and PracticeThe Future of Educational TechnologyThe Impact of Inquiry-Based Science Teaching in Secondary SchoolsThe Multifaced Opportunities for Utilization of Simulation-Based Learning Models in K-12 and Professional Development in EducationThe Nature, Quality and Dynamics of Teacher–Pupil Relationships in the ClassroomThe Necessity of Collaborative and Innovative Ways of Learning and Total Quality Management in Secondary EducationThe Power of Literacy in Underserved Languages: Strategies for Improving Reading, Writing, and Access to KnowledgeThe Power of Play: Gamification for Engaging and Effective LearningThe Power of Writing and Reading in Contemporary Times: A Pressing Need for Social ChangeThe Psychological and Educational Effects of COVID-19: Now and ThenThe Quality of Classroom AssessmentsThe Relation between Supplementary Education and Public SchoolingThe Role of Assessment in Supporting an Equitable PedagogyThe Role of Generative Artificial Intelligence in Supporting Student Learning in Postsecondary EducationThe Role of Technology in Education: Progress, Problems, and PossibilitiesThe State of the Art and the Future of EducationThe Technological Revolution in Education: Exploring New Trends in LearningThe Territorial Dimension of School-to-Work Transitions: Youth Opportunities across Space, Contexts and Multilevel GovernanceTheory and Research in Data Science EducationThere is a Crack in Everything—Education and Religion in a Secular AgeTowards an Entrepreneurial Education and Global CitizenshipTowards Excellence in Engineering EducationTransfer of Training in Lifelong Learning Education and BeyondTransformative Paradigms in Art Education: Navigating Technology, Social Justice, and Cultural Change in Complex TimesTransformative Pedagogies: Fostering Motivation, Enhancing Attractiveness, and Cultivating Commitment in Early Childhood Education through Teacher EducationTransforming Educational LeadershipTransforming Teacher Education for Academic ExcellenceTrends and Prospects: Comparative Studies in Doctoral Education—a Look into How Doctoral Programs Navigate Legislative LandminesTrends in STEM EducationTrends in the Promotion of Physical Education and Physical Activity and Sport from a Gender PerspectiveTrends on Educational Gamification II: Game-Based Learning and Teacher InstructionTrends on Educational Gamification: Challenges and Learning OpportunitiesTriggering Motivation through Play and CuriosityUndergraduate Research as a High Impact Practice in Higher EducationUnderstanding School Success of Migrant Students: An International PerspectiveUnderstanding the Rich Context of Minority Serving InstitutionsUrban/City SchoolsUsing Game- and Play-Based Learning to Foster Critical Thinking SkillsUsing Learning Analytics for Personalised, Data-Informed Feedback and Support: Studies of Impact, Challenges, and Future DirectionsValorization of Physical EducationViews of Physics Teachers on Inquiry and Their Approaches to Classroom PracticesVirtual Schools for K-12 Education: Lessons Learned and Implications for Digital K-12 and Other Sectors of EducationVisible Learning – What’s next? A Decade after An Educational MilestoneVitality in Movement: Rethinking Physical Education from a Pedagogical to a Physiological DimensionVocabulary DevelopmentVocational Education, Skill Formation, and Social DevelopmentWeb-Mediated Approaches to Teachers’ Professional DevelopmentWeb-Mediated Instructional DesignWidening Participation in Higher EducationWorldly Orientations to Internationalising Education: Non-Western Languages, Theoretic-linguistic Tools and Modes of CritiqueYoung Children, Maker Literacies and Social Change All Special Issues Volume Issue Number Page Logical OperatorOperator AND OR Search Text Search Type All fields Title Abstract Keywords Authors Affiliations Doi Full Text References add_circle_outline remove_circle_outline Journals Education Sciences Volume 14 Issue 9 10.3390/educsci14090988 Submit to this Journal Review for this Journal Propose a Special Issue ► ▼ Article Menu Article Menu Academic Editor João Piedade Subscribe SciFeed Recommended Articles Related Info Link Google Scholar More by Authors Links on DOAJ Katsantonis, A. Katsantonis, I. G. on Google Scholar Katsantonis, A. Katsantonis, I. G. on PubMed Katsantonis, A. Katsantonis, I. G. /ajax/scifeed/subscribe Article Views Citations - Table of Contents Altmetric share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles thumb_up 0 Endorse textsms 0 Comment Need Help? Support Find support for a specific problem in the support section of our website. Get Support Feedback Please let us know what you think of our products and services. Give Feedback Information Visit our dedicated information section to learn more about MDPI. Get Information clear JSmol Viewer clear first_page Download PDF settings Order Article Reprints Font Type: Arial Georgia Verdana Font Size: Aa Aa Aa Line Spacing:    Column Width:    Background: Open AccessArticle University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes by Argyrios KatsantonisArgyrios Katsantonis SciProfiles Scilit Preprints.org Google Scholar 1 and Ioannis G. KatsantonisIoannis G. Katsantonis SciProfiles Scilit Preprints.org Google Scholar 2,* 1 Department of Education Sciences and Social Work, School of Humanities and Social Sciences, University of Patras, 26504 Patras, Greece 2 Faculty of Education, School of Humanities and Social Sciences, University of Cambridge, Cambridge CB2 8PQ, UK * Author to whom correspondence should be addressed. Educ. Sci. 2024, 14(9), 988; https://doi.org/10.3390/educsci14090988 Submission received: 26 June 2024 / Revised: 28 July 2024 / Accepted: 6 September 2024 / Published: 8 September 2024 Download keyboard_arrow_down Download PDF Download PDF with Cover Download XML Download Epub Download Supplementary Material Browse Figures Versions Notes Abstract: Artificial intelligence (AI) drives new modes of learning and improves the workflow of instructors. Nevertheless, there are concerns about academic integrity, plagiarism, and the reduction of critical thinking in higher education. Therefore, it is important to record and analyze university social sciences students’ attitudes toward AI, which is a significant predictor of later use of AI technologies. A sample of 190 university students (82.45% female) from a Greek social sciences department was selected. Descriptive statistics revealed that students’ attitudes toward AI were mostly positive. A principal components analysis confirmed a three-component solution of attitudes toward AI, comprising cognitive, behavioral, and emotional dimensions. Comparative analysis of the three components indicated that the emotional dimension was the highest ranked, followed by the cognitive and behavioral dimensions. Pairwise correlation analyses revealed that the strongest correlate of cognitive, behavioral, and emotional components of attitudes toward AI was the future frequency of AI use, followed by general feelings of safety with technology. In conclusion, students display more emotional and cognitive favorable dispositions toward AI. The social background of the students and the prospective future use of AI play a key role in the formulation of attitudes toward AI. University educators need to provide more teaching and learning about AI to improve students’ attitudes toward AI and future AI use. Keywords: attitudes; artificial intelligence; university students; social sciences 1. IntroductionThe use of Artificial Intelligence (AI) is becoming particularly widespread in different activities in life, and the use of AI in education cannot be excluded from this trend [1,2]. AI can be defined as a field of computer science that aims to address different problems of a cognitive nature, such as problem-solving or learning, but AI can also be defined as a theoretical model that can set useful guidelines for the creation and use of intelligent computer systems that mimic the characteristic capabilities of human beings [3,4]. Outside of education, AI applications are deployed, for instance, for predictive analytics and personalised medicine in healthcare contexts [5]. In the finance sector, AI is used, for example, for smart designing, planning, and developing financial products and services [6]. In short, AI is revolutionizing society [7] and higher education by introducing new modes of learning and knowledge acquisition [8].AI applications in education are usually called AIEd [9]. AI in education can be used to replace a tutor or instructor, to improve the tutor-student relationship, and/or to act as a fellow student to facilitate one’s own learning either through collaborative learning or through tutoring of a less knowledgeable student [10]. Additionally, AIEd applications can be used as a platform to assist instruction and learning (e.g., interactive learning environment) or as a tool that expediates instruction or learning (e.g., automatic grading) [10]. Alternatively, AIEd apps can be conceptualised as directly unnecessary for instruction or learning, but they can act in a supplemental assisting capacity to gain a greater understanding of and predict learning behaviours, characteristics, and patterns in learning and instruction [10].Although AI-powered tools have significantly improved the pedagogical and administrative workflow of teachers [11], it remains to be seen whether students are accepting of these new AIEd advances and what their attitudes toward AI are. Given the above considerations, the current study aims to record university social sciences students’ attitudes toward AI in their education and future profession and the factors associated with their attitudes. In this study, the focus is on social sciences students because research on AI attitudes has shown significant differences between STEM (i.e., Science, Technology, Engineering, and Mathematics) versus non-STEM university students [12,13], and non-STEM students might not have any training on AI [13]. 1.1. Related Empirical Research on Students’ Attitudes toward AIThe study of students’ attitudes toward AI is a very important, albeit usually ignored, area of educational research. Most of the research methodologies implemented in this field follow the survey research design, which is appropriate for the measurement of attitudes [14]. An attitude is defined as a favorable or unfavorable disposition toward a specific object, person, or behaviour [15,16,17]. Attitudes can have affective, behavioral, and cognitive dimensions [18,19]. According to [19], attitudes can be conceptualized as a person’s particular way of thinking, feeling, and acting. Yet, few comprehensive empirical studies on students’ attitudes toward AI have been published [12,20,21,22,23,24]. A summary of the reviewed studies is presented in Table 1.Yet, some of these studies [21,22] have not provided a comprehensive picture of students’ attitudes toward AI since they have not clearly distinguished the cognitive, emotional/affective, and behavioral components of AI attitudes [18]. This highlights the need for a more in-depth investigation of university students’ attitudes toward AI. Additionally, some of these studies [21,23] conflated the attitudes of non-STEM students with those of STEM students, who usually have more positive attitudes toward AI [12]. Studying students’ attitudes toward AI is important because attitudes play a major role in shaping individuals’ future intention to use AI-powered applications and their subsequent behavioral actualization of this intention, according to the theory of planned behaviour [15,25]. Therefore, the present study utilizes a recently proposed scale [18] to measure social sciences university students’ attitudes toward AI in a multidimensional way. 1.2. Measuring Attitudes toward AIThe measurement of students’ attitudes toward AI remains an actively researched field. Some studies employ single questions to ask students to reflect on their beliefs about AI and to respond to Likert-type items or rank several items [20,23]. Another strand of research has utilized several items to create scales that measure different aspects of attitudes such as emotional attitudes toward AI [24]. Recently, an effort has been observed in the literature to construct psychometrically valid multi-item and multi-dimensional scales that tap into attitudes toward AI. For instance, a study with radiology undergraduates presented a multi-item scale of attitudes toward AI in radiology [24].A particularly useful measure of students’ attitudes toward AI has been recently proposed in the literature. This measure is called SATAI—a scale measuring students’ attitudes toward AI that was developed in the context of K-12 education in Korea [18]. The methodological innovation of this scale is that it comprehensively captures cognitive, affective, and behavioral aspects of attitudes toward AI using multiple items that tap into the domains of education/schooling and future professional life. Therefore, the present study attempts to adapt this latter scale for university students studying in a social sciences department to investigate university students’ attitudes toward AI and to examine whether the scale’s good psychometric properties can be replicated in the social sciences university setting. 1.3. Conceptual Framework and Potential Correlates of Attitudes toward AI in Education and Professional LifeThe correlates of university students’ attitudes toward AI are still being researched. Empirical research on students’ attitudes toward technology in general has shown that attitudes were associated with the mother’s education level but not with the father’s education [26]. Evidence from sociological analyses of adolescents’ use of social media and digital skills originating in cultural capital theory [27,28] has revealed that students with more educated parents and more cultural practices and resources had greater digital skills and social media use [29]. However, a study conducted within the context of AI attitudes reported that socio-cultural factors were exhibiting associations with attitudes toward AI and particularly, students struggling due to socio-cultural factors held more positive attitudes toward AI if they had an AI education [30]. These inconclusive findings suggest that we need to gain a greater understanding of the associations between students’ socio-economic background and their attitudes toward AI. Evidence coming both from the attitudes toward technology [26] and the attitudes toward AI [21,31,32] areas has shown a null effect for students’ gender, suggesting that gender is not associated with attitudes toward AI. For example, a study with education undergraduates found no differences between female and male students in perceived usefulness, ease of use, enjoyment, and job relevance of AI [33]. Yet, there is also evidence to the contrary, indicating statistically significant differences between males and females in attitudes toward AI [24,30]. This disagreement indicates that more research on gender and attitudes toward AI is needed.Beyond the above background factors, research has been conducted regarding the potential links between attitudes toward AI and other practical factors. A recent study, for instance, showed that intention to use AI-powered applications frequently was associated with greater attitudes toward the AI chatbot called ChatGPT [34]. Another important factor to consider as a correlate of students’ attitudes toward AI is the students’ year of university studies because a research study found significant differences between students studying in different years, whereby the higher the students’ year, the lower their attitudes of accepting AI [35]. However, another study showed that students in higher undergraduate years found that the benefits of AI increased [20]. Hence, it is necessary to account for students’ years of studies as well. Furthermore, the issue of the safety of AI applications has also been of long-standing importance, with several studies highlighting the need for safety [23,36]. Therefore, we measured the general sense of digital safety using three items as a potential precursor of AI attitudes. Given the above evidence, it is important to verify what is the nature of the associations between all these factors and students’ attitudes toward AI.Within this conceptual framework, AIEd applications can be used in multiple ways to improve and university students’ learning. For instance, in the case of SES-based disparities, AIEd can become particularly useful by serving as an intelligent tutor [10]. AIEd apps can become more inclusive and adapt to specific learning preferences [9], which may differ by gender. The AIEd applications can be used to adapt the content and the difficulty levels of the learning specifically for the different years of study, as well. Ensuring a general feeling of safety can lead students to deploy AIEd apps more confidently, and building a positive attitude toward AI can lead to possible increased future use of AI apps. 1.4. The Present StudyThe present study follows the tenets of exploratory quantitative survey research [14] to explore social sciences university students’ attitudes toward AI. The purpose of the current study is threefold. First, given the recent interest in AI and its widespread use through chatbots such as ChatGPT [37], the study records and presents the attitudes of social sciences university students toward AI in education and their future profession. Second, the study explores whether an adapted version of the multidimensional attitudes toward AI measure developed by [18] is valid for social sciences university students. Third, the study provides additional and updated evidence on the association of students’ attitudes toward AI with several background characteristics as well as with the students’ future frequency of AI use and general feelings of digital safety. Hence, the following research questions and hypotheses guide the present study.RQ1:Are social sciences students more favorably or unfavorably disposed toward AI in their education and future profession?RQ2:Is the adapted SATAI multidimensional measure of attitudes toward AI psychometrically valid for social sciences students?RQ3:Are students’ socio-economic background, gender, year of studies, general sense of digital safety, and frequency of future AI use associated with their attitudes toward AI?Based on the reviewed literature, there is overwhelming evidence that university students’ attitudes toward AI are quite positive. Hence, we hypothesize that the students will exhibit generally positive attitudes toward AI in their education and future professional lives (H1). Furthermore, it is expected that the adapted SATAI measure will function adequately (H2) in this sample of Greek university students, given its previously commendable psychometric properties [18]. The student’s gender is assumed to have a null effect (H3), given that the preceding evidence is rather mixed concerning the gender differences in technology and AI attitudes [21,26]. The student’s year of studies is hypothesized to be negatively linked with attitudes toward AI (H4) because a previous study has shown that the students from a higher year of study had fewer positive attitudes [35]. However, another study found a positive association between the year of study and attitudes toward AI [20]. Hence, a positive association might be possible (H5).Additionally, we expect a positive association of attitudes toward AI with socio-economic status indicators (H6) and digital safety (H7). The reason for this is that sociological perspectives stress the role of socio-economic gaps in technology literacy and acceptance [21,29]. Further, it is hypothesized that attitudes will be positively correlated with future intentions to use AI more frequently (H8). This latter hypothesis has support from the TPB [15,24], whereby attitudes can predict subsequent behavioral intention. 2. Materials and Method 2.1. ParticipantsThe sample of the current study comprises 190 university undergraduate students from a department of social sciences (education and social work) at a Greek university. Amongst these students, 82.45% were female, 15.96% were male, and 1.6% did not disclose their gender. This university department has an overwhelming majority of female students, which is also reflected in the gender composition of the sample. The majority of the sample (78.19%) were in their first year of studies, followed by 12.77% in the fourth year of studies, and 9.04% in the third year. Regarding attained parental education, most fathers (36.36%) and mothers (37.84%) had completed secondary school studies. Amongst the fathers, 19.79% had completed higher education degrees, and amongst the mothers, 22.16% had attained a higher education degree. 2.2. Measures Cognitive, Behavioral, and Emotional Attitudes toward AITo measure cognitive, behavioral, and emotional attitudes toward AI, a recently introduced scale (SATAI) by [18] was adapted and administered. The scale is made up of twenty-six items tapping into the cognitive, behavioral, and emotional dimensions of attitudes [18]. This measure was created by drawing upon previous items being used in the information and communication technologies (ICT) and STEM fields [18]. The cognitive dimension comprised four items, such as “I think that it is important to learn about AI in school”. The behavioral dimension consisted of twelve items, such as “I want to work in the field of AI”. Finally, the emotional dimension comprised 10 items, such as “I think AI makes people’s lives more convenient”. The items were scored using a five-point Likert scale ranging from 1 “strongly disagree” to 5 “strongly agree”. Cronbach’s alphas for the cognitive, behavioral, and cognitive dimensions in the initial study were 0.905, 0.956, and 0.924, respectively [18]. As can be understood, the content of this scale needed to be adapted to the Greek university context. Therefore, three items were replaced with context-appropriate items and the initial fifteenth item was adapted. The adapted item wordings are presented in the Supplemental Materials Table S1. The Greek version of the adapted item wordings is presented in Supplemental Materials Table S2. The original English version is presented in Supplemental Materials Figure S1. 2.3. Covariates 2.3.1. GenderStudents’ gender was measured using a binary variable capturing sex assigned at birth; that is, 1 stood for “female”, and 0 stood for “male”. 2.3.2. Year of StudiesStudents reported on their year of studies on a scale ranging from 1 “first year” to 5 “more than four years”. The students were concentrated in the first, third, and fourth years of studies. 2.3.3. Mother’s and Father’s Educational AttainmentA clear indicator of families’ socio-economic status is attained parental educational level [38]. Therefore, we measured both parents’/carers’ highest attained education level on a scale ranging from 1 “did not attend school” to 7 “postgraduate degree”. 2.3.4. Cultural PracticesStudents’ families’ cultural practices, which is an essential component of cultural capital [28,39], were measured using a scale of eleven items, which was adapted from previous research [40]. A sample item was “How frequently do you visit museums?”. The items were scored using a four-point scale ranging from 1 “never” to 4 “frequently”. Cronbach’s alpha based on polychoric correlations was sufficient at 0.73. 2.3.5. General Digital SafetyStudents’ general digital safety was measured using three items such as “how safe do you feel in your financial transactions with e-shops, digital contact with administration, and participation in social media?”. Cronbach’s alpha was 0.54, probably due to the low number of items. 2.3.6. Frequency of Future AI UseA single question was posed to the students to indicate whether they will be using AI more in the future. This question was coded using a five-point scale ranging from 1 “Surely, I wouldn’t use AI apps in the future” to 5 “Surely, I would ask for more and more help from AI apps”. 2.4. ProcedureThe SATAI scale was translated using back-and-forward translation from English to Greek and vice versa. An expert in the English language validated the accuracy of the translation. A pilot study with 10 participants was conducted before the main phase of the survey to ascertain any problems with the item wordings or the questionnaire in general. The questionnaire was administered online using Google Forms and was distributed as widely as possible to all undergraduate students in the academic year 2023–2024. 2.5. Statistical AnalysesIn the first instance, the data were recoded to calculate percentages of students that expressed agreement (Strongly agree/agree) with each item of the adapted SATAI scale. These percentages were plotted to gain a greater understanding of the students’ attitudes toward AI in general. Afterwards, a principal components analysis (PCA) was conducted to verify the dimensional structure of the adapted scale for measuring attitudes toward AI. The PCA was selected to closely correspond to the results of the initial validating study [18]. The PCA is an ideal dimension reduction method that clusters together items that can form linear combinations (i.e., principal components) [41]. To determine the number of components to be retained, a PCA parallel analysis [42] and Velicer’s minimum average partial (MAP) retention criterion [43] were calculated in the psych package [44] in R [45]. Afterwards, composite summed scores were created per principal component [46]. To be able to compare the students’ overall attitude scores on the three components (cognitive, affective/emotional, behavioral), a minimum-maximum linear transformation was implemented using the following formula to place the three composite scores on a range from zero to one hundred: 𝑋𝑛𝑒𝑤=[𝑋−𝑋𝑚𝑖𝑛𝑟𝑎𝑛𝑔𝑒]×100Xnew=[X−Xminrange]×100 [47]. The reason for this transformation was that different components comprise different numbers of items and are, therefore, not comparable without being placed on the same metric. Next, bivariate Pearson correlations were estimated based on the raw variables to examine the associations between gender, year of studies, general digital safety, and future frequency of use with the cognitive, behavioral, and emotional components of attitudes toward AI. The correlogram was calculated using the heatplot package in Stata [48]. 3. Results 3.1. Descriptive StatisticsThe percentage of agreement per attitude toward AI items is presented in Figure 1. The item wordings are available in Supplemental Materials Table S1. From the data in Figure 1, it becomes apparent that more than 50% of the students agreed that AI classes are important and that AI courses should be taught at university. Next, 65.2% of the students agreed that every student at university should learn about AI, and 62.1% of the students agreed that AI can make people’s lives more convenient. Additionally, 56.3% of the students agreed that AI is related to their daily lives, whereas the vast majority (70%) agreed that they will need AI in their lives in the future. A large percentage of students (74.7%) agreed that it is valuable to learn well how to use AI, and the majority of future professions will require knowledge regarding AI (77.3%). Slightly more than half the students (51%) expressed agreement with the fact that they wanted to continue learning about AI. Most students (58.4%) indicated that it is interesting to use AI, and half the students (50.5%) reported that they will need AI as professionals (in their professional lives) in the future. 3.2. Principal Components Analysis of the Attitudes toward AI Scale-AdaptedIn the first instance, the data were screened using the Kaiser–Meyer–Olkin (KMO) criterion to evaluate the adequacy of the sample for PCA and Bartlett’s test of Sphericity to assess the factorability of the correlation matrix [49]. The KMO for the current sample with all 26 items of the adapted SATAI scale was 0.895, indicating excellent sampling adequacy [49]. Bartlett’s test of Sphericity achieved a χ2 (325) = 2282.712, p < 0.001, suggesting that the correlation matrix was factorable [49]. The univariate skewness and kurtosis statistics were computed to evaluate the degree of deviation from a univariate normal distribution per item. The skewness values ranged between −1.171 and 0.011, and kurtosis values were all below 4, suggesting a minimal departure from normality [50]. Additionally, Q–Q plots were utilized to assess the normality of the item responses and it was found that the scores were reasonably normally distributed. Hence, we could proceed with the PCA analysis. To select the optimal number of principal components, a principal components parallel analysis was run, which indicated that the optimal number of principal components was three. Similarly, Velicer’s MAP criterion reached a minimum of three components. Therefore, we could be confident that the three components adequately represented the data. This three-component solution matched the PCA solution provided in the original study to a great extent [18]. Nevertheless, the initial component structure was not fully replicated since some items loaded on different principal components (i.e., they were adapted), and one item (“I am afraid of AI in education”) was not loading on any component. The current rotated PCA loadings, the variance explained by each principal component and its eigenvalues are presented in Table 2. The mean score across all items belonging to each principal component is also presented in Table 2. From the transformed mean values per component in Table 2, it becomes clear that the emotional dimension of attitudes was the most dominant, followed by the cognitive and behavioral dimensions. 3.3. Factors Associated with Cognitive, Behavioural, and Emotional Components of Attitudes toward AIRegarding the bivariate associations between the key covariates and the three attitude components, some interesting conclusions can be reached based on the correlations (Figure 2). As shown in Figure 2, the cognitive component was positively correlated with year of studies, r = 0.13, p < 0.05, with the mother’s education, r = 0.19, p < 0.01, with future frequency of use of AI, r = 0.23, p < 0.01, and with general feelings of safety, r = 0.25, p < 0.001. The behavioral component was modestly associated with future frequency of AI use, r = 0.31, p < 0.001, and with general safety, r = 0.19, p < 0.001. The emotional component was positively associated with the mother’s education level, r = 0.16, p < 0.05, with future frequency of AI use, r = 0.39, p < 0.001, and with general safety, r = 0.14, p < 0.05. Robust bivariate correlations were observed between the three component scores ranging from 0.61 to 0.65 and were all statistically significant at p < 0.001. 4. DiscussionThe use of AI-powered applications is becoming widespread in education and has attracted substantial attention lately [9,51]. AI in education drives improvements in instruction and learning not only in the conventional classroom but also in online higher education settings [9]. However, few empirical studies have examined the attitudes of social sciences students toward AI in their education and future professional lives. Additionally, previous research has not comprehensively explored social sciences students’ attitudes toward AI from a multidimensional perspective and has not provided a clear picture regarding the correlates of these multidimensional attitudes. Hence, the present study sought to address these empirical gaps in the literature.To address the first research objective, descriptive frequencies were computed to assess the students’ degree of agreement with several attitudinal items. The descriptive findings showed that more than half of the students believed that AI courses were important, and a notable percentage of students indicated that every student should learn about AI. These findings suggest that there is a growing awareness amongst students about the relevance of AI applications in education and that basic knowledge about AI is becoming important. These results suggest the need for a university-wide universal approach to teaching and learning about AI. Previous empirical research has also reported a wide acceptance of AI in education [20]. A large, recent cross-national study showed that there was a general positive attitude toward the ChatGPT chatbot [32]. However, the current study is taking a more multidimensional perspective and provides more comprehensive information about attitudes toward AI beyond simple single items targeting specific AI applications.In addition to the role of AI in university education, we noted a large percentage of agreement with the statement that AI will make daily lives more convenient, acknowledging the relevance of AI for both daily life and the future necessity of AI. About half the students also expressed an interest in AI and indicated that they expected to use AI in their future profession. This suggests that there was a moderate but still significant trend to engage with AI, and there were some expectations about AI use in the future of their profession, which has been noted in previous studies as well [22,24]. All the above evidence raises important questions about educational practice in higher education institutions. Specifically, the findings illustrate the need for curriculum change at the university level to incorporate some AI courses/modules to prepare students for integrating AI-powered applications in their future professional lives. At the same time, the findings raise awareness of the fact that educators need to increase students’ interest and engagement with AI since only half the students were interested in AI, despite the far-reaching implications of the latter [2]. Overall, H1 was partially supported.Now, turning to the second research objective, a PCA was conducted to verify whether the adapted version of the SATAI measure [18] was functioning appropriately in a sample of Greek university students. The SATAI scale was developed to target secondary school student samples [18]. Therefore, there were some challenges in effectively translating the items to refer to the university setting and, specifically, to a Greek social sciences department. The major challenge was to accurately translate the items and to adjust the context (e.g., from school to university) as necessary. The results of the PCA analysis revealed that there was, to a great extent, a match between the current adapted version and the original version. Some significant differences occurred in the cognitive principal component, whereby three items referring to the good side of AI, the value of AI, and the importance of AI for society were loading on the cognitive component instead of the affective component. Additionally, the three modified items that described the students’ capability to use AI apps both in the future and in a professional capacity were loaded onto the affective component, suggesting that the future use of AI and the capacity to use AI-powered apps is contingent on affective/emotional dispositions. Despite the above, the three-component structure of the scale was largely replicated, and all components exhibited good reliability and discriminant validity, given the modest correlations between the three components. Hence, H2 was partially supported.Last but not least, the current study examined the bivariate correlations between the students’ demographic characteristics, general sense of digital safety, and future intention to use AI with the three attitude dimensions arising from the PCA. Similar to previous research [21,31], the correlational analysis did not reveal any statistically significant correlations between gender and attitudes toward AI. This indicates that female and male students did not differ with regard to their disposition toward AI. Yet, previous research on AI attitudes has shown that there were gender differences [22,24]. This discrepancy might have occurred due to the unbalanced nature of gender groups in the current sample. Hence, H3 was sustained. In contrast to a study on AI attitudes [35], the current analyses did not identify any significant association between the students’ year of study and their affective and behavioral attitudes toward AI. The reason for this null effect might be tied to the fact that there is no widespread teaching and learning about AI in the department where the students were sampled. Some support for this claim also comes from the fact that the students indicated the need for universal learning about AI at the university level. Yet, a small positive association was noted between the year of studies and the cognitive dimension, suggesting that students who are further along in their studies have a greater understanding of the importance of AI and the greater value of AI. This seems to be in line with some previous evidence showing a positive correlation between the benefits of AI as students progress in their studies [20]. Thus, H4 was partially rejected, and H5 was supported.The results of the correlational analyses illustrated that only mothers’ educational level was associated with more positive cognitive and affective/emotional attitudes toward AI. The families’ cultural practices and the father’s attained educational level were not statistically significant correlates of the three dimensions of attitudes toward AI. These results confirm to some extent that there exist socio-economic differences in the attitudes toward AI, with students having more educated mothers agreeing more that AI education was needed and that AI can have a great impact on their professional and daily lives and can make people’s lives more convenient. This association might have occurred also because of cultural reasons because higher maternal education might be linked with better understanding and integration of technology in general [52]. Although the link between socio-economic factors and technology attitudes and skills has been recorded in the literature [40], there is no evidence regarding the socioeconomic correlates of AI attitudes. Therefore, the current study adds to the literature by highlighting the link between maternal education and more positive attitudes toward AI. Overall, H6 was partially supported.The seventh hypothesis assumed that students’ general sense of digital safety would be associated with their attitudes toward AI. Past studies have highlighted that technology and, specifically, AI can pose a safety risk [23,36]. Hence, the participants were asked to rate their general sense of digital safety in their transactions and interactions to be able to correlate this with their cognitive, emotional, and behavioral attitudes toward AI. The results of the correlational analysis illustrated that a greater sense of digital safety was associated with more positive cognitive, emotional, and behavioral dimensions of attitudes toward AI. Although there is no direct evidence to corroborate this finding, this result appears to be sensible. This finding suggests that more positive attitudes toward AI can be formulated by strengthening students’ sense of safety with digital transactions and interactions in social media and the administration. Overall, these findings suggest that H7 was supported. Finally, the analyses revealed that the three dimensions of attitudes were positively associated with future intentions to use AI more frequently. This finding shows, in line with TPB [15,25], that students’ attitudes toward AI can play an important role in shaping their behavioral intention to use AI-powered apps more frequently in the future. However, later developments in the field of the technology acceptance model [53,54] suggest that attitudes are not very strong explanatory factors of future behavioral intention. The current findings seem to support this claim since the correlations between the three dimensions of attitudes and future intention were rather small to moderate. Yet, it should be noted that the strongest correlation with future intention to use AI was with the emotional/affective dimension, indicating that emotions are stronger drives of future intention to use AI. Hence, H8 was retained. 4.1. LimitationsAs with all studies, the current empirical study has some limitations. Although the sample size is sufficient for the current analyses, it might be considered small, and thus, more advanced analytic approaches were not implemented. Another limitation of the study’s design is the cross-sectional correlational nature of the data, which does not permit the drawing of any causal conclusions. Additionally, the sampling method followed the principles of convenience sampling, which means that the results are not necessarily generalizable to the whole target population. Finally, the measure was not validated in the past for Greek higher education students and hence, this is the first attempt to provide evidence on its validity for the current target population. 4.2. Directions for Future Research and PracticeFuture research is needed in this area to replicate the principal components’ structure of this measure. Future studies can include diverse samples from different higher education university departments to gain a better picture of students’ attitudes toward AI in general. On the practical side of things, the current findings suggest that higher education students and, specifically, social sciences students held mostly positive attitudes toward AI but underscored the need for further education on this topic. This indicates that university departments should provide additional instruction to improve students’ awareness of the benefits and threats of the use of AI-powered tools. Teaching students about AI can be particularly beneficial since AI can be used to streamline the educational learning process [51]. Going forward, it is recommended to investigate if some items are differentially functioning between subject domains (e.g., social sciences vs. natural sciences) and between different demographic categories. 5. ConclusionsIn conclusion, the current study highlights a growing awareness amongst social sciences university students regarding the importance of AI in their education and future professional and daily lives. The findings provide support for integrating AI courses/modules in university education, reflecting the increasing relevance of AI for higher education. Additionally, the study confirms the value of a multidimensional measurement of attitudes toward AI, confirming the existence of cognitive, emotional, and behavioral indicators of attitudes. Finally, the study reveals socio-economic influences and underscores the critical role of general digital safety in promoting students’ positive attitudes toward AI. Supplementary MaterialsThe following supporting information can be downloaded at: https://www.mdpi.com/article/10.3390/educsci14090988/s1; Table S1: Item wordings in English (Adapted scale for the Greek university context); Table S2: Item wordings in Greek (Adapted scale for the Greek university context); Figure S1: Original scale as presented in Suh and Ahn (2022) [18].Author ContributionsConceptualization, A.K.; Methodology, A.K.; Software, A.K.; Validation, A.K. and I.G.K.; Formal analysis, A.K. and I.G.K.; Investigation, A.K.; Resources, A.K.; Data curation, A.K.; Writing—original draft, A.K. and I.G.K.; Writing—review & editing, A.K. and I.G.K.; Visualization, A.K.; Supervision, I.G.K.; Project administration, A.K. All authors have read and agreed to the published version of the manuscript.FundingThis research received no external funding. The APC was funded by the University of Cambridge’s Institutional Open Access Fund.Institutional Review Board StatementThe study was conducted in line with the Declaration of Helsinki. Ethics approval was received from the Department of Education and Social Work, University of Patras, Greece (7 March 2024).Informed Consent StatementParticipants were adults who consented to participate after being informed of the purposes of the study.Data Availability StatementThe data are available upon reasonable request.Conflicts of InterestThe authors have no known conflicts of interest to disclose.ReferencesSalas-Pilco, S.Z.; Yang, Y. Artificial Intelligence Applications in Latin American Higher Education: A Systematic Review. Int. J. Educ. Technol. High. Educ. 2022, 19, 21. [Google Scholar] [CrossRef]Zhai, X.; Chu, X.; Chai, C.S.; Jong, M.S.Y.; Istenic, A.; Spector, M.; Liu, J.-B.; Yuan, J.; Li, Y. A Review of Artificial Intelligence (Ai) in Education from 2010 to 2020. Complexity 2021, 2021, 8812542. [Google Scholar] [CrossRef]Chassignol, M.; Khoroshavin, A.; Klimova, A.; Bilyatdinova, A. Artificial Intelligence Trends in Education: A Narrative Overview. Procedia Comput. Sci. 2018, 136, 16–24. [Google Scholar] [CrossRef]Chen, L.; Chen, P.; Lin, Z. Artificial Intelligence in Education: A Review. IEEE Access 2020, 8, 75264–75278. [Google Scholar] [CrossRef]Rajpurkar, P.; Chen, E.; Banerjee, O.; Topol, E.J. AI in Health and Medicine. Nat. Med. 2022, 28, 31–38. [Google Scholar] [CrossRef]Cao, L. AI in Finance: Challenges, Techniques, and Opportunities. ACM Comput. Surv. 2022, 55, 64. [Google Scholar] [CrossRef]Makridakis, S. The Forthcoming Artificial Intelligence (AI) Revolution: Its Impact on Society and Firms. Futures 2017, 90, 46–60. [Google Scholar] [CrossRef]Walczak, K.; Cellary, W. Challenges for Higher Education in the Era of Widespread Access to Generative AI. EBR 2023, 9, 71–100. [Google Scholar] [CrossRef]Ouyang, F.; Zheng, L.; Jiao, P. Artificial Intelligence in Online Higher Education: A Systematic Review of Empirical Research from 2011 to 2020. Educ. Inf. Technol. 2022, 27, 7893–7925. [Google Scholar] [CrossRef]Xu, W.; Ouyang, F. A Systematic Review of AI Role in the Educational System Based on a Proposed Conceptual Framework. Educ. Inf. Technol. 2022, 27, 4195–4223. [Google Scholar] [CrossRef]Chen, X.; Xie, H.; Zou, D.; Hwang, G.-J. Application and Theory Gaps during the Rise of Artificial Intelligence in Education. Comput. Educ. Artif. Intell. 2020, 1, 100002. [Google Scholar] [CrossRef]Hajam, K.B.; Gahir, S. Unveiling the Attitudes of University Students toward Artificial Intelligence. J. Educ. Technol. Syst. 2024, 52, 335–345. [Google Scholar] [CrossRef]Kong, S.-C.; Man-Yin Cheung, W.; Zhang, G. Evaluation of an Artificial Intelligence Literacy Course for University Students with Diverse Study Backgrounds. Comput. Educ. Artif. Intell. 2021, 2, 100026. [Google Scholar] [CrossRef]Cohen, L.; Manion, L.; Morrison, K. Research Methods in Education, 8th ed.; Routledge: London, UK, 2018; ISBN 978-1-315-45651-5. [Google Scholar]Ajzen, I.; Schmidt, P. Changing Behavior Using the Theory of Planned Behavior. In The Handbook of Behavior Change; Hagger, M.S., Cameron, L.D., Hamilton, K., Hankonen, N., Lintunen, T., Eds.; Cambridge University Press: Cambridge, UK, 2020; pp. 17–31. ISBN 978-1-108-75011-0. [Google Scholar]Kemp, A.; Palmer, E.; Strelan, P. A Taxonomy of Factors Affecting Attitudes towards Educational Technologies for Use with Technology Acceptance Models. Br. J. Educ. Technol. 2019, 50, 2394–2413. [Google Scholar] [CrossRef]Ajzen, I.; Fishbein, M. Attitudes and the Attitude-Behavior Relation: Reasoned and Automatic Processes. Eur. Rev. Soc. Psychol. 2000, 11, 1–33. [Google Scholar] [CrossRef]Suh, W.; Ahn, S. Development and Validation of a Scale Measuring Student Attitudes toward Artificial Intelligence. SAGE Open 2022, 12, 215824402211004. [Google Scholar] [CrossRef]Metsärinne, M.; Kallio, M. How Are Students’ Attitudes Related to Learning Outcomes? Int. J. Technol. Des. Educ. 2016, 26, 353–371. [Google Scholar] [CrossRef]Almaraz-López, C.; Almaraz-Menéndez, F.; López-Esteban, C. Comparative Study of the Attitudes and Perceptions of University Students in Business Administration and Management and in Education toward Artificial Intelligence. Educ. Sci. 2023, 13, 609. [Google Scholar] [CrossRef]Pellas, N. The Influence of Sociodemographic Factors on Students’ Attitudes toward AI-Generated Video Content Creation. Smart Learn. Environ. 2023, 10, 57. [Google Scholar] [CrossRef]Yüzbaşıoğlu, E. Attitudes and Perceptions of Dental Students towards Artificial Intelligence. J. Dent. Educ. 2021, 85, 60–68. [Google Scholar] [CrossRef]Ghotbi, N.; Ho, M.T.; Mantello, P. Attitude of College Students towards Ethical Issues of Artificial Intelligence in an International University in Japan. AI Soc. 2022, 37, 283–290. [Google Scholar] [CrossRef]Pinto dos Santos, D.; Giese, D.; Brodehl, S.; Chon, S.H.; Staab, W.; Kleinert, R.; Maintz, D.; Baeßler, B. Medical Students’ Attitude towards Artificial Intelligence: A Multicentre Survey. Eur. Radiol. 2019, 29, 1640–1646. [Google Scholar] [CrossRef] [PubMed]Ajzen, I. The Theory of Planned Behavior: Frequently Asked Questions. Hum. Behav. Emerg. Technol. 2020, 2, 314–324. [Google Scholar] [CrossRef]Ardies, J.; De Maeyer, S.; Gijbels, D.; van Keulen, H. Students Attitudes towards Technology. Int. J. Technol. Des. Educ. 2015, 25, 43–65. [Google Scholar] [CrossRef]Bourdieu, P. The Forms of Capital. In Handbook of Theory and Research for the Sociology of Education; Richardson, J.G., Ed.; Greenwood Press: New York, NY, USA, 1986; pp. 241–258. [Google Scholar]Huang, X. Understanding Bourdieu-Cultural Capital and Habitus. Rev. Eur. Stud. 2019, 11, 45. [Google Scholar] [CrossRef]Ren, W.; Zhu, X.; Yang, J. The SES-Based Difference of Adolescents’ Digital Skills and Usages: An Explanation from Family Cultural Capital. Comput. Educ. 2022, 177, 104382. [Google Scholar] [CrossRef]Kim, S.-W.; Lee, Y. Investigation into the Influence of Socio-Cultural Factors on Attitudes toward Artificial Intelligence. Educ. Inf. Technol. 2024, 29, 9907–9935. [Google Scholar] [CrossRef]Gado, S.; Kempen, R.; Lingelbach, K.; Bipp, T. Artificial Intelligence in Psychology: How Can We Enable Psychology Students to Accept and Use Artificial Intelligence? Psychol. Learn. Teach. 2022, 21, 37–56. [Google Scholar] [CrossRef]Abdaljaleel, M.; Barakat, M.; Alsanafi, M.; Salim, N.A.; Abazid, H.; Malaeb, D.; Mohammed, A.H.; Hassan, B.A.R.; Wayyes, A.M.; Farhan, S.S.; et al. A Multinational Study on the Factors Influencing University Students’ Attitudes and Usage of ChatGPT. Sci. Rep. 2024, 14, 1983. [Google Scholar] [CrossRef]Zhang, C.; Schießl, J.; Plößl, L.; Hofmann, F.; Gläser-Zikuda, M. Acceptance of Artificial Intelligence among Pre-Service Teachers: A Multigroup Analysis. Int. J. Educ. Technol. High. Educ. 2023, 20, 49. [Google Scholar] [CrossRef]Acosta-Enriquez, B.G.; Arbulú Ballesteros, M.A.; Huamaní Jordan, O.; López Roca, C.; Saavedra Tirado, K. Analysis of College Students’ Attitudes toward the Use of ChatGPT in Their Academic Activities: Effect of Intent to Use, Verification of Information and Responsible Use. BMC Psychol. 2024, 12, 255. [Google Scholar] [CrossRef] [PubMed]Cho, K.A.; Seo, Y.H. Dual Mediating Effects of Anxiety to Use and Acceptance Attitude of Artificial Intelligence Technology on the Relationship between Nursing Students’ Perception of and Intention to Use Them: A Descriptive Study. BMC Nurs. 2024, 23, 212. [Google Scholar] [CrossRef] [PubMed]Eitel-Porter, R. Beyond the Promise: Implementing Ethical AI. AI Ethics 2021, 1, 73–80. [Google Scholar] [CrossRef]Ray, P.P. ChatGPT: A Comprehensive Review on Background, Applications, Key Challenges, Bias, Ethics, Limitations and Future Scope. Internet Things Cyber-Phys. Syst. 2023, 3, 121–154. [Google Scholar] [CrossRef]Katsantonis, I.; Gibbons, R.A.; Symonds, J.E.; Costello, N. To Persist or Not? Examining the Relations between Parental Education, Self-Regulation, School Engagement and Persistence in Post-Compulsory Education. Br. Educ. Res. J. 2024, 50, 2020–2042. [Google Scholar] [CrossRef]Goldthorpe, H.J. “Cultural Capital”: Some Critical Observations. Sociologica 2007, 2, 1–23. [Google Scholar] [CrossRef]Ren, Y.; Zhang, F.; Jiang, Y.; Huang, S. Family Socioeconomic Status, Educational Expectations, and Academic Achievement among Chinese Rural-to-Urban Migrant Adolescents: The Protective Role of Subjective Socioeconomic Status. J. Early Adolesc. 2021, 41, 1129–1150. [Google Scholar] [CrossRef]Pituch, K.A.; Stevens, J.P. Applied Multivariate Statistics for the Social Sciences: Analyses with SAS and IBM’s SPSS, 6th ed.; Routledge: New York, NY, USA; Taylor and Francis Group: London, UK, 2016; ISBN 978-0-415-83666-1. [Google Scholar]Horn, J.L. A Rationale and Test for the Number of Factors in Factor Analysis. Psychometrika 1965, 30, 179–185. [Google Scholar] [CrossRef]Velicer, W.F.; Eaton, C.A.; Fava, J.L. Construct Explication through Factor or Component Analysis: A Review and Evaluation of Alternative Procedures for Determining the Number of Factors or Components. In Problems and Solutions in Human Assessment; Goffin, R.D., Helmes, E., Eds.; Springer: Boston, MA, USA, 2000; pp. 41–71. ISBN 978-1-4613-6978-3. [Google Scholar]Revelle, W. Psych: Procedures for Psychological, Psychometric, and Personality Research; Northwestern University: Evanston, IL, USA, 2022. [Google Scholar]R Core Team. R: A Language and Environment for Statistical Computing; R Foundation for Statistical Computing: Vienna, Austria, 2023. [Google Scholar]Finch, W.H.; Immekus, C.J.; French, B.F. Applied Psychometrics Using SPSS and AMOS; Information Age Publishing: Charlotte, NC, USA, 2016. [Google Scholar]Moeller, J. A Word on Standardization in Longitudinal Studies: Don’t. Front. Psychol. 2015, 6, 1389. [Google Scholar] [CrossRef]Jann, B. HEATPLOT: Stata Module to Create Heat Plots and Hexagon Plots; Boston College Department of Economics: Boston, UK, 2019. [Google Scholar]Tabachnick, B.G.; Fidell, L.S. Using Multivariate Statistics, 6th ed.; Pearson: Boston, UK, 2012; ISBN 978-0-205-84957-4. [Google Scholar]Kline, R.B. Principles and Practice of Structural Equation Modeling, 5th ed.; Guilford Press: New York, NY, USA, 2023. [Google Scholar]Zawacki-Richter, O.; Marín, V.I.; Bond, M.; Gouverneur, F. Systematic Review of Research on Artificial Intelligence Applications in Higher Education—Where Are the Educators? Int. J. Educ. Technol. High. Educ. 2019, 16, 39. [Google Scholar] [CrossRef]Ochoa, W.; Reich, S.M. Parents’ Beliefs about the Benefits and Detriments of Mobile Screen Technologies for Their Young Children’s Learning: A Focus on Diverse Latine Mothers and Fathers. Front. Psychol. 2020, 11, 570712. [Google Scholar] [CrossRef] [PubMed]Marangunić, N.; Granić, A. Technology Acceptance Model: A Literature Review from 1986 to 2013. Univ. Access Inf. Soc. 2015, 14, 81–95. [Google Scholar] [CrossRef]Venkatesh, V.; Morris, M.G.; Davis, G.B.; Davis, F.D. User Acceptance of Information Technology: Toward a Unified View. MIS Q. 2003, 27, 425–478. [Google Scholar] [CrossRef] Figure 1. Percentage of students agreeing with the 25 items of attitudes toward AI scale (adapted). Note: T1 to T25 refers to the number of the items. Please refer to the Supplemental Materials Table S1 to check the correspondence between the item wordings and the item labels. Figure 1. Percentage of students agreeing with the 25 items of attitudes toward AI scale (adapted). Note: T1 to T25 refers to the number of the items. Please refer to the Supplemental Materials Table S1 to check the correspondence between the item wordings and the item labels. Figure 2. Bivariate Pearson correlations between the key covariates and outcomes. Note: *** p < 0.001; ** p < 0.01; * p < 0.05; GENDER: female vs. male; YEAR: year of studies; FED: Father’s education level; MED: Mother’s education level; FREQUSE: Frequency of future use; SAFETY: General safety in digital transactions; CPRACTICES: Cultural practices; COGNITIVE: Cognitive dimension of AI attitudes; BEHAVIOURAL: Behavioral dimension of AI attitudes; EMOTIONAL: Emotional dimension of AI attitudes; Stronger positive correlations are depicted with colors closer to fuchsia, whereas negative correlations are depicted with deeper blue colors. Figure 2. Bivariate Pearson correlations between the key covariates and outcomes. Note: *** p < 0.001; ** p < 0.01; * p < 0.05; GENDER: female vs. male; YEAR: year of studies; FED: Father’s education level; MED: Mother’s education level; FREQUSE: Frequency of future use; SAFETY: General safety in digital transactions; CPRACTICES: Cultural practices; COGNITIVE: Cognitive dimension of AI attitudes; BEHAVIOURAL: Behavioral dimension of AI attitudes; EMOTIONAL: Emotional dimension of AI attitudes; Stronger positive correlations are depicted with colors closer to fuchsia, whereas negative correlations are depicted with deeper blue colors. Table 1. Summary of selected empirical studies on students’ attitudes toward AI in higher education. Table 1. Summary of selected empirical studies on students’ attitudes toward AI in higher education. StudyStudents’ Academic DisciplineStudy DesignMeasure(s)Outcome(s)LimitationsCountry[20]Mixed (Economics/Business/Education)Mixed (survey, interview)Self-report, single itemsGenerally positive attitudes toward general AINot multidimensional, not validated measures, no reliability reportedSpain[12]Mixed (Arts, Science, Commerce)QUANSelf-report,multiple itemsVery positive attitudes toward general AINo validation reported, no reliability reportedIndia[21]Mixed (Arts, Education, STEM, Business, Media)QUANSelf-report,multiple itemsVery positive attitudes toward Machine LearningNot distinguishing between STEM vs. non-STEM studentsGreece[22]Dental studentsQUANSelf-report,multiple itemsVery positive attitudes toward AI in dentistryNo validation reported,Not clearly distinguishing between cognitive, affective, or behavioural dimensions Türkiye[23]Mixed (unclassified)QUANEssay task (lexical analysis)Generally positive emotions (trust) and some concerns about unemploymentNot distinguishing between STEM vs. non-STEM studentsJapan[24]Radiology medical studentsQUANSelf-report,Single and multiple itemsGenerally positive attitudes toward AINo validation reported, no reliability reported, and no clear distinction between cognitive, affective, or behavioural dimensionsGermany Note: QUAN: Quantitative; QUAL: Qualitative: Mixed: Both QUAN and QUAL. Table 2. Principal components analysis of the attitudes toward AI scale-adapted for a Greek sample of higher education students (rotated loadings matrix). Table 2. Principal components analysis of the attitudes toward AI scale-adapted for a Greek sample of higher education students (rotated loadings matrix). ItemsPrincipal Component Loadings123Behavioural Component (1) 1.I like using apps related to AI.0.607 2.It is fun to learn about AI.0.770 3.I want to continue learning about AI.0.750 4.I’m interested in AI-related TV programs or online videos.0.749 5.I want to make something that makes human life more convenient through AI.0.632 6.I am interested in the development of AI.0.666 7.It is interesting to use AI.0.618 8.I think that there should be more class time devoted to AI in university.0.550 Cognitive Component (2) 1.I think that it is important to integrate AI in my university studies. 0.567 2.AI classes are important. 0.777 3.I think that lessons about AI should be taught in university. 0.785 4.I think every university student should learn about AI in university. 0.692 5.AI is very important for developing society. a 0.506 6.AI produces more good than bad. a 0.474 7.It is worth to know AI very well. a 0.466 Affective/Emotional Component (3) 1.I think AI makes people’s lives more convenient. 0.4412.AI is related to my daily life. 0.6073.I will use AI to solve problems in daily life. 0.7674.AI helps me solve problems in real life. 0.6295.I will need AI in my life in the future. 0.5156.AI is necessary for everyone. 0.5577.I think that most jobs in the future will require knowledge related to AI. 0.4268.I can use well the apps based on AI. b 0.6219.I will use AI in the future in my professional life. b 0.42310.It would be very helpful for me to have available AI apps in my professional life. b 0.441PCA Eigenvalues9.1161.8881.683% Variance explained by each component35.0607.2626.474Cronbach’s alpha per component0.8950.8160.828Transformed mean score across items per component54.9862.0864.06 Note: a This item loads on a different component compared to the initial validation study; b This item is a new adaption to the scale; Varimax rotation with Kaiser Normalization; items were adapted from Suh and Ahn (2022) [18]; No additional permission was required due to CC-BY 4 license. Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style Katsantonis, A.; Katsantonis, I.G. University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Educ. Sci. 2024, 14, 988. https://doi.org/10.3390/educsci14090988 AMA Style Katsantonis A, Katsantonis IG. University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Education Sciences. 2024; 14(9):988. https://doi.org/10.3390/educsci14090988 Chicago/Turabian Style Katsantonis, Argyrios, and Ioannis G. Katsantonis. 2024. ""University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes"" Education Sciences 14, no. 9: 988. https://doi.org/10.3390/educsci14090988 APA Style Katsantonis, A., & Katsantonis, I. G. (2024). University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Education Sciences, 14(9), 988. https://doi.org/10.3390/educsci14090988 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. Article Metrics Yes Citations No citations were found for this article, but you may check on Google Scholar No Article Access Statistics For more information on the journal statistics, click here. Multiple requests from the same IP address are counted as one view. Supplementary Material Supplementary File 1: ZIP-Document (ZIP, 465 KiB) clear Zoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene Cite Export citation file: BibTeX | EndNote | RIS MDPI and ACS Style Katsantonis, A.; Katsantonis, I.G. University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Educ. Sci. 2024, 14, 988. https://doi.org/10.3390/educsci14090988 AMA Style Katsantonis A, Katsantonis IG. University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Education Sciences. 2024; 14(9):988. https://doi.org/10.3390/educsci14090988 Chicago/Turabian Style Katsantonis, Argyrios, and Ioannis G. Katsantonis. 2024. ""University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes"" Education Sciences 14, no. 9: 988. https://doi.org/10.3390/educsci14090988 APA Style Katsantonis, A., & Katsantonis, I. G. (2024). University Students’ Attitudes toward Artificial Intelligence: An Exploratory Study of the Cognitive, Emotional, and Behavioural Dimensions of AI Attitudes. Education Sciences, 14(9), 988. https://doi.org/10.3390/educsci14090988 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. clear Educ. Sci., EISSN 2227-7102, Published by MDPI RSS Content Alert Further Information Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications and newsletters from MDPI journals Acoustics Acta Microbiologica Hellenica Actuators Administrative Sciences Adolescents Advances in Respiratory Medicine Aerobiology Aerospace Agriculture AgriEngineering Agrochemicals Agronomy AI Air Algorithms Allergies Alloys Analytica Analytics Anatomia Anesthesia Research Animals Antibiotics Antibodies Antioxidants Applied Biosciences Applied Mechanics Applied Microbiology Applied Nano Applied Sciences Applied System Innovation AppliedChem AppliedMath Aquaculture Journal Architecture Arthropoda Arts Astronomy Atmosphere Atoms Audiology Research Automation Axioms Bacteria Batteries Behavioral Sciences Beverages Big Data and Cognitive Computing BioChem Bioengineering Biologics Biology Biology and Life Sciences Forum Biomass Biomechanics BioMed Biomedicines BioMedInformatics Biomimetics Biomolecules Biophysica Biosensors BioTech Birds Blockchains Brain Sciences Buildings Businesses C Cancers Cardiogenetics Catalysts Cells Ceramics Challenges ChemEngineering Chemistry Chemistry Proceedings Chemosensors Children Chips CivilEng Clean Technologies Climate Clinical and Translational Neuroscience Clinical Bioenergetics Clinics and Practice Clocks & Sleep Coasts Coatings Colloids and Interfaces Colorants Commodities Complications Compounds Computation Computer Sciences & Mathematics Forum Computers Condensed Matter Conservation Construction Materials Corrosion and Materials Degradation Cosmetics COVID Crops Cryo Cryptography Crystals Current Issues in Molecular Biology Current Oncology Dairy Data Dentistry Journal Dermato Dermatopathology Designs Diabetology Diagnostics Dietetics Digital Disabilities Diseases Diversity DNA Drones Drugs and Drug Candidates Dynamics Earth Ecologies Econometrics Economies Education Sciences Electricity Electrochem Electronic Materials Electronics Emergency Care and Medicine Encyclopedia Endocrines Energies Energy Storage and Applications Eng Engineering Proceedings Entropy Environmental Sciences Proceedings Environments Epidemiologia Epigenomes European Burn Journal European Journal of Investigation in Health, Psychology and Education Fermentation Fibers FinTech Fire Fishes Fluids Foods Forecasting Forensic Sciences Forests Fossil Studies Foundations Fractal and Fractional Fuels Future Future Internet Future Pharmacology Future Transportation Galaxies Games Gases Gastroenterology Insights Gastrointestinal Disorders Gastronomy Gels Genealogy Genes Geographies GeoHazards Geomatics Geometry Geosciences Geotechnics Geriatrics Glacies Gout, Urate, and Crystal Deposition Disease Grasses Hardware Healthcare Hearts Hemato Hematology Reports Heritage Histories Horticulturae Hospitals Humanities Humans Hydrobiology Hydrogen Hydrology Hygiene Immuno Infectious Disease Reports Informatics Information Infrastructures Inorganics Insects Instruments Intelligent Infrastructure and Construction International Journal of Environmental Research and Public Health International Journal of Financial Studies International Journal of Molecular Sciences International Journal of Neonatal Screening International Journal of Plant Biology International Journal of Topology International Journal of Translational Medicine International Journal of Turbomachinery, Propulsion and Power International Medical Education Inventions IoT ISPRS International Journal of Geo-Information J Journal of Ageing and Longevity Journal of Cardiovascular Development and Disease Journal of Clinical & Translational Ophthalmology Journal of Clinical Medicine Journal of Composites Science Journal of Cybersecurity and Privacy Journal of Dementia and Alzheimer's Disease Journal of Developmental Biology Journal of Experimental and Theoretical Analyses Journal of Functional Biomaterials Journal of Functional Morphology and Kinesiology Journal of Fungi Journal of Imaging Journal of Intelligence Journal of Low Power Electronics and Applications Journal of Manufacturing and Materials Processing Journal of Marine Science and Engineering Journal of Market Access & Health Policy Journal of Molecular Pathology Journal of Nanotheranostics Journal of Nuclear Engineering Journal of Otorhinolaryngology, Hearing and Balance Medicine Journal of Parks Journal of Personalized Medicine Journal of Pharmaceutical and BioTech Industry Journal of Respiration Journal of Risk and Financial Management Journal of Sensor and Actuator Networks Journal of the Oman Medical Association Journal of Theoretical and Applied Electronic Commerce Research Journal of Vascular Diseases Journal of Xenobiotics Journal of Zoological and Botanical Gardens Journalism and Media Kidney and Dialysis Kinases and Phosphatases Knowledge LabMed Laboratories Land Languages Laws Life Limnological Review Lipidology Liquids Literature Livers Logics Logistics Lubricants Lymphatics Machine Learning and Knowledge Extraction Machines Macromol Magnetism Magnetochemistry Marine Drugs Materials Materials Proceedings Mathematical and Computational Applications Mathematics Medical Sciences Medical Sciences Forum Medicina Medicines Membranes Merits Metabolites Metals Meteorology Methane Methods and Protocols Metrics Metrology Micro Microbiology Research Micromachines Microorganisms Microplastics Minerals Mining Modelling Modern Mathematical Physics Molbank Molecules Multimodal Technologies and Interaction Muscles Nanoenergy Advances Nanomanufacturing Nanomaterials NDT Network Neuroglia Neurology International NeuroSci Nitrogen Non-Coding RNA Nursing Reports Nutraceuticals Nutrients Obesities Oceans Onco Optics Oral Organics Organoids Osteology Oxygen Parasitologia Particles Pathogens Pathophysiology Pediatric Reports Pets Pharmaceuticals Pharmaceutics Pharmacoepidemiology Pharmacy Philosophies Photochem Photonics Phycology Physchem Physical Sciences Forum Physics Physiologia Plants Plasma Platforms Pollutants Polymers Polysaccharides Populations Poultry Powders Proceedings Processes Prosthesis Proteomes Psychiatry International Psychoactives Psychology International Publications Quantum Beam Science Quantum Reports Quaternary Radiation Reactions Real Estate Receptors Recycling Regional Science and Environmental Economics Religions Remote Sensing Reports Reproductive Medicine Resources Rheumato Risks Robotics Ruminants Safety Sci Scientia Pharmaceutica Sclerosis Seeds Sensors Separations Sexes Signals Sinusitis Smart Cities Social Sciences Société Internationale d’Urologie Journal Societies Software Soil Systems Solar Solids Spectroscopy Journal Sports Standards Stats Stresses Surfaces Surgeries Surgical Techniques Development Sustainability Sustainable Chemistry Symmetry SynBio Systems Targets Taxonomy Technologies Telecom Textiles Thalassemia Reports Therapeutics Thermo Time and Space Tomography Tourism and Hospitality Toxics Toxins Transplantology Trauma Care Trends in Higher Education Tropical Medicine and Infectious Disease Universe Urban Science Uro Vaccines Vehicles Venereology Veterinary Sciences Vibration Virtual Worlds Viruses Vision Waste Water Wild Wind Women World World Electric Vehicle Journal Youth Zoonotic Diseases Select optionsAcousticsActa Microbiologica HellenicaActuatorsAdministrative SciencesAdolescentsAdvances in Respiratory MedicineAerobiologyAerospaceAgricultureAgriEngineeringAgrochemicalsAgronomyAIAirAlgorithmsAllergiesAlloysAnalyticaAnalyticsAnatomiaAnesthesia ResearchAnimalsAntibioticsAntibodiesAntioxidantsApplied BiosciencesApplied MechanicsApplied MicrobiologyApplied NanoApplied SciencesApplied System InnovationAppliedChemAppliedMathAquaculture JournalArchitectureArthropodaArtsAstronomyAtmosphereAtomsAudiology ResearchAutomationAxiomsBacteriaBatteriesBehavioral SciencesBeveragesBig Data and Cognitive ComputingBioChemBioengineeringBiologicsBiologyBiology and Life Sciences ForumBiomassBiomechanicsBioMedBiomedicinesBioMedInformaticsBiomimeticsBiomoleculesBiophysicaBiosensorsBioTechBirdsBlockchainsBrain SciencesBuildingsBusinessesCCancersCardiogeneticsCatalystsCellsCeramicsChallengesChemEngineeringChemistryChemistry ProceedingsChemosensorsChildrenChipsCivilEngClean TechnologiesClimateClinical and Translational NeuroscienceClinical BioenergeticsClinics and PracticeClocks & SleepCoastsCoatingsColloids and InterfacesColorantsCommoditiesComplicationsCompoundsComputationComputer Sciences & Mathematics ForumComputersCondensed MatterConservationConstruction MaterialsCorrosion and Materials DegradationCosmeticsCOVIDCropsCryoCryptographyCrystalsCurrent Issues in Molecular BiologyCurrent OncologyDairyDataDentistry JournalDermatoDermatopathologyDesignsDiabetologyDiagnosticsDieteticsDigitalDisabilitiesDiseasesDiversityDNADronesDrugs and Drug CandidatesDynamicsEarthEcologiesEconometricsEconomiesEducation SciencesElectricityElectrochemElectronic MaterialsElectronicsEmergency Care and MedicineEncyclopediaEndocrinesEnergiesEnergy Storage and ApplicationsEngEngineering ProceedingsEntropyEnvironmental Sciences ProceedingsEnvironmentsEpidemiologiaEpigenomesEuropean Burn JournalEuropean Journal of Investigation in Health, Psychology and EducationFermentationFibersFinTechFireFishesFluidsFoodsForecastingForensic SciencesForestsFossil StudiesFoundationsFractal and FractionalFuelsFutureFuture InternetFuture PharmacologyFuture TransportationGalaxiesGamesGasesGastroenterology InsightsGastrointestinal DisordersGastronomyGelsGenealogyGenesGeographiesGeoHazardsGeomaticsGeometryGeosciencesGeotechnicsGeriatricsGlaciesGout, Urate, and Crystal Deposition DiseaseGrassesHardwareHealthcareHeartsHematoHematology ReportsHeritageHistoriesHorticulturaeHospitalsHumanitiesHumansHydrobiologyHydrogenHydrologyHygieneImmunoInfectious Disease ReportsInformaticsInformationInfrastructuresInorganicsInsectsInstrumentsIntelligent Infrastructure and ConstructionInternational Journal of Environmental Research and Public HealthInternational Journal of Financial StudiesInternational Journal of Molecular SciencesInternational Journal of Neonatal ScreeningInternational Journal of Plant BiologyInternational Journal of TopologyInternational Journal of Translational MedicineInternational Journal of Turbomachinery, Propulsion and PowerInternational Medical EducationInventionsIoTISPRS International Journal of Geo-InformationJJournal of Ageing and LongevityJournal of Cardiovascular Development and DiseaseJournal of Clinical & Translational OphthalmologyJournal of Clinical MedicineJournal of Composites ScienceJournal of Cybersecurity and PrivacyJournal of Dementia and Alzheimer's DiseaseJournal of Developmental BiologyJournal of Experimental and Theoretical AnalysesJournal of Functional BiomaterialsJournal of Functional Morphology and KinesiologyJournal of FungiJournal of ImagingJournal of IntelligenceJournal of Low Power Electronics and ApplicationsJournal of Manufacturing and Materials ProcessingJournal of Marine Science and EngineeringJournal of Market Access & Health PolicyJournal of Molecular PathologyJournal of NanotheranosticsJournal of Nuclear EngineeringJournal of Otorhinolaryngology, Hearing and Balance MedicineJournal of ParksJournal of Personalized MedicineJournal of Pharmaceutical and BioTech IndustryJournal of RespirationJournal of Risk and Financial ManagementJournal of Sensor and Actuator NetworksJournal of the Oman Medical AssociationJournal of Theoretical and Applied Electronic Commerce ResearchJournal of Vascular DiseasesJournal of XenobioticsJournal of Zoological and Botanical GardensJournalism and MediaKidney and DialysisKinases and PhosphatasesKnowledgeLabMedLaboratoriesLandLanguagesLawsLifeLimnological ReviewLipidologyLiquidsLiteratureLiversLogicsLogisticsLubricantsLymphaticsMachine Learning and Knowledge ExtractionMachinesMacromolMagnetismMagnetochemistryMarine DrugsMaterialsMaterials ProceedingsMathematical and Computational ApplicationsMathematicsMedical SciencesMedical Sciences ForumMedicinaMedicinesMembranesMeritsMetabolitesMetalsMeteorologyMethaneMethods and ProtocolsMetricsMetrologyMicroMicrobiology ResearchMicromachinesMicroorganismsMicroplasticsMineralsMiningModellingModern Mathematical PhysicsMolbankMoleculesMultimodal Technologies and InteractionMusclesNanoenergy AdvancesNanomanufacturingNanomaterialsNDTNetworkNeurogliaNeurology InternationalNeuroSciNitrogenNon-Coding RNANursing ReportsNutraceuticalsNutrientsObesitiesOceansOncoOpticsOralOrganicsOrganoidsOsteologyOxygenParasitologiaParticlesPathogensPathophysiologyPediatric ReportsPetsPharmaceuticalsPharmaceuticsPharmacoepidemiologyPharmacyPhilosophiesPhotochemPhotonicsPhycologyPhyschemPhysical Sciences ForumPhysicsPhysiologiaPlantsPlasmaPlatformsPollutantsPolymersPolysaccharidesPopulationsPoultryPowdersProceedingsProcessesProsthesisProteomesPsychiatry InternationalPsychoactivesPsychology InternationalPublicationsQuantum Beam ScienceQuantum ReportsQuaternaryRadiationReactionsReal EstateReceptorsRecyclingRegional Science and Environmental EconomicsReligionsRemote SensingReportsReproductive MedicineResourcesRheumatoRisksRoboticsRuminantsSafetySciScientia PharmaceuticaSclerosisSeedsSensorsSeparationsSexesSignalsSinusitisSmart CitiesSocial SciencesSociété Internationale d’Urologie JournalSocietiesSoftwareSoil SystemsSolarSolidsSpectroscopy JournalSportsStandardsStatsStressesSurfacesSurgeriesSurgical Techniques DevelopmentSustainabilitySustainable ChemistrySymmetrySynBioSystemsTargetsTaxonomyTechnologiesTelecomTextilesThalassemia ReportsTherapeuticsThermoTime and SpaceTomographyTourism and HospitalityToxicsToxinsTransplantologyTrauma CareTrends in Higher EducationTropical Medicine and Infectious DiseaseUniverseUrban ScienceUroVaccinesVehiclesVenereologyVeterinary SciencesVibrationVirtual WorldsVirusesVisionWasteWaterWildWindWomenWorldWorld Electric Vehicle JournalYouthZoonotic Diseases Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Terms and Conditions Privacy Policy We use cookies on our website to ensure you get the best experience. Read more about our cookies here. Accept Share Link Copy clear Share https://www.mdpi.com/2947406 clear Back to TopTop Annotate Highlight"
"     Consumer attitude toward using artificial intelligence (AI) devices in hospitality services
 |  Emerald Insight",https://www.emerald.com/insight/content/doi/10.1108/jhti-08-2023-0551/full/html,未知,2024-11-21 17:05:23,en,AI AND attitude,google,"Consumer attitude toward using artificial intelligence (AI) devices in hospitality services | Emerald Insight <iframe src=""https://www.googletagmanager.com/ns.html?id=GTM-KGFMJXR"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> Books and journals Case studies Expert Briefings Open Access Publish with us Advanced search To read this content please select one of the options below: Access and purchase options Purchase options Rent this content from DeepDyve Rent from DeepDyve Other access You may be able to access this content by logging in via your Emerald profile. Login If you think you should have access to this content, click to contact our support team. Contact us Please note you do not have access to teaching notes Access and purchase options Purchase options Other access You may be able to access teaching notes by logging in via your Emerald profile. Login If you think you should have access to this content, click to contact our support team. Contact us Consumer attitude toward using artificial intelligence (AI) devices in hospitality services Kamrul Hasan Bhuiyan (School of Interdisciplinary Studies, University of Glasgow, Glasgow, UK) Selim Ahmed (Department of Business Administration, World University of Bangladesh, Dhaka, Bangladesh) (Research Fellow, INTI International University, Nilai, Malaysia) Israt Jahan (School of Social and Political Sciences, University of Glasgow, Glasgow, UK) Journal of Hospitality and Tourism Insights ISSN: 2514-9792 Article publication date: 16 February 2024 Issue publication date: 30 April 2024 Downloads 914 Abstract Purpose The study investigates the consumer’s attitude to using artificial intelligence (AI) devices in hospitality service settings considering social influence, hedonic motivation, anthropomorphism, effort expectancy, performance expectancy and emotions. Design/methodology/approach This study employed a quantitative methodology to collect data from Bangladeshi consumers who utilized AI-enabled technologies in the hospitality sector. A total of 343 data were collected using a purposive sampling method. The SmartPLS 4.0 software was used to determine the constructs' internal consistency, reliability and validity. This study also applied the partial least squares structural equation modeling (PLS-SEM) to test the research model and hypotheses. Findings The finding shows that consumer attitude toward AI is influenced by social influence, hedonic motivation, anthropomorphism, performance and effort expectancy and emotions. Specifically, hedonic motivation, social influence and anthropomorphism affect performance and effort expectations, affecting consumer emotion. Moreover, emotions ultimately influenced the perceptions of hotel customers' willingness to use AI devices. Practical implications This study provides a practical understanding of issues when adopting more stringent AI-enabled devices in the hospitality sector. Managers, practitioners and decision-makers will get helpful information discussed in this article. Originality/value This study investigates the perceptions of guests' attitudes toward the use of AI devices in hospitality services. This study emphasizes the cultural context of the hospitality industry in Bangladesh, but its findings may be reflected in other areas and regions. Keywords Consumer attitude Artificial intelligence Hospitality Technology acceptance PLS-SEM Citation Bhuiyan, K.H., Ahmed, S. and Jahan, I. (2024), ""Consumer attitude toward using artificial intelligence (AI) devices in hospitality services"", Journal of Hospitality and Tourism Insights, Vol. 7 No. 2, pp. 968-985. https://doi.org/10.1108/JHTI-08-2023-0551 Publisher: Emerald Publishing Limited Copyright © 2024, Emerald Publishing Limited Related articles Support & Feedback Manage cookies All feedback is valuable Please share your general feedback Report an issue or find answers to frequently asked questions Contact Customer Support © 2024 Emerald Publishing Limited. All rights reserved, including rights for text and data mining, artificial intelligence training and similar technologies. Services Authors Editors Librarians Researchers Reviewers About About Emerald Working for Emerald Contact us Publication sitemap Policies and information Privacy notice Site policies Modern Slavery Act Chair of Trustees governance statement Accessibility ×"
Home,https://attitudestoai.uk/,未知,2024-11-21 17:05:50,en,AI AND attitude,google,"Home Skip to main content AI Insights How do people feel about AI? Findings Findings Explore the executive summary and how to read the findings depending on your interests. Key findings Awareness and experience Benefits and concerns Attitudes towards technologies Understanding, trust, and thoughts about regulation Insights into attitudes towards AI-powered technologies Download and citation About the survey About the survey Find out why and how the survey was run and who took part. Why did we run this survey? Definitions of AI uses Methodology Partner information and acknowledgements Download and citation Comment & context Comment & context Discover context related to the survey findings and comment by experts and from different perspectives. Blog AI in practice Explainers How do people feel about AI? A nationally representative survey of public attitudes to artificial intelligence (AI) in BritainAI technologies already interact with many aspects of people’s lives, from using facial recognition to unlock mobile phones, to predicting the risk of cancer.In November 2022, the Ada Lovelace Institute and The Alan Turing Institute conducted a nationally representative survey of over 4,000 adults in Britain, to understand how the public currently experience AI. Explore findings 72% Nearly three-quarters of the British public express concern about driverless cars 56% Using AI to predict the risk of cancer is perceived to be one of the most beneficial technologies, but over half of British adults are concerned about relying too heavily on this technology over professional judgements 70% 70% of British adults feel that speeding up processing at border control is a benefit of facial recognition technology About the survey Find out more about the different AI uses, and learn why and how the survey was run About the survey Comment & context AI in practice 5 July 2024 Climate research simulation AI in practice 5 July 2024 Virtual reality in education AI in practice 5 July 2024 Virtual healthcare assistant Explore comment & context hello@adalovelaceinstitute.org +44 (0)20 7323 6274 100 St John Street London, EC1M 4EH twitter linkedin Citation and PDF Privacy © 2024 Ada Lovelace Institute and The Alan Turing Institute Website by William Joseph"
Three Crucial Attitudes For Adopting AI,https://www.forbes.com/councils/forbestechcouncil/2023/07/19/three-crucial-attitudes-for-adopting-ai/,未知,2024-11-21 17:06:00,en,AI AND attitude,google,"Three Crucial Attitudes For Adopting AISubscribe To NewslettersBETATHIS IS A BETA EXPERIENCE. OPT-OUT HEREMore From ForbesNov 7, 2024,12:26pm ESTForbes Creator Upfronts In Partnership With Walmart CreatorNov 20, 2024,01:53pm ESTHow AI Is Reshaping Industries And Creating Tomorrow’s Job MarketNov 20, 2024,01:15pm EST20 Game-Changing Ways Devs Can Use GenAI For CodingNov 20, 2024,10:15am ESTOpportunities And Challenges Of Applying Generative AI To BusinessesNov 20, 2024,10:00am ESTWhy Supply Chain Technology Keeps Failing—And How To Fix ItNov 20, 2024,09:45am ESTThe Art Of Being A CISO: What They Don't Teach You In SchoolNov 20, 2024,09:30am ESTConquering Complexity And Optimizing Inventory Management: A Strategic GuideNov 20, 2024,09:15am ESTWhy Savvy Executives View IT As A Profit CenterNov 20, 2024,09:00am ESTEnsuring Mental Health Issues Aren't Demonized In LeadershipEdit StoryForbesInnovationThree Crucial Attitudes For Adopting AIAl KingsleyForbes Councils MemberForbes Technology CouncilCOUNCIL POSTExpertise from Forbes Councils members, operated under license. Opinions expressed are those of the author.| Membership (fee-based)Jul 19, 2023,09:30am EDTShare to FacebookShare to TwitterShare to LinkedinAl Kingsley is CEO of NetSupport, Chair of a multi-academy trust in the U.K, tech writer, speaker and author of multiple education books. getty For people who don't follow technology developments, it might seem that artificial intelligence (AI) has come from nowhere. Even for a long-time IT professional like myself, everything concerning AI appears to have suddenly shifted up a gear. It's being talked about wherever you turn: how it will change how we do things, be cleverer than humans and take over the world. This is a massive generalization, of course. But for all those—whether you're a business, a school or an individual—taking their first steps into AI (or into their awareness of it), I've outlined three attitudes in this article that I've adopted that might help you keep things in perspective. 1. Diminish the fear. People often fear what they don't understand. Right now, there are plenty of alarmist headlines about AI all over the internet and social media. Yes, AI presents a good many risks, but it is now in the public domain and here to stay. So, rather than stick our heads in the sand, we need to educate ourselves to be wise to them. Think of it this way: We now have this digital stranger in our workplaces. How do we know whether we can trust it? It's not like getting to know a human colleague with all the nonverbal body language cues we innately pick up on to help inform our judgment of their character and competence. Instead, we should make ourselves aware of how AI learns and works—and this awareness will need to be ongoing, given the speed of its current development. In a nutshell, generative AI, the kind that works from a text prompt to create something ""new,"" is trained on massive amounts of data containing examples of desired outputs that appear authentically human. Essentially, it's drawing on the information it has been fed, so what it creates is not necessarily ""new""—I've even heard of it being called a ""plagiarism algorithm."" As it stands, the basic entry level of ChatGPT is mainly trained on data up to 2021, and even the live search versions of ChatGPT or Bard are still error-prone. At this point, we humans need to validate the output and check the facts cited. Once we understand this position, we can adjust our thinking to regard our AI ""stranger"" as an assistant rather than an authority. 2. Be aware of the risks. While it's important to not only focus on fear-driven outcomes, it’s still sensible to be aware of potential pitfalls where AI is concerned. For example, chatbots run on AI. Increasingly, businesses use them to operate low-level help functions on their websites to increase efficiency and free up staff to do other tasks. However, they collect enormous amounts of data, which presents all sorts of issues around privacy; indeed, many companies are simply not even close to adhering to data protection regulations. What’s more, there are reports of chatbots being commandeered for hacking purposes (cybersecurity risk), being used to access credit card details (financial risk) or more complex phishing email fraud to elicit all kinds of personal information. Exercising caution and checking any requests you are suspicious of at the source is your key to staying safe. Also, read all you can about these risks as they develop, so you can be vigilant and alert to any unorthodox activity. 3. Embrace the possibilities. However, let’s not think AI is all risk and no reward. It is opening up new possibilities and disrupting many sectors. For example, it’s helping with even earlier detection of cancers; online, its algorithms serve up personalized suggested content; it’s helping us to tackle climate change with its forecasting and analytics tools; saving teachers time creating lesson plans—and so much more. Generative AI has the potential to make businesses more productive in terms of efficiency and accuracy. Plus, if we want to achieve good results in the outputs we request, it will encourage us to record better quality data to avoid the ""garbage in, garbage out"" scenario. We will also learn to become skilled in writing prompts to elicit the exact information we need in the correct format. A Tool, Not A Master As people rush to try to understand AI, what it can do and what impact it will have in the future on education, work and life, we should not lose sight of what we should use it for to improve human life, not negatively impact it. Just because AI is here and available to us does not mean we should apply it to every situation. The use cases we choose should align with human values and what humans need. We can only ensure this happens by keeping ourselves informed and aware of what is happening in its orbit. Now we've started to get to know our digital stranger, let's build on that relationship. Used as a tool, it can empower us to do more, move faster, be more effective and shake up how we work—but it's not our master. Humans are still needed to judge bias and context, make emotional calls and oversee results and ethics. Those are the things we should keep at the center of our thinking. Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify? Follow me on Twitter or LinkedIn. Check out my website. Al KingsleyEditorial StandardsForbes Accolades"
Global Study: Consumer and Employee Attitudes Toward AI | XM Institute,https://www.xminstitute.com/research/global-consumer-employee-attitude-ai/,未知,2024-11-21 17:06:34,en-US,AI AND attitude,google,"Global Study: Consumer and Employee Attitudes Toward AI | XM Institute <iframe src=""https://www.googletagmanager.com/ns.html?id=GTM-WLGMQG9"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> secret message XM LIBRARYEVENTSTRAININGXM PROSCERTIFICATIONABOUT XM Library Events Training XM Pros Certification About XM LIBRARY RESEARCH RESEARCH Global Study: Consumer and Employee Attitudes Toward AI Bruce Temkin, Cecelia Herbert, and Talia Quaadgras, November 15, 2023 Print EXECUTIVE SUMMARY As part of Qualtrics’ and Qualtrics XM Institute’s global employee and global consumer studies, respondents shared their concerns about artificial intelligence (AI), their comfort using AI to complete five common consumer activities, and their desire to use AI to complete seven common employee activities. From their responses, we were able to understand the global appetite for artificial intelligence. Key Findings As part of Qualtrics’ and Qualtrics XM Institute’s Global Employee and Global Consumer Studies, we asked respondents a series of questions to understand their attitudes toward artificial intelligence (AI). These questions explored their concerns about AI as well as their comfort using the technology to complete common consumer and employee activities. We used their responses to gauge the global appetite for artificial intelligence. We learned that: Consumers in most countries think AI will have a positive impact on society. Consumers in 20 of 26 countries think AI will positively affect society. Consumers are most optimistic in UAE, India, and Singapore with a net positive view of +57, +52, and +52, respectively. Meanwhile, 6 countries have a net negative view of AI, with the most negative outlook projected in Canada (-20), the UK (-16), and the US (-12). Consumers are most concerned about the lack of a human being to connect with. On average, 48% of consumers are highly concerned about the lack of a human being to connect with if companies use AI to automate their interactions with customers. They are also highly concerned about the poor quality of interactions and loss of jobs for employees, with 45% of respondents citing each as a concern. There is no one-size-fits-all approach to AI. Consumers from different countries expressed substantially different levels of comfort using AI to complete common tasks, with gaps up to 59 percentage points depending on the activity. Employees’ interest in using AI to accomplish tasks also diverges widely across countries, differing by up to 48 percentage points. Appetite for AI support varies. Consumers feel most comfortable using AI to check the status of an order (73%), a relatively light-touch task, compared to just 45% who are comfortable using AI to get advice about a medical problem. Meanwhile 61% of employees want AI help with writing tasks, and only 29% would be interviewed for a new job/promotion by [an] AI bot. Figures Here are the figures in this Data Snapshot: Societal Impact of AI (see Figure 1) Concern with Companies AI Usage (see Figure 2) Consumer Concerns with AI (see Figure 3) Consumer AI Concerns: Human Connection (see Figure 4) Consumer AI Concerns: Interaction Quality (see Figure 5) Consumer AI Concerns: Job Loss (see Figure 6) Consumer AI Concerns: Misuse of Data (see Figure 7) Consumer AI Concerns: Effort Required (see Figure 8) Consumer AI Concerns: Trust (see Figure 9) Consumer Comfort with AI: by Activity (see Figure 10) Consumer Comfort with AI: Overall (see Figure 11) Consumer Comfort with AI: Order Status (see Figure 12) Consumer Comfort with AI: Tech Support (see Figure 13) Consumer Comfort with AI: Booking a Plane Ticket (see Figure 14) Consumer Comfort with AI: Billing Issues (see Figure 15) Consumer Comfort with AI: Medical Advice (see Figure 16) Employee Desire for AI: by Activity (see Figure 17) Employee Desire for AI: Writing Tasks (see Figure 18) Employee Desire for AI: Personal Assistant (see Figure 19) Employee Desire for AI: Contacting Internal Support (see Figure 20) Employee Desire for AI: Coaching for Growth (see Figure 21) Employee Desire for AI: Formal Education (see Figure 22) Employee Desire for AI: Performance Appraisal (see Figure 23) Employee Desire for AI: Job Interview (see Figure 24) Methodology (see Figure 25) Download Full PDF Fig. 1 Fig. 2 Fig. 3 Fig. 4 Fig. 5 Fig. 6 Fig. 7 Fig. 8 Fig. 9 Fig. 10 Fig. 11 Fig. 12 Fig. 13 Fig. 14 Fig. 15 Fig. 16 Fig. 17 Fig. 18 Fig. 19 Fig. 20 Fig. 21 Fig. 22 Fig. 23 Fig. 24 Fig. 25 Core Experiences Experience Management (XM) Customer Experience (CX) Employee Experience (EX) Product Experience (PX) Brand Experience (BX)Competencies LEAD an XM Program REALIZE Value from XM ACTIVATE the Organization ENLIGHTEN through Insights RESPOND with Actions DISRUPT through DesignSpecial Interests AI Editor’s Picks ROI Data Industry Data Digital B2B Culture Human Behavior Get the latest XM insights.Straight to your inbox. SUBSCRIBE Related Resources BLOG Employee Experiences in High-Performing Organizations See More RESEARCH 2025 Employee Experience Trends: Americas See More RESEARCH 2025 Employee Experience Trends: EMEA See More RESEARCH 2025 Employee Experience Trends: APJ See More Research Launchpads How-To Guide Research Blog Tools Multimedia Data Snippets Special Interests Editor’s Picks B2B ROI Data Industry Data Culture Human Behavior XM Institute About the Institute Our Team Qualtrics Citation Policy Resources Training XM Pros Certification Blog Upcoming Events Terms of Service Privacy Statement Security Statement © 2024 Qualtrics. All Rights Reserved. Warning: file_get_contents(https://www.qualtrics.com/wp-content/themes/qualtrics/json/country-codes/country-codes.json): failed to open stream: HTTP request failed! HTTP/1.0 404 Not Found in /var/www/staging.xminstitute.com/wp-content/themes/qualtrics/footer.php on line 158"
2 General attitudes toward AI | Artificial Intelligence: American Attitudes and Trends,https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/general-attitudes-toward-ai.html,未知,2024-11-21 17:06:50,未知,AI AND attitude,google,"2 General attitudes toward AI | Artificial Intelligence: American Attitudes and Trends Table of Contents 1 Executive summary 1.1 Select results 1.2 Reading notes 1.3 Press coverage 2 General attitudes toward AI 2.1 More Americans support than oppose developing AI 2.2 Support for developing AI is greater among those who are wealthy, educated, male, or have experience with technology 2.3 An overwhelming majority of Americans think that AI and robots should be carefully managed 2.4 Harmful consequences of AI in the context of other global risks 2.5 Americans’ understanding of key technology terms 3 Public opinion on AI governance 3.1 Americans consider many AI governance challenges to be important; prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation 3.2 Americans who are younger, who have CS or engineering degrees express less concern about AI governance challenges 3.3 Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology 4 AI policy and U.S.-China relations 4.1 Americans underestimate the U.S. and China’s AI research and development 4.2 Communicating the dangers of a U.S.-China arms race requires explaining policy trade-offs 4.3 Americans see the potential for U.S.-China cooperation on some AI governance challenges 5 Trend across time: attitudes toward workplace automation 5.1 Americans do not think that labor market disruptions will increase with time 5.2 Extending the historical time trend 6 High-level machine intelligence 6.1 The public predicts a 54% likelihood of high-level machine intelligence within 10 years 6.2 Americans express mixed support for developing high-level machine intelligence 6.3 High-income Americans, men, and those with tech experience express greater support for high-level machine intelligence 6.4 The public expects high-level machine intelligence to be more harmful than good Appendices A Appendix A: Methodology A.1 YouGov sampling and weights A.2 Demographic subgroups A.3 Analysis A.4 Data sharing B Appendix B: Topline questionnaire B.1 Global risks B.2 Survey experiment: what the public considers AI, automation, machine learning, and robotics B.3 Knowledge of computer science (CS)/technology B.4 Support for developing AI B.5 Survey experiment: AI and/or robots should be carefully managed B.6 Trust of actors to develop AI B.7 Trust of actors to manage AI B.8 AI governance challenges B.9 Survey experiment: comparing perceptions of U.S. vs. China AI research and development B.10 Survey experiment: U.S.-China arms race B.10.1 Control B.10.2 Nationalism treatment B.10.3 War risks treatment B.10.4 Common humanity treatment B.11 Issue areas for possible U.S.-China cooperation B.12 Trend across time: job creation or job loss B.13 High-level machine intelligence: forecasting timeline B.14 Support for developing high-level machine intelligence B.15 Expected outcome of high-level machine intelligence C Appendix C: Additional data analysis results C.1 Support for developing AI C.2 Survey experiment and cross-national comparison: AI and/or robots should be carefully managed C.3 Harmful consequences of AI in the context of other global risks C.4 Survey experiment: what the public considers AI, automation, machine learning, and robotics C.5 AI governance challenges: prioritizing governance challenges C.6 Trust in various actors to develop and manage AI in the interest of the public C.7 Survey experiment: comparing perceptions of U.S. vs. China AI research and development C.8 Survey experiment: U.S.-China arms race C.9 Trend across time: job creation or job loss C.10 High-level machine intelligence: forecasting timeline C.11 Support for developing high-level machine intelligence C.12 Expected outcome of high-level machine intelligence Acknowledgements Primary researchers Editing and design Funders For media or other inquiries Recommended citation About us About the Center for the Governance of AI About the Future of Humanity Institute References Center for the Governance of AI Future of Humanity Institute University of Oxford Artificial Intelligence: American Attitudes and Trends 2 General attitudes toward AI 2.1 More Americans support than oppose developing AI We measured respondents’ support for the further development of AI after providing them with basic information about the technology. Respondents were given the following definition of AI: Artificial Intelligence (AI) refers to computer systems that perform tasks or make decisions that usually require human intelligence. AI can perform these tasks or make these decisions without explicit human instructions. Today, AI has been used in the following applications: [five randomly selected applications] Each respondent viewed five applications randomly selected from a list of 14 that included translation, image classification, and disease diagnosis. Afterward, respondents were asked how much they support or oppose the development of AI. (See Appendix B for the list of the 14 applications and the survey question.) Figure 2.1: Support for developing AI Americans express mixed support for the development of AI, although more support than oppose the development of AI, as shown in Figure 2.1. A substantial minority (41%) somewhat or strongly supports the development of AI. A smaller minority (22%) somewhat or strongly oppose its development. Many express a neutral attitude: 28% of respondents state that they neither support nor oppose while 10% indicate they do not know. Our survey results reflect the cautious optimism that Americans express in other polls. In a recent survey, 51% of Americans indicated that they support continuing AI research while 31% opposed it (Morning Consult 2017). Furthermore, 77% of Americans expressed that AI would have a “very positive” or “mostly positive” impact on how people work and live in the next 10 years, while 23% thought that AI’s impact would be “very negative” or “mostly negative” (Northeastern University and Gallup 2018). 2.2 Support for developing AI is greater among those who are wealthy, educated, male, or have experience with technology We examined support for developing AI by 11 demographic subgroup variables, including age, gender, race, and education. (See Appendix A for descriptions of the demographic subgroups.) We performed a multiple linear regression to predict support for developing AI using all these demographic variables. Figure 2.2: Support for developing AI across demographic characteristics: distribution of responses Support for developing AI varies greatly between demographic subgroups, with gender, education, income, and experience being key predictors. As seen in Figure 2.2, a majority of respondents in each of the following four subgroups express support for developing AI: those with four-year college degrees (57%), those with an annual household income above $100,000 (59%), those who have completed a computer science or engineering degree (56%), and those with computer science or programming experience (58%). In contrast, women (35%), those with a high school degree or less (29%), and those with an annual household income below $30,000 (33%), are much less enthusiastic about developing AI. One possible explanation for these results is that subgroups that are more vulnerable to workplace automation express less enthusiasm for developing AI. Within developed countries, women, those with low levels of education, and low-income workers have jobs that are at higher risk of automation, according to an analysis by the Organisation for Economic Co-operation and Development (Nedelkoska and Quintini 2018). We used a multiple regression that includes all of the demographic variables to predict support for developing AI. The support for developing AI outcome variable was standardized, such that it has mean 0 and unit variance. Significant predictors of support for developing AI include: Being a Millennial/post-Millennial (versus being a Gen Xer or Baby Boomer) Being a male (versus being a female) Having graduated from a four-year college (versus having a high school degree or less) Identifying as a Democrat (versus identifying as a Republican) Having a family income of more than $100,000 annually (versus having a family income of less than $30,000 annually) Not having a religious affiliation (versus identifying as a Christian) Having CS or programming experience (versus not having such experience) Figure 2.3: Support for developing AI across demographic characteristics: average support across groups Some of the demographic differences we observe in this survey are in line with existing public opinion research. Below we highlight three salient predictors of support for AI based on the existing literature: gender, education, and income. Around the world, women have viewed AI more negatively than men. Fifty-four percent of women in EU countries viewed AI positively, compared with 67% of men (Eurobarometer 2017). Likewise in the U.S., 44% of women perceived AI as unsafe – compared with 30% of men (Morning Consult 2017). This gender difference could be explained by the fact that women have expressed higher distrust of technology than men do. In the U.S., women, compared with men, were more likely to view genetically modified foods or foods treated with pesticides as unsafe to eat, to oppose building more nuclear power plants, and to oppose fracking (Funk and Rainie 2015). One’s level of education also predicts one’s enthusiasm toward AI, according to existing research. Reflecting upon their own jobs, 32% of Americans with no college education thought that technology had increased their opportunities to advance – compared with 53% of Americans with a college degree (Smith and Anderson 2016). Reflecting on the economy at large, 38% of those with post-graduate education felt that automation had helped American workers while only 19% of those with less than a college degree thought so (Graham 2018). A similar trend holds in the EU: those with more years of education, relative to those with fewer years, were more likely to value AI as good for society and less likely to think that AI steals people’s jobs (Eurobarometer 2017). Another significant demographic divide in attitudes toward AI is income: low-income respondents, compared with high-income respondents, view AI more negatively. For instance, 40% of EU residents who had difficulty paying their bills “most of the time” hold negative views toward robots and AI, compared with 27% of those who “almost never” or “never” had difficulty paying their bills (Eurobarometer 2017). In the U.S., 19% of those who made less than $50,000 annually think that they are likely to lose their job to automation – compared with only 8% of Americans who made more than $100,000 annually (Graham 2018). Furthermore, Americans’ belief that AI will help the economy, as well as their support for AI research is positively correlated with their income (Morning Consult 2017). Figure 2.4: Predicting support for developing AI using demographic characteristics: results from a multiple linear regression that includes all demographic variables 2.3 An overwhelming majority of Americans think that AI and robots should be carefully managed To compare Americans’ attitudes with those of EU residents, we performed a survey experiment that replicated a question from the 2017 Special Eurobarometer #460. (Details of the survey experiment are found in Appendix B.) The original question asked respondents to what extent they agree or disagree with the following statement: Robots and artificial intelligence are technologies that require careful management. We asked a similar question except respondents were randomly assigned to consider one of these three statements: AI and robots are technologies that require careful management. AI is a technology that requires careful management. Robots are technologies that require careful management. Our respondents were given the same answer choices presented to the Eurobarometer subjects. The overwhelming majority of Americans – more than eight in 10 – agree that AI and/or robots should be carefully managed, while only 6% disagree, as seen in Figure 2.5.5 We find that variations in the statement wording produce minor differences, statistically indistinguishable from zero, in responses. Figure 2.5: Agreement with statement that AI and/or robots should be carefully managed Figure 2.6: Agreement with statement that AI and/or robots should be carefully managed by experimental condition Next, we compared our survey results with the responses from the 2017 Special Eurobarometer #460 by country (Eurobarometer 2017). For the U.S., we used all the responses to our survey question, unconditional on the experimental condition, because the variations in question-wording do not affect responses. The percentage of those in the U.S. who agree with the statement (82%) is not far off from the EU average (88% agreed with the statement). Likewise, the percentage of Americans who disagree with the statement (6% disagree) is comparable with the EU average (7% disagreed). The U.S. ranks among the lowest regarding the agreement with the statement in part due to the relatively high percentage of respondents who selected the “don’t know” option. Figure 2.7: Agreement with statement that robots and AI require careful management (EU data from 2017 Special Eurobarometer #460) 2.4 Harmful consequences of AI in the context of other global risks Figure 2.8: The American public’s perceptions of 15 potential global risks At the beginning of the survey, respondents were asked to consider five out of 15 potential global risks (the descriptions are found in Appendix B). The purpose of this task was to compare respondents’ perception of AI as a global risk with their notions of other potential global risks. The global risks were selected from the Global Risks Report 2018, published by the World Economic Forum. We edited the description of each risk to be more comprehensible to non-expert respondents while preserving the substantive content. We gave the following definition for a global risk: A “global risk” is an uncertain event or condition that, if it happens, could cause significant negative impact for at least 10 percent of the world’s population. That is, at least 1 in 10 people around the world could experience a significant negative impact.6 After considering each potential global risk, respondents were asked to evaluate the likelihood of it happening globally within 10 years, as well as its impact on several countries or industries. We use a scatterplot (Figure 2.8 to visualize results from respondents’ evaluations of global risks. The x-axis is the perceived likelihood of the risk happening globally within 10 years. The y-axis is the perceived impact of the risk. The mean perceived likelihood and impact is represented by a dot. The corresponding ellipse contains the 95% confidence region. In general, Americans perceive all these risks to be impactful: on average they rate each as having between a moderate (2) and severe (3) negative impact if they were to occur. Americans perceive the use of weapons of mass destruction to be the most impactful – at the “severe” level (mean score 3.0 out of 4). Although they do not think this risk as likely as other risks, they still assign it an average of 49% probability of occurring within 10 years. Risks in the upper-right quadrant are perceived to be the most likely as well as the most impactful. These include natural disasters, cyber attacks, and extreme weather events. The American public and the nearly 1,000 experts surveyed by the World Economic Forum share similar views regarding most of the potential global risks we asked about (World Economic Forum 2018). Both the public and the experts rank extreme weather events, natural disasters, and cyber attacks as the top three most likely global risks; likewise, both groups consider weapons of mass destruction to be the most impactful. Nevertheless, compared with experts, Americans offer a lower estimate of the likelihood and impact of the failure to address climate change. The American public appears to over-estimate the likelihoods of these risks materializing within 10 years. The mean responses suggest (assuming independence) that about eight (out of 15) of these global risks, which would have a significant negative impact on at least 10% of the world’s population, will take place in the next 10 years. One explanation for this is that it arises from the broad misconception that the world is in a much worse state than it is in reality (Pinker 2018; Rosling, Rönnlund, and Rosling 2018). Another explanation is that it arises as a byproduct of respondents interpreting “significant negative impact” in a relatively minimal way, though this interpretation is hard to sustain given the mean severity being between “moderate” and “severe.” Finally, this result may be because subjects centered their responses within the distribution of our response options, the middle value of which was the 40-60% option; thus, the likelihoods should not be interpreted literally in the absolute sense. The adverse consequences of AI within the next 10 years appear to be a relatively low priority in respondents’ assessment of global risks. It – along with adverse consequences of synthetic biology – occupy the lower left quadrant, which contains what are perceived to be lower-probability, lower-impact risks.7 These risks are perceived to be as impactful (within the next 10 years) as the failure to address climate change, though less probable. One interpretation of this is that the average American simply does not regard AI as posing a substantial global risk. This interpretation, however, would be in tension with some expert assessment of catastrophic risks that suggests unsafe AI could pose significant danger (World Economic Forum 2018; Sandberg and Bostrom 2008). The gap between experts and the public’s assessment suggests that this is a fruitful area for efforts to educate the public. Another interpretation of our results is that Americans do have substantial concerns about the long-run impacts of advanced AI, but they do not see these risks as likely in the coming 10 years. As support for this interpretation, we later find that 12% of American’s believe the impact of high-level machine intelligence will be “extremely bad, possibly human extinction,” and 21% that it will be “on balance bad.” Still, even though the median respondent expects around a 54% chance of high level machine intelligence within 10 years, respondents may believe that the risks from high level machine intelligence will manifest years later. If we assume respondents believe global catastrophic risks from AI only emerge from high-level AI, we can infer an implied global risk, conditional on high-level AI (within 10 years), of 80%. Future work should try to unpack and understand these beliefs. 2.5 Americans’ understanding of key technology terms We used a survey experiment to understand how the public understands the terms AI, automation, machine learning, and robotics. (Details of the survey experiment are found in Appendix B.) We randomly assigned each respondent one of these terms and asked them: In your opinion, which of the following technologies, if any, uses [artificial intelligence (AI)/automation/machine learning/robotics]? Select all that apply. Because we wanted to understand respondents’ perceptions of these terms, we did not define any of the terms. Respondents were asked to consider 10 technological applications, each of which uses AI or machine learning. Though the respondents show at least a partial understanding of the terms and can identify their use within the considered technological applications correctly, the respondents underestimate the prevalence of AI, machine learning, and robotics in everyday technological applications, as reported in Figure 2.9. (See Appendix C for details of our statistical analysis.) Among those assigned the term AI, a majority think that virtual assistants (63%), smart speakers (55%), driverless cars (56%), social robots (64%), and autonomous drones use AI (54%). Nevertheless, a majority of respondents assume that Facebook photo tagging, Google Search, Netflix or Amazon recommendations, or Google Translate do not use AI. Why did so few respondents consider the products and services we listed to be applications of AI, automation, machine learning, or robotics? Figure 2.9: What applications or products that the public thinks use AI, automation, machine learning, or robotics A straightforward explanation is that inattentive respondents neglect to carefully consider or select the items presented to them (i.e., non-response bias). Even among those assigned the term robotics, only 62% selected social robots and 68% selected industrial robots. Our analysis (found in Appendix C) confirms that respondent inattention, defined as spending too little or too much time on the survey, predicts non-response to this question. Another potential explanation for the results is that the American public – like the public elsewhere – lack awareness of AI or machine learning. As a result, the public does not know that many tech products and services use AI or machine learning. According to a 2017 survey, nearly half of Americans reported that they were unfamiliar with AI (Morning Consult 2017). In the same year, only 9% of the British public said they had heard of the term “machine learning” (Ipsos MORI 2018). Similarly, less than half of EU residents reported hearing, reading, or seeing something about AI in the previous year (Eurobarometer 2017). Finally, the so-called “AI effect” could also explain the survey result. The AI effect describes the phenomenon that the public does not consider an application that uses AI to utilize AI once that application becomes commonplace (McCorduck 2004). Because 85% of Americans report using digital products that deploy AI (e.g., navigation apps, video or music streaming apps, digital personal assistants on smartphones, etc.) (Reinhart 2018), they may not think that these everyday applications deploy AI. References Eurobarometer. 2017. “Special Eurobarometer 460: Attitudes Towards the Impact of Digitisation and Automation on Daily Life.” Eurobarometer. https://perma.cc/9FRT-ADST. Funk, Cary, and Lee Rainie. 2015. “Public and Scientists’ Views on Science and Society.” Survey report. Pew Research Center. https://perma.cc/9XSJ-8AJA. Graham, Edward. 2018. “Views on Automation’s U.s. Workforce Impact Highlight Demographic Divide.” Morning Consult. https://perma.cc/544D-WRUM. Ipsos MORI. 2018. “Public Views of Machine Learning: Findings from Public Research and Engagement Conducted on Behalf of the Royal Society.” Survey report. The Royal Society. https://perma.cc/79FE-TEHH. McCorduck, Pamela. 2004. Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence. New York: A K Peters/CRC Press. Morning Consult. 2017. “National Tracking Poll 170401.” Survey report. Morning Consult. https://perma.cc/TBJ9-CB5K. Nedelkoska, Ljubica, and Glenda Quintini. 2018. “Automation, Skills Use and Training.” Working Papers No. 202. Organisation for Economic Co-operation; Development. https://doi.org/10.1787/2e2f4eea-en. Northeastern University and Gallup. 2018. “Optimism and Anxiety: Views on the Impact of Artificial Intelligence and Higher Education’s Response.” Survey report. Northeastern University; Gallup. https://perma.cc/57NW-XCQN. Pinker, Steven. 2018. Enlightenment Now: The Case for Reason, Science, Humanism, and Progress. New York: Penguin. Reinhart, RJ. 2018. “Most Americans Already Using Artificial Intelligence Products.” Survey report. Gallup. https://perma.cc/RVY5-WP9W. Rosling, Hans, Anna Rosling Rönnlund, and Ola Rosling. 2018. Factfulness: Ten Reasons We’re Wrong About the World–and Why Things Are Better Than You Think. New York: Flatiron Books. Sandberg, Anders, and Nick Bostrom. 2008. “Global Catastrophic Risks Survey.” Future of Humanity Institute, Oxford University. https://perma.cc/TA97-KD3Z. Smith, Aaron, and Monica Anderson. 2016. “Automation in Everyday Life.” Pew Research Center. https://perma.cc/WU6B-63PZ. World Economic Forum. 2018. “The Global Risks Report 2018: 13th Edition.” World Economic Forum. https://perma.cc/8XM8-LKEN. These percentages that we discuss here reflect the average response across the three statements. See Appendix B for the topline result for each statement.↩ Our definition of global risk borrowed from the Global Challenges Foundation’s definition: “an uncertain event or condition that, if it happens, can cause a significant negative impact on at least 10% of the world’s population within the next 10 years” (Cotton-Barratt et al. 2016).↩ The World Economic Forum’s survey asked experts to evaluate the “adverse consequences of technological advances,” defined as “[i]ntended or unintended adverse consequences of technological advances such as artificial intelligence, geo-engineering and synthetic biology causing human, environmental and economic damage.” The experts considered these “adverse consequences of technological advances” to be less likely and lower-impact, compared with other potential risks.↩"
Who Likes Artificial Intelligence? Personality Predictors of Attitudes toward Artificial Intelligence,https://pubmed.ncbi.nlm.nih.gov/35015615/,未知,2024-11-21 17:09:24,en,AI AND attitude,google,"Who Likes Artificial Intelligence? Personality Predictors of Attitudes toward Artificial Intelligence Who Likes Artificial Intelligence? Personality Predictors of Attitudes toward Artificial Intelligence J Psychol. 2022;156(1):68-94. doi: 10.1080/00223980.2021.2012109. Epub 2022 Jan 11. Authors Jiyoung Park 1 , Sang Eun Woo 2 Affiliations 1 Duksung Women's University. 2 Purdue University. PMID: 35015615 DOI: 10.1080/00223980.2021.2012109 Abstract We examined how individuals' personality relates to various attitudes toward artificial intelligence (AI). Attitudes were organized into two dimensions of affective components (positive and negative emotions) and two dimensions of cognitive components (sociality and functionality). For personality, we focused on the Big Five personality traits (extraversion, agreeableness, conscientiousness, neuroticism, openness) and personal innovativeness in information technology. Based on a survey of 1,530 South Korean adults, we found that extraversion was related to negative emotions and low functionality. Agreeableness was associated with both positive and negative emotions, and it was positively associated with sociality and functionality. Conscientiousness was negatively related to negative emotions, and it was associated with high functionality, but also with low sociality. Neuroticism was related to negative emotions, but also to high sociality. Openness was positively linked to functionality, but did not predict other attitudes when other proximal predictors were included (e.g. prior use, personal innovativeness). Personal innovativeness in information technology consistently showed positive attitudes toward AI across all four dimensions. These findings provide mixed support for our hypotheses, and we discuss specific implications for future research and practice. Keywords: Artificial intelligence; Big Five personality traits; attitudes; personal innovativeness in information technology. MeSH terms Adult Artificial Intelligence* Attitude Emotions Extraversion, Psychological* Humans Personality"
"The Relationship between Attitude and Intelligence …and, the possibility of AI Attitude – Waterloo Intuition Blog",https://blog.waterloointuition.com/the-relationship-between-attitude-and-intelligence-and-the-possibility-of-ai-attitude/,未知,2024-11-21 17:09:46,en-US,AI AND attitude,google,"The Relationship between Attitude and Intelligence …and, the possibility of AI Attitude – Waterloo Intuition Blog Skip to content Search Search Home About Testimonials​ Contact Blog Menu Home About Testimonials​ Contact Blog The Relationship between Attitude and Intelligence …and, the possibility of AI Attitude Rick Baker April 18, 2023 Understanding AttitudeAttitude can be defined as an individual’s mental disposition toward a particular situation, object, or person. Our attitudes influence how we perceive and respond to situations, people, and events.Consider Attitude as a component of Intelligence or, even better, recognize Attitude can be a result of Intelligence.Several years ago, I attended a ‘Fish Philosophy’ event. The creators of the ‘Fish Philosophy’ got it right with their “Choose Your Attitude” mantra. By following this guidance, your Intelligence can influence your decisions, leading you to adopt a positive and productive Attitude. You can choose to use Attitudes that embrace positive possibilities…and, perhaps, become a “possibilitarian”.Attitude, be it positive, negative, or neutral, is not an inherent personality trait.Attitude is a matter of choice.A positive, winning Attitude is a smart choice.Can Artificial Intelligence have Attitude?As Artificial Intelligence continues to advance, the question of whether Artificial Intelligence can possess an Attitude similar to humans arises. Artificial Intelligence systems are designed to “learn”, adapt, and respond to various inputs and situations, much like humans do with their Attitude. However, Artificial Intelligence’s “Attitude” is determined by its programming, algorithms, and the data it is exposed to, rather than personal experiences or emotions.In this context, Artificial Intelligence’s “Attitude” may be considered a reflection of its decision-making process, influenced by the data it has been trained on and the objectives set by its developers. Artificial Intelligence systems can be designed to exhibit a positive, negative, or neutral “Attitude”, depending on the desired outcome. However, it is essential to remember that Artificial Intelligence’s “Attitude” is a by-product of its programming and does not possess the emotional depth or complexity found in human Attitude.We can choose the “Attitude” used by Artificial Intelligence. For example, I asked ChatGPT to provide “angry” and “happy” opinions about war…here’s the result:“Angry” Description of War: War is a devastating, senseless force that wreaks havoc on humanity, tearing families apart and leaving destruction in its wake. “Happy” Description of War: War, though seemingly destructive, can be a force for change and renewal, sweeping away outdated systems and paving the way for new beginnings. ConclusionUnderstanding Intelligence as a multifaceted concept and acknowledging the role of Attitude within it can lead to a more comprehensive approach to personal development. By embracing the diverse aspects of Intelligence and recognizing the significance of Attitude as an outcome of Intelligence, we can make better choices and cultivate a winning, positive outlook on life…with greater peace of mind. Additionally, exploring the potential for Artificial Intelligence to exhibit an Attitude-like decision-making process offers intriguing possibilities for the future of AI-human interaction. Together our conversations can expand solutions and value We look forward to helping you bring your ideas and solutions to life. Contact Us Previous Next Share the Post: Leave a Reply Cancel replyYour email address will not be published. Required fields are marked *Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment. Linkedin Facebook Youtube Instagram Twitter Quick Links Home About Testimonials​ Contact Blog Home About Testimonials​ Contact Blog Get In Touch Email: hi@waterloointuition.com Phone: 226-476-1616 PO Box 97Elmira PO Main, ON, N3B 2Z5Canada Additional accessible formats for this information are available upon request. Contact Us for more information. Copyright © 2024 Waterloo Intuition & Technology Corporation (WITco). All Rights Reserved."
Affective neuroscience theory and attitudes towards artificial intelligence | AI & SOCIETY,https://link.springer.com/article/10.1007/s00146-023-01841-8,未知,2024-11-21 17:10:00,en,AI AND attitude,google,"Affective neuroscience theory and attitudes towards artificial intelligence | AI & SOCIETY <iframe src=""https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> <iframe src=""https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> Skip to main content Log in Menu Find a journal Publish with us Track your research Search Cart Home AI & SOCIETY Article Affective neuroscience theory and attitudes towards artificial intelligence Open Forum Open access Published: 28 January 2024 (2024) Cite this article Download PDF You have full access to this open access article AI & SOCIETY Aims and scope Submit manuscript Affective neuroscience theory and attitudes towards artificial intelligence Download PDF Christian Montag ORCID: orcid.org/0000-0001-8112-08371, Raian Ali ORCID: orcid.org/0000-0002-5285-78292 & Kenneth L. Davis3 3723 Accesses 2 Citations 1 Altmetric Explore all metrics AbstractArtificial intelligence represents a key technology being inbuilt into evermore products. Research investigating attitudes towards artificial intelligence surprisingly is still scarce, although it becomes apparent that artificial intelligence will shape societies around the globe. To better understand individual differences in attitudes towards artificial intelligence, the present study investigated in n = 351 participants associations between the Affective Neuroscience Personality Scales (ANPS) and the Attitudes towards Artificial Intelligence framework (ATAI). It could be observed that in particular higher levels of SADNESS were associated with higher negative attitudes towards AI (fearing AI). The findings are discussed in this work from an evolutionary perspective because primary emotional systems—according to Affective Neuroscience Theory—represent tools for survival, which have been homologously conserved across mammalian species including homo sapiens. Similar content being viewed by others Acceptance and Fear of Artificial Intelligence: associations with personality in a German and a Chinese sample Article Open access 31 January 2022 How Much Emotionally Intelligent AI Can Be? Chapter © 2021 The Ethics of Emotional Artificial Intelligence: A Mixed Method Analysis Article 02 December 2022 Explore related subjects Discover the latest articles, news and stories from top researchers in related subjects. Artificial Intelligence Use our pre-submission checklist Avoid common mistakes on your manuscript. 1 IntroductionArtificial intelligence is built into a growing number of products, with which humans interact on a daily basis. Such products are language assistants in smartphones, self-driving cars and social robots. The AI revolution is also discussed in other areas such as sales platforms (Ravindar et al. 2022). In particular with the release of the generative artificial intelligence ChatGPT (Lund & Wang 2023), artificial intelligence represents a hotly debated topic in the mass media (The Economist 2022) being accompanied also by fears that artificial intelligence will have negative impacts on the world for instance resulting in job loss (Chelliah 2017). Others are more optimistic about the chances that will arise from artificial intelligence, for instance, aiding humans by taking over repetitious tasks and some researchers think about the benefits of “human–machine coexistence” (Hamid et al. 2017). For discussions around the automation term in the context of artificial intelligence research see a recent work (Montag et al. 2023). As artificial intelligence will likely also become a driver of the economy (Aghion et al. 2017), societies with more optimistic views might more profit from AI as early adopters of this technology. See also recent work in the area of medical disciplines (Pinto dos Santos et al. 2019).Until now, only a few studies have investigated which factors might influence individual differences in attitudes towards AI. A recent study not only presented a short screening tool to assess positive and negative attitudes towards AI (called ATAI or ATAI framework), but also observed that females reported on average lower positive attitudes towards AI than males—in particular in the German sample described, and less so in the Chinese sample (Sindermann et al. 2021). However, gender differences in negative attitudes were less visible in this work. Pronounced differences could be also observed for culture effects in the same work, with Chinese participants reporting on average higher positive attitudes towards AI than German participants (accepting AI dimension).Differences between negative attitudes towards AI were less pronounced between Chinese and German study participants (fearing AI dimension). The investigation of the personality variables fit also within a recently proposed framework, which has been put forward to understand how attitudes towards AI might be shaped. The so-called IMPACT model proposes that an Interplay of Modality, Person, Area, Country/Culture and Transparency variables are relevant to understand how attitudes toward AI form (Montag et al., 2024a; Montag et al., 2024b). Personality belongs to the P-variable. Of interest for the present study are observations that personality seems to play a role to understand individual differences in attitudes towards AI, but effect sizes are small (Sindermann et al. 2022): consistently it was observed that the only Big Five personality trait linked to negative attitudes towards AI was neuroticism. Here, higher scores of neuroticism were linked to higher negative attitudes towards AI in both Germany and China (Sindermann et al. 2022). Neuroticism represents a super-dimension comprising many facets such as being anxious and tending to be more in a depressed state (Zhang 2020). To better understand what facets of neuroticism might be driving negative attitudes towards AI, we conducted the present study not focusing on the Big Five personality traits to understand individual differences in attitudes towards AI, but instead on an evolutionary approach from personality psychology: Whereas the Big Five have been derived from a lexical approach (Montag and Elhai 2019; Piedmont and Aycock 2007), Jaak Panksepp carved out seven primary emotional systems by means of electrical stimulation studies, lesion and pharmacological studies being homologously conserved across the mammalian brain (Panksepp 1998, 2011). These primary emotional systems represent tools for survival and are called SEEKING, CARE, PLAY, LUST (positive emotions) and FEAR, ANGER, SADNESS (negative emotions). According to Panksepp’s Affective Neuroscience Theory (ANT), these primary emotional systems are anchored in subcortical regions of the brain (Davis and Montag 2019). Individual differences in the neural circuits underlying these primary emotional systems in terms of structure and function underly individual differences in human personality. Information about the exact brain regions involved according to Pankseppian Affective Neuroscience Theory can be found in older works (Montag and Panksepp 2016; Panksepp 2011).Of interest, for the present study, both the Big Five concept and primary emotional systems can be brought together. A meta-analysis provided support for the idea that high SEEKING might be the bottom-up driver of high Openness to Experience, high PLAY the bottom-up driver of high Extraversion, high CARE / low ANGER the bottom-up driver of Agreeableness and for the present study of most interest high FEAR, SADNESS, and ANGER the bottom up driver of neuroticism (Marengo et al. 2021). Bottom-up driver means that anatomically speaking the primary emotional systems—according to ANT being subcortically located—influence the lexically derived Big Five of Personality from evolutionary ancient brain regions (for further discussions see Montag and Davis ( 2018)). The emotion FEAR is of evolutionary significance because activation of the FEAR circuitry helps humans to get out of a danger zone. SADNESS is activated by separation distress and finally ANGER activity can be observed in situations of territorial conflicts in the mammalian kingdom and also frustrations, to name a few (Montag and Panksepp 2017; Panksepp 1998). To better understand the link between negative attitudes towards AI and neuroticism, we applied Panksepp’s Affective Neuroscience Theory. Due to the link between FEAR, SADNESS, ANGER and neuroticism (Davis et al. 2003; Montag and Panksepp 2017), we expected these three primary emotional systems to be positively associated with negative attitudes towards AI. Against the small effect sizes (neuroticism-negative attitudes towards AI/fearing AI) observed in the recent work, we also expected only small associations in the present work. Correlations are presented in the present work for all primary emotional systems and ATAI dimensions. Correlations going beyond our hypothesis should be seen as exploratory.2 Methods2.1 ParticipantsA total of 402 participants could be recruited via the website https://www.anps-research.com for the present study. We only included participants who stated either to be a native English speaker or, otherwise, competent in English. n = 17 participants stated to be non-native speakers and not to be competent and were therefore excluded from the analysis. We asked participants to only fill in the survey when being at least eighteen years old. 16 participants were younger than that age and were therefore also excluded. This resulted in n = 369 participants. Further, we checked for monotonous answer behavior regarding the ANPS and the ATAI scale (separately). One can debate if data cleaning on five items of the ATAI is meaningful, in particular with those who entered five items a neutral response (which is imaginable to stay completely neutral on the AI topic). For reasons of uniformity in our strategy, we decided to exclude those participants, too (this explains the light dip in the middle of the “normal distribution” of the ATAI depicted in Fig. 1). Correlations do not change meaningfully though with including and excluding those participants. Following our cleaning strategy, in total 18 participants showed monotonous answer behavior on either the ANPS and/or the ATAI and were excluded. The final sample consisted of 351 participants (n = 142 males and n = 209 females; mean-age: 35.1 (SD = 13.3)). Education level was as follows (highest degree received): less than high school (0.6%), high school graduate (9.4%), some college (16.5%), bachelor’s degree (30.8%), master’s degree (31.1%), doctoral degree (11.7%). Numbers do not add up to 100% due to rounding errors. Both the larger data set (n = 369) and the here presented smaller data set (n = 351) can be found at the Open Science Framework for further analysis of interested scientists to independently check on differences in correlations between the larger and the smaller data set: https://osf.io/9zf28/.Fig. 1Distribution of scores of the positive and negative attitude towards artificial intelligence scales (please note that on the x-axis total sum scores are presented; histograms were produced with the SPSS package)Full size imageAll participants provided informed e-consent and agreed that the data could be shared in an anonymous way with the scientific community to foster open science practices. Please note that IRB approval was not deemed necessary, because data collection was completely anonymous, and participants could stop filling in the survey at all times (before finally submitting their data). As an incentive, all participants were provided with insights on their primary emotional systems scores at the end of the survey.2.2 QuestionnairesParticipants filled in the English version of the Affective Neuroscience Personality Scales (ANPS 3.1) which consists of 112 items (Montag et al. 2021). From these 112 items, each 14 items assess one primary emotional system. The only primary emotional system not assessed is the LUST system because answering items on one’s sexual activity might in particular trigger biased responses. Each item was answered on a six Likert scale ranging from strongly disagree (1) to strongly agree (6). Please note that some items need to be recorded before building the scales. Internal consistencies of the primary emotional systems were excellent with SEEKING (α = 0.79), PLAY (α = 0.85), CARE (α = 0.82) and FEAR (α = 0.92), SADNESS (α = 0.83) and ANGER (α = 0.87). In the ANPS also a spirituality dimension (assessed with 12 items) can be found, which was included against the background of spirituality being a relevant variable in the treatment of addiction. The internal consistency of the spirituality scale was also excellent (α = 0.93).Aside from the ANPS 3.1. all participants filled in the English version of the Attitudes towards Artificial Intelligence Scale (ATAI) (Sindermann et al. 2021). This scale consists of five items with two items assessing positive attitudes towards AI (accepting AI) and three items assessing negative attitudes towards AI (fearing AI). In the present work, the ATAI items were answered with a five-point Likert scale ranging from strongly disagree (1) to strongly agree (5). Internal consistencies for the ATAI were as follows: negative attitudes towards AI (α = 0.62) and positive attitudes towards AI (α = .62). Please note that the ATAI consists of few items. Therefore, also in light of the external validity provided by Sindermann et al. (2021), the internal consistencies were deemed to be adequate.2.3 Statistical analysesBoth the ANPS scales and ATAI dimensions were visually inspected and resembled rather normal distributions (see also Fig. 1). Jamovi 2.4.8.0 was used to produce descriptive statistics and run t-tests. Descriptive statistics are presented for all relevant variables for the total sample and male/females subsamples. We decided to run parametric tests. In a first simple analysis, two-tailed Pearson correlations are presented. These were also produced with the Jamovi package. In the next step, we also investigated in a stepwise regression model what represents the best predictor variables for negative attitudes towards AI. This was done with SPSS 29.0.1.0 (same was true for the education analysis and production of histograms). We decided upon an atheoretical stepwise approach to understand what variables are most critical in our data set to understand negative attitudes towards AI. No regression model is presented for predicting positive attitudes towards AI because primary emotional systems played a negligible role here.3 ResultsIn Table 1 descriptive statistics for all relevant psychological variables are presented. Please note that not only descriptive statistics for the total sample but also for the male and female subsamples are presented (see Table 2). Females scored significantly higher on the CARE, FEAR and SADNESS of the ANPS. Females were associated with more negative attitudes towards AI compared to males. Females were also associated with lower positive attitudes towards AI compared to males. The ATAI differences failed to be significant though (see Table 3). Of interest, education levels were not significantly associated with negative attitudes towards AI (F(5,345) = 0.658, p = 0.655, eta2 = 0.009) nor with positive attitudes towards AI (F(5,345) = 1.188, p = 0.314, eta2 = 0.017). Correlations for the associations between psychological variables of interest are presented in Table 4.Table 1 Descriptive statistics for all relevant psychological variablesFull size tableTable 2 Descriptive statistics for all relevant psychological variables for the male and female subsamplesFull size tableTable 3 Independent T-Test analysis regarding gender and the scales of interestFull size tableTable 4 Correlations between primary emotional systems and attitudes towards artificial intelligence (n = 351); further correlations are reported with spirituality and also between both ATAI dimensionsFull size tableBased on the correlational analysis in Table 5 we also provide a deeper analysis of the negative primary emotional systems and ATAI scores on the item level.Table 5 Correlations between primary emotional systems and single items of the negative attitude towards artificial intelligence scaleFull size tableA stepwise regression model was conducted with negative attitudes towards artificial intelligence representing the dependent variable. Independent variables were the negative primary emotional systems (FEAR, SADNESS, ANGER) and the spirituality dimension all entered into one block. The regression model explaining most variance (5.5%) included the SADNESS and spirituality variables only (second proposed regression model: F(2,348) = 10,038, p < 0.001, R2 = 0.055; adjusted R2 = 0.049). SADNESS alone explained 3.7% of the variance (first regression model: F(1,349) = 13,507, p < 0.001; R2 = 0.037; adjusted R2 = 0.035), adding spirituality led to an increment of 1.8% explained variance.4 DiscussionRecent research showed that neuroticism is positively associated with negative attitudes towards artificial intelligence (Sindermann et al. 2022). As affective neuroscience theory suggests that the primary emotional systems FEAR, SADNESS and ANGER are neuroanatomically bottom-up drivers of neuroticism (Marengo et al. 2021), the present work aimed to shed light on associations between these primary emotional systems and negative attitudes towards artificial intelligence.In line with our expectations, positive correlations appeared between all negative primary emotional systems and negative attitudes towards artificial intelligence. The highest association with negative attitudes towards AI was observed for the SADNESS system, which was also backed up by our stepwise regression model. According to Jaak Panksepp the SADNESS system is triggered by separation distress (Davis and Montag 2023; Watt 2023; Watt and Panksepp 2009), which from an evolutionary point of view, represents a dangerous state for a human, because young children separated from caregivers are especially vulnerable, but in general our species is stronger in groups than alone. Eliciting psychic pain (being sad) when facing a loss of a significant person might result in searching for social support (being taken CARE of) to reduce the activity of the SADNESS system as well as contributing to social group cohesion. From this perspective, one might also think of designing AI with having in mind that if AI fulfilled human social needs, it might reduce negative attitudes towards this technology.Interestingly a higher active SADNESS system (in terms of an emotional personality trait) was robustly associated with all single items assessing negative attitudes towards artificial intelligence including fearing AI, believing that artificial intelligence will destroy human mankind and AI being a driver of job loss. Our data suggest that SAD persons might be more drawn into the mentioned scenarios/items, with the highest correlation found on item level, namely, “AI destroying human mankind”. Our data speak for the idea that SADNESS underlying a neurotic personality trait might be the facet of neuroticism being most strongly linked to negative attitudes towards AI. Our data could also mean that being in a stable relationship with persons might buffer against negative attitudes towards AI.Beyond this investigation, we also observed an interesting finding, namely that spirituality was significantly associated with both negative and positive attitudes towards artificial intelligence. In detail, higher spirituality went along with more negative and less positive views on artificial intelligence. Persons scoring high on spirituality report among others to be “touched by the beauty of creation” and spirituality “being a primary source of inner peace” (see page 165: Montag et al. 2021). Being a spiritual person thereby goes along with being more skeptical towards AI. This result was not hypothesized and therefore it is just a starting point to be further explored. Please note that the finding in the present work would also not hold for adjustments for many correlations such as Bonferroni corrections.The present study comes with several limitations. First of all, this study is of cross-sectional nature, and we cannot disentangle cause and effect. Second, this study is based on self-report and therefore could be influenced by a lack of introspective abilities, etc. This is also important to stress because the neuroscientific assessment of activity in the brain circuitries underlying primary emotional systems would make an important add to the present investigation. In this realm, it needs also to be mentioned that further unraveling of the brain mechanisms underlying cognitive functions might stimulate progress in AI developments and perhaps AI democratization (Bain and McCay 2023). This is also true when one considers the brain’s efficiency in terms of energy needed to solve tasks compared to current AI systems (Fuller 2019). A third limitation: the study sample is not representative and included people from different countries. We did not assess country or culture, just being a native English speaker and being proficient in the English language, when not being a native speaker. This said, the data quality of the present work looks good because gender differences in the ANPS are as often reported in the literature (Özkarar-Gradwohl and Turnbull 2021): For instance, we also observed higher CARE and SADNESS scores in females compared to males, which fits with observations in other samples. Internal consistencies were mostly in a respectable area. Further, we observed again the trend that males had more positive views towards AI than females (Sindermann et al. 2021).Further limitations: Recent work observed some cultural effects on the ATAI (Sindermann et al. 2021, 2022)—this could not be investigated in the present sample. Finally, the here observed effect sizes and explained variance of the regression model are in the very small area showing that other variables beyond personality are likely of higher importance to understand individual differences in attitudes towards artificial intelligence. Regarding the administered attitudes towards AI scale, we mention that the ATAI is only a rough screener for attitudes towards AI, and future studies also need to administer more fine-grained tools to grasp more facets of attitudes towards artificial intelligence (see a twenty item measure on general attitudes towards AI: Schepman and Rodway 2020), which might lead to different personality-attitudes towards AI associations; see exemplary recent works using not the ATAI presenting further associations (Kaya et al. 2022; Park and Woo 2022).In sum, the present work is to our knowledge the first to use Panksepp’s primary emotional systems and Affective Neuroscience Theory to shed light on attitudes towards artificial intelligence. This work shows that such an evolutionary framework might help to understand a bit better why some persons have more negative or positive views on artificial intelligence. Going beyond our work, more integration of neuroscientific research could further stimulate AI developments (Bain and McCay 2023), but ethical aspects should be strongly considered when knowledge from neuroscience and AI is applied, e.g. as outlined in a recent paper on diagnostics of criminal recidivism in teenagers (Muñoz and Marinaro 2022). Data availability Data is available via the Open Science Framework: https://osf.io/9zf28/. ReferencesAghion P, Jones BF, Jones CI (2017) Artificial intelligence and economic growth (working paper 23928). Natl Bureau Econ Res. https://doi.org/10.3386/w23928Article Google Scholar Bain M, McCay A (2023) The neural democratisation of AI. AI & Soc. https://doi.org/10.1007/s00146-023-01706-0Article Google Scholar Chelliah J (2017) Will artificial intelligence usurp white collar jobs? Hum Resour Manag Int Dig 25(3):1–3. https://doi.org/10.1108/HRMID-11-2016-0152Article Google Scholar Davis KL, Montag C (2019) Selected principles of pankseppian affective neuroscience. Front Neurosci. 12:1025. https://doi.org/10.3389/fnins.2018.01025Article Google Scholar Davis, K. L., & Montag, C (2023) Unraveling the roots of depression–it’s complicated. Neuropsychoanalysis 25(2):171–174. https://doi.org/10.1080/15294145.2023.2261454Article Google Scholar Davis KL, Panksepp J, Normansell L (2003) The affective neuroscience personality scales: normative data and implications. Neuropsychoanalysis 5(1):57–69. https://doi.org/10.1080/15294145.2003.10773410Article Google Scholar Fuller S (2019) The brain as artificial intelligence: prospecting the frontiers of neuroscience. AI & Soc 34(4):825–833. https://doi.org/10.1007/s00146-018-0820-1Article Google Scholar Hamid OH, Smith NL, Barzanji A (2017) Automation, per se, is not job elimination: How artificial intelligence forwards cooperative human-machine coexistence. 2017 IEEE 15th International Conference on Industrial Informatics (INDIN). p 899–904. https://doi.org/10.1109/INDIN.2017.8104891Kaya F, Aydin F, Schepman A, Rodway P, Yetişensoy O, Demir Kaya M (2022) The roles of personality traits, AI anxiety, and demographic factors in attitudes toward artificial intelligence. Int J Hum Comput Interact. https://doi.org/10.1080/10447318.2022.2151730Article Google Scholar Lund BD, Wang T (2023) Chatting about ChatGPT: how may AI and GPT impact academia and libraries? Library Hi Tech News. https://doi.org/10.1108/LHTN-01-2023-0009Article Google Scholar Marengo D, Davis KL, Gradwohl GÖ, Montag C (2021) A meta-analysis on individual differences in primary emotional systems and big five personality traits. Sci Rep. https://doi.org/10.1038/s41598-021-84366-8Article Google Scholar Montag C, Davis KL (2018) Affective neuroscience theory and personality: an update. Pers Neurosci. 1:e9. https://doi.org/10.1017/pen.2018.10Article Google Scholar Montag C, Elhai JD (2019) A new agenda for personality psychology in the digital age? Pers Individ Differ 147:128–134. https://doi.org/10.1016/j.paid.2019.03.045Article Google Scholar Montag C, Elhai JD, Davis KL (2021) A comprehensive review of studies using the Affective Neuroscience Personality Scales in the psychological and psychiatric sciences. Neurosci Biobehav Rev 125:160–167. https://doi.org/10.1016/j.neubiorev.2021.02.019Article Google Scholar Montag C, Kraus J, Baumann M, Rozgonjuk D (2023) The propensity to trust in automated technology mediates the links between technology self-efficacy and fear and acceptance of artificial intelligence. Comput Hum Behav Rep. 11:100315. https://doi.org/10.1016/j.chbr.2023.100315Article Google Scholar Montag C, Nakov P, Ali R (2024a) Considering the IMPACT framework to understand the AI-well-being complex from an interdisciplinary perspective. Telemat Inform Rep.13:100112. https://doi.org/10.1016/j.teler.2023.100112Montag C, Ali R, Al-Thani D, Hall BJ (2024b) On artificial intelligene and global mental health. Asian J Psychiatr. 91:103855. https://doi.org/10.1016/j.ajp.2023.103855Article Google Scholar Montag C, Panksepp J (2016) Primal emotional-affective expressive foundations of human facial expression. Motiv Emot 40(5):760–766. https://doi.org/10.1007/s11031-016-9570-xArticle Google Scholar Montag C, Panksepp J (2017) Primary emotional systems and personality: an evolutionary perspective. Front Psychol. 8:464. https://doi.org/10.3389/fpsyg.2017.00464Article Google Scholar Muñoz JM, Marinaro JÁ (2022) Algorithmic biases: caring about teens’ neurorights. AI Soc. https://doi.org/10.1007/s00146-022-01516-wArticle Google Scholar Özkarar-Gradwohl FG, Turnbull OH (2021) Gender effects in personality: a cross-cultural affective neuroscience perspective. Culture and Brain 9(2):79–96. https://doi.org/10.1007/s40167-021-00099-5Article Google Scholar Panksepp J (1998) Affective neuroscience: the foundations of human and animal emotions. Oxford University Press, OxfordBook Google Scholar Panksepp J (2011) Cross-species affective neuroscience decoding of the primal affective experiences of humans and related animals. PLoS ONE 6(9):e21236. https://doi.org/10.1371/journal.pone.0021236Article Google Scholar Park J, Woo S (2022) Who likes artificial intelligence? Personality predictors of attitudes toward artificial intelligence. J Psychol 156:1–27. https://doi.org/10.1080/00223980.2021.2012109Article Google Scholar Piedmont RL, Aycock W (2007) An historical analysis of the lexical emergence of the big five personality adjective descriptors. Pers Individ Differ 42(6):1059–1068. https://doi.org/10.1016/j.paid.2006.09.015Article Google Scholar Pinto dos Santos D, Giese D, Brodehl S, Chon SH, Staab W, Kleinert R, Maintz D, Baeßler B (2019) Medical students’ attitude towards artificial intelligence: a multicentre survey. Eur Radiol 29(4):1640–1646. https://doi.org/10.1007/s00330-018-5601-1Article Google Scholar Ravindar M, Ashmi C, Gupta S, Gupta M (2022) AI: a new strategic method for marketing and sales platforms. Impact of artificial intelligence on organizational transformation. John Wiley & Sons Ltd, New York, pp 183–199. https://doi.org/10.1002/9781119710301.ch12Chapter Google Scholar Schepman A, Rodway P (2020) Initial validation of the general attitudes towards Artificial Intelligence Scale. Comput Hum Behav Rep 1:100014. https://doi.org/10.1016/j.chbr.2020.100014Article Google Scholar Sindermann C, Sha P, Zhou M, Wernicke J, Schmitt HS, Li M, Sariyska R, Stavrou M, Becker B, Montag C (2021) Assessing the attitude towards artificial intelligence: introduction of a short measure in German, Chinese, and English Language. KI Künstl Intell 35(1):109–118. https://doi.org/10.1007/s13218-020-00689-0Article Google Scholar Sindermann C, Yang H, Elhai JD, Yang S, Quan L, Li M, Montag C (2022) Acceptance and fear of artificial intelligence: associations with personality in a German and a Chinese sample. Discover Psychology 2(1):8. https://doi.org/10.1007/s44202-022-00020-yArticle Google Scholar The Economist (2022) How good is ChatGPT? The Economist. https://www.economist.com/business/2022/12/08/how-good-is-chatgptWatt DF (2023) The separation distress hypothesis of depression—an update and systematic review*. Neuropsychoanalysis. https://doi.org/10.1080/15294145.2023.2240340Article Google Scholar Watt D, Panksepp J (2009) Depression: an evolutionarily conserved mechanism to terminate separation distress? A review of aminergic, peptidergic, and neural network perspectives. Neuropsychoanalysis. 11:7–51. https://doi.org/10.1080/15294145.2009.10773593Article Google Scholar Zhang F (2020) Neuroticism. The Wiley encyclopedia of personality and individual differences. John Wiley & Sons Ltd, New York, pp 281–286. https://doi.org/10.1002/9781119547143.ch47Chapter Google Scholar Download referencesFundingOpen Access funding enabled and organized by Projekt DEAL.Author informationAuthors and AffiliationsDepartment of Molecular Psychology, Institute of Psychology and Education, Ulm University, Helmholtzstr. 8/1, 89081, Ulm, GermanyChristian MontagCollege of Science and Engineering, Hamad Bin Khalifa University, Doha, QatarRaian AliPegasus International, Greensboro, NC, 27408, USAKenneth L. DavisAuthorsChristian MontagView author publicationsYou can also search for this author in PubMed Google ScholarRaian AliView author publicationsYou can also search for this author in PubMed Google ScholarKenneth L. DavisView author publicationsYou can also search for this author in PubMed Google ScholarCorresponding authorCorrespondence to Christian Montag.Ethics declarations Conflict of interest None. However, for reasons of transparency, the author Christian Montag mentions that he has received grants from agencies such as the German Research Foundation (DFG). He has performed grant reviews for several agencies; has edited journal sections and articles; has given academic lectures in clinical or scientific venues or companies; and has generated books or book chapters for publishers of mental health texts. For some of these activities, he received royalties, but never from gaming or social media companies. The author Christian Montag mentions that he was part of a discussion circle (Digitalität und Verantwortung) debating ethical questions linked to social media, digitalization and society/democracy at Facebook. In this context, he received no salary for his activities. Finally, he mentions that he currently functions as an independent scientist on the scientific advisory board of the Nymphenburg group (Munich, Germany). This activity is financially compensated. Moreover, he is on the scientific advisory board of Applied Cognition (Redwood City, CA, USA), an activity which is also compensated. Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Rights and permissions Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissionsAbout this articleCite this articleMontag, C., Ali, R. & Davis, K.L. Affective neuroscience theory and attitudes towards artificial intelligence. AI & Soc (2024). https://doi.org/10.1007/s00146-023-01841-8Download citationReceived: 03 May 2023Accepted: 11 December 2023Published: 28 January 2024DOI: https://doi.org/10.1007/s00146-023-01841-8Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative KeywordsArtificial intelligenceTrustSADNESSFEARAttitudes towards artificial intelligencePersonalityBig fivePrimary emotional systems Use our pre-submission checklist Avoid common mistakes on your manuscript. Advertisement Search Search by keyword or author Search Navigation Find a journal Publish with us Track your research Discover content Journals A-Z Books A-Z Publish with us Journal finder Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support Cancel contracts here 103.14.92.60 Not affiliated © 2024 Springer Nature"
Frontiers | Development and validation of the AI attitude scale (AIAS-4): a brief measure of general attitude toward artificial intelligence,https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1191628/full,未知,2024-11-21 17:10:04,en,AI AND attitude,google,"Frontiers | Development and validation of the AI attitude scale (AIAS-4): a brief measure of general attitude toward artificial intelligence Skip to main content <iframe src=""https://tag-manager.frontiersin.org/ns.html?id=GTM-M322FV2&gtm_auth=owVbWxfaJr21yQv1fe1cAQ&gtm_preview=env-1&gtm_cookies_win=x"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> Top bar navigation Frontiers in Psychology About us About us Who we are Mission and valuesHistoryLeadershipAwardsImpact and progress Frontiers' impactProgress Report 2022All progress reportsPublishing model How we publishOpen accessFee policyPeer reviewResearch integrityResearch TopicsServices SocietiesNational consortiaInstitutional partnershipsCollaboratorsMore from Frontiers Frontiers ForumFrontiers Planet PrizePress officeSustainabilityCareer opportunitiesContact us All journalsAll articles Submit your research Search Frontiers in Psychology Sections Sections Addictive BehaviorsAuditory Cognitive NeuroscienceCognitionCognitive ScienceComparative PsychologyConsciousness ResearchCultural PsychologyDecision NeuroscienceDevelopmental PsychologyEating BehaviorEducational PsychologyEmotion ScienceEnvironmental PsychologyEvolutionary PsychologyForensic and Legal PsychologyHealth PsychologyMedia PsychologyMindfulnessMovement ScienceNeuropsychologyOrganizational PsychologyPediatric PsychologyPerception SciencePerformance SciencePersonality and Social PsychologyPositive PsychologyPsycho-OncologyPsychology for Clinical SettingsPsychology of AgingPsychology of LanguagePsychopathologyQuantitative Psychology and MeasurementSport PsychologyTheoretical and Philosophical Psychology ArticlesResearch TopicsEditorial board About journal About journal Scope Field chief editorsMission & scopeFactsJournal sectionsOpen access statementCopyright statementQualityFor authors Why submit?Article typesAuthor guidelinesEditor guidelinesPublishing feesSubmission checklistContact editorial office About us About us Who we are Mission and valuesHistoryLeadershipAwardsImpact and progress Frontiers' impactProgress Report 2022All progress reportsPublishing model How we publishOpen accessFee policyPeer reviewResearch integrityResearch TopicsServices SocietiesNational consortiaInstitutional partnershipsCollaboratorsMore from Frontiers Frontiers ForumFrontiers Planet PrizePress officeSustainabilityCareer opportunitiesContact us All journalsAll articles Submit your research Frontiers in Psychology Sections Sections Addictive BehaviorsAuditory Cognitive NeuroscienceCognitionCognitive ScienceComparative PsychologyConsciousness ResearchCultural PsychologyDecision NeuroscienceDevelopmental PsychologyEating BehaviorEducational PsychologyEmotion ScienceEnvironmental PsychologyEvolutionary PsychologyForensic and Legal PsychologyHealth PsychologyMedia PsychologyMindfulnessMovement ScienceNeuropsychologyOrganizational PsychologyPediatric PsychologyPerception SciencePerformance SciencePersonality and Social PsychologyPositive PsychologyPsycho-OncologyPsychology for Clinical SettingsPsychology of AgingPsychology of LanguagePsychopathologyQuantitative Psychology and MeasurementSport PsychologyTheoretical and Philosophical Psychology ArticlesResearch TopicsEditorial board About journal About journal Scope Field chief editorsMission & scopeFactsJournal sectionsOpen access statementCopyright statementQualityFor authors Why submit?Article typesAuthor guidelinesEditor guidelinesPublishing feesSubmission checklistContact editorial office Frontiers in Psychology Sections Sections Addictive BehaviorsAuditory Cognitive NeuroscienceCognitionCognitive ScienceComparative PsychologyConsciousness ResearchCultural PsychologyDecision NeuroscienceDevelopmental PsychologyEating BehaviorEducational PsychologyEmotion ScienceEnvironmental PsychologyEvolutionary PsychologyForensic and Legal PsychologyHealth PsychologyMedia PsychologyMindfulnessMovement ScienceNeuropsychologyOrganizational PsychologyPediatric PsychologyPerception SciencePerformance SciencePersonality and Social PsychologyPositive PsychologyPsycho-OncologyPsychology for Clinical SettingsPsychology of AgingPsychology of LanguagePsychopathologyQuantitative Psychology and MeasurementSport PsychologyTheoretical and Philosophical Psychology ArticlesResearch TopicsEditorial board About journal About journal Scope Field chief editorsMission & scopeFactsJournal sectionsOpen access statementCopyright statementQualityFor authors Why submit?Article typesAuthor guidelinesEditor guidelinesPublishing feesSubmission checklistContact editorial office Submit your research Search Download article Download PDF ReadCube EPUB XML (NLM) Share on Export citation EndNote Reference Manager Simple Text file BibTex 30,5K Total views 7,6K Downloads 26 Citations Citation numbers are available from Dimensions View article impact View altmetric score Share on Edited by Runxi Zeng Chongqing University, China Reviewed by Minh-Hoang Nguyen Centre for Interdisciplinary Social Research, Phenikaa University, Vietnam Verena Nitsch Institute of Industrial Engineering and Ergonomics, Faculty of Mechanical Engineering, RWTH Aachen University, Germany Table of contents Abstract 1. Introduction 2. Methods 3. Results 4. Discussion 5. Conclusion Data availability statement Ethics statement Author contributions Acknowledgments Conflict of interest Publisher’s note Supplementary material Footnotes References Export citation EndNote Reference Manager Simple Text file BibTex Check for updates Download article Download Download PDF ReadCube EPUB XML (NLM) ORIGINAL RESEARCH article Front. Psychol., 24 July 2023 Sec. Media Psychology Volume 14 - 2023 | https://doi.org/10.3389/fpsyg.2023.1191628 This article is part of the Research Topic Coping with an AI-Saturated World: Psychological Dynamics and Outcomes of AI-Mediated Communication View all 6 articles Development and validation of the AI attitude scale (AIAS-4): a brief measure of general attitude toward artificial intelligence Simone Grassini1,2* 1Department of Psychosocial Science, University of Bergen, Bergen, Norway 2Cognitive and Behavioral Neuroscience Lab, University of Stavanger, Stavanger, Norway The rapid advancement of artificial intelligence (AI) has generated an increasing demand for tools that can assess public attitudes toward AI. This study proposes the development and the validation of the AI Attitude Scale (AIAS), a concise self-report instrument designed to evaluate public perceptions of AI technology. The first version of the AIAS that the present manuscript proposes comprises five items, including one reverse-scored item, which aims to gauge individuals’ beliefs about AI’s influence on their lives, careers, and humanity overall. The scale is designed to capture attitudes toward AI, focusing on the perceived utility and potential impact of technology on society and humanity. The psychometric properties of the scale were investigated using diverse samples in two separate studies. An exploratory factor analysis was initially conducted on a preliminary 5-item version of the scale. Such exploratory validation study revealed the need to divide the scale into two factors. While the results demonstrated satisfactory internal consistency for the overall scale and its correlation with related psychometric measures, separate analyses for each factor showed robust internal consistency for Factor 1 but insufficient internal consistency for Factor 2. As a result, a second version of the scale is developed and validated, omitting the item that displayed weak correlation with the remaining items in the questionnaire. The refined final 1-factor, 4-item AIAS demonstrated superior overall internal consistency compared to the initial 5-item scale and the proposed factors. Further confirmatory factor analyses, performed on a different sample of participants, confirmed that the 1-factor model (4-items) of the AIAS exhibited an adequate fit to the data, providing additional evidence for the scale’s structural validity and generalizability across diverse populations. In conclusion, the analyses reported in this article suggest that the developed and validated 4-items AIAS can be a valuable instrument for researchers and professionals working on AI development who seek to understand and study users’ general attitudes toward AI. 1. Introduction Artificial Intelligence (AI) is a terminology that refers to a technology that enables software and machines to emulate human intelligence (see, e.g., Fetzer and Fetzer, 1990). The potential uses of AI systems span various disciplines, including computer science, engineering, biology, neuroscience, and psychology. AI is rapidly transforming various aspects of modern society, including healthcare, transportation, finance, and education (Harari, 2017; Makridakis, 2017). As AI technologies become more integrated into daily life, understanding public attitudes and perceptions toward AI is crucial for guiding their development, adoption, and regulation (Zhang and Dafoe, 2019; Araujo et al., 2020; O’Shaughnessy et al., 2022). The ongoing progress in AI has led to the development of potentially groundbreaking technological developments such as self-driving cars (Hong et al., 2021). Additionally, AI-based products like Apple’s Siri and Amazon’s Alexa have gained widespread adoption in everyday life (Brill et al., 2019), providing voice-command services for tasks like weather forecasts and navigation. Moreover, AI research has given rise to humanoid robots, such as social robots (Sandoval et al., 2014; Glas et al., 2016; for a review see Hentout et al., 2019). AI’s integration into daily life offers numerous benefits, such as safer driving (Manoharan, 2019), and improved healthcare (Johnson et al., 2021). This expansion of artificial intelligence presents both exciting opportunities and potential challenges. While AI has the potential to revolutionize numerous industries, it also carries risks such as job displacement due to automation (Tschang and Almirall, 2021). Although AI can generate new employment opportunities, these roles may be markedly different from those that AI replaces (Wilson et al., 2017). Public opinion regarding AI is diverse, with some embracing its benefits, while others exhibit ambivalence or even anxiety (Fast and Horvitz, 2017). Prominent figures such as Stephen Hawking have cautioned that the progression of AI could ultimately endanger human existence (Cellan-Jones, 2014; Kumar and Choudhury, 2022). Similarly, Tesla CEO and tech entrepreneur Elon Musk has expressed concerns about AI development on multiple occasions (Cellan-Jones, 2014). He, along with other experts, has advocated for a ban on military robots (Gibbs, 2017). In March of this year, Musk and several key figures in the AI field penned an open letter calling for a temporary halt to AI development (Pause Giant AI Experiments: An Open Letter, 2023), though this proposal has received mixed reactions from other experts that instead emphasizes as AI can be an opportunity for the future of human kind (Samuel, 2023; see also earlier works as, e.g., Archer, 2021; Samuel, 2021). Considering the ongoing debate and the numerous perspectives held by various interested parties, stakeholders, and ordinary citizens, it is of paramount importance for psychologists and cognitive scientists interested in human-computer interaction and more in general in the acceptability and adoption of emerging technologies, to develop and implement a concise, reliable, and valid instrument for assessing attitudes toward AI for assessing attitudes toward AI, that can be easily implemented and assessed. Investigating this area can yield invaluable insights into the determinants influencing individuals’ acceptance or resistance to AI, as well as the potential benefits and risks they associate with its deployment (Nadimpalli, 2017; Eitel-Porter, 2021). Moreover, understanding these factors can aid in the development of educational initiatives and public awareness campaigns that address concerns and misconceptions about AI while emphasizing its potential for positive impact and rationally inform about possible dangers. By fostering a deeper understanding of public sentiment and expectations surrounding AI, researchers and policymakers can work together to ensure that AI is developed and implemented responsibly and ethically. Such collaboration will serve to maximize the benefits of AI while mitigating its potential risks, ultimately leading to a more harmonious integration of AI technologies into our daily lives and society. To develop the scale items, recent literature on the topic of AI perception and attitude was examined to identify major themes and previous efforts to create similar scales (see, e.g., Schepman and Rodway, 2020; Sindermann et al., 2021). It was found that experts, the public, and the media all expressed both positive and negative views about AI (Fast and Horvitz, 2017; Anderson et al., 2018; Cave et al., 2019). Large-scale surveys confirmed these mixed perspectives and reflected similar key themes (Zhang and Dafoe, 2019). Public concerns included job displacement, ethical issues, and non-transparent decision-making. In contrast, positive attitudes centered on AI’s potential to improve efficiency and provide innovative solutions in various domains. Interestingly, a phenomenon called “algorithm appreciation” has been reported, wherein AI was preferred over humans in certain contexts (Logg et al., 2019). Some studies delved deeper into specific aspects of attitude toward AI. For instance, potential job loss emerged as a significant concern due to the high computerizability of numerous occupations (Chui et al., 2016; Frey and Osborne, 2017). Ethical concerns were also prominent, particularly in medical settings (Morley et al., 2020). Public opinion was divided on using AI for tasks traditionally performed by medical staff, and many people felt uncomfortable with personal medical information being used in AI applications. Vayena et al. (2018) suggested that trust in AI could be promoted through employing measures for personal data protection, unbiased decision-making, appropriate regulation, and transparency, a view supported by other recent discussions on trust in AI (Schaefer et al., 2016; Barnes et al., 2019; Sheridan, 2019; Middleton et al., 2022). Although there are existing instruments that evaluate people’s acceptance of technology (e.g., Davis, 1989; Parasuraman and Colby, 2015), most of them do not specifically address AI, which may differ in key aspects regarding acceptance. The Technology Acceptance construct (Davis, 1989) primarily emphasizes a user’s willingness to adopt technology through consumer choice. However, AI adoption frequently does not involve consumer choice, as large organizations and governments often implement AI without seeking input from end users, who are then left with no option but to engage with it. Consequently, traditional technology acceptance measures may not be ideally suited for gauging attitudes toward AI. Other existing measures tend to be lengthy, context-specific, or lack empirical validation. Additionally, most questionnaires designed for evaluating technological attitudes and acceptance (e.g., The Media and Technology Usage and Attitudes Scale - MTUAS, see Rosen et al., 2013) have not been specifically developed and validated considering modern consumer-oriented AI systems, such as OpenAI ChatGPT or text-to-image AI services. In recent years, few scales have been developed specifically with the aim to assess attitudes toward AI. Schepman and Rodway (2020) introduced the General Attitudes toward Artificial Intelligence Scale (GAISS), a 20-item scale with a two-factor structure (positive and negative attitudes toward AI). Sindermann et al. (2021) proposed a concise 5-item scale, the Attitude Toward Artificial Intelligence scale (ATAI), featuring a two-factor structure (acceptance and fear). Another scale (Threat of Artificial Intelligent Scale, TAI; Kieslich et al., 2021) has also been developed to specifically assess fear in AI technology. However, these scales face challenges concerning their practicality and possible use in the context of assessing general attitude. The GAISS, with its many items, can be time-consuming to administer, making it less suitable for large-scale surveys or situations with limited participant time. In contrast, the ATAI scale is easier to administer but emphasizes extreme outcomes with items like “Artificial intelligence will destroy humankind” or “Artificial intelligence will benefit humankind.” This approach may not capture the nuanced attitudes people may have, which could include a mix of optimism, skepticism, and concerns about specific issues. The scale also appears to focus more on negative AI aspects (fear, destruction, and job losses) than on positive or factual aspects, potentially leading to an overemphasis on negative attitudes and a less accurate representation of people’s overall AI perceptions. The ATAI scale also highlights emotions, with three out of five items loading on the “fear” factor and only two on the “acceptance” factor in the two-factor model proposed by Sindermann et al. (2021). Moreover, both scales were developed and tested in a context where large language models (LLMs) like ChatGPT were not yet widely available and the public debates following these AI systems did not emerge yet (for a review see Leiter et al., 2023) or discussed. The rapid implementation of these AI systems and increased exposure to information about them may have significantly altered public attitudes toward AI technology and how these systems are perceived as useful and impactful for the future. Developing a concise, reliable, and valid scale for assessing AI attitudes would facilitate research and practice in understanding and addressing public perceptions of AI. A short and usable scale would also support non-psychologists, as AI professionals and the technology sector in gathering critical individual variables when testing products for the general population. 1.1. The present study This study aims to address the existing gap in scientific understanding by creating and validating the AI Attitude Scale (AIAS), a concise instrument designed to assess public attitudes toward AI. The AIAS strives to be a brief yet dependable tool that is easy and quick to administer for both psychological research and non-psychologists interested in gauging users’ or citizens’ attitudes toward AI, such as software developers, businesses, organizations, and stakeholders. Drawing on established theoretical frameworks, such as the Technology Acceptance Model (TAM) (Davis, 1989) and the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh et al., 2003), as well as empirical research on AI-related risks (Cheatham et al., 2019; Kaur et al., 2022), and societal implications (Ernst et al., 2019; Tschang and Almirall, 2021). The scale considers various aspects of AI’s potential influence on individuals’ lives, work, and the broader human experience, while also addressing the likelihood of future AI adoption. The AIAS scale is designed to capture attitudes toward AI by focusing on the perceived utility and potential impact of technology on society and humanity, including perceived benefits, potential risks, and intentions to use AI technologies. In line with items (and theoretical standpoints) discussed in similar scales (GAISS and ATAI), the AIAS also includes some items related that can be related to emotional statements, however, does not focus in evaluating AI-elicited emotions but attempts to provide a balanced assessment of attitudes toward AI. The present article, based on 2 studies from distinct populations, outlines the development and validation of the AIAS, starting with an examination of the theoretical underpinnings of the scale items and the methods employed in scale development. Subsequently, the results of an exploratory factor analysis (EFA) and an assessment of the scale’s internal consistency and convergent validity are presented. Finally, the scale factor structure was evaluated using confirmatory factor analysis (CFA) with a different sample of participants. 2. Methods 2.1. Scale development The initial AIAS consisted of 5 items. One of the items was reverse scored to reduce response bias (DeVellis and Thorpe, 2021). Participants rated their agreement with each item using a 10-point Likert scale (1 = Not at all, 10 = Completely Agree). A 10-point scale was chosen for the high test–retest reliability, easy to use (Preston and Colman, 2000), as well as good level of granularity. The selection of these specific items was based on several considerations. Primarily, the intent was to provide a comprehensive coverage of key dimensions of attitudes toward AI, derived from prior theoretical and empirical work in technology acceptance and risk perception literature. This included factors such as perceived usefulness, potential societal impact, adoption intention, risk perception, and overall evaluation of AI’s impact on humanity. Furthermore, it was sought to incorporate the breadth of public opinion, acknowledging that attitudes toward AI can range from optimistic enthusiasm to concern and even fear of potential risks. The items originally included in the t-items scale were the following: 1) I believe that AI will improve my life. This item was derived from the Technology Acceptance Model (TAM) (Davis, 1989), which posits that perceived usefulness (i.e., the degree to which an individual believes that using a technology will enhance their performance) is a primary determinant of technology acceptance. In the context of AI, this item assesses individuals’ beliefs about the potential benefits of AI in improving their quality of life. Such theory has been applied recently in the field of AI (see, e.g., Sohn and Kwon, 2020). 2) I believe that AI will improve my work. Like the first item, this item is also grounded in the TAM (Davis, 1989) and reflects individuals’ beliefs about the potential benefits of AI in the work domain. This item addresses the role of AI in generally improving human work condition, in line with expectations of AI technology discussed in the current scientific literature (Wijayati et al., 2022). 3) I think I will use AI technology in the future. This item is based on the concept of behavioral intention, which is central to several theories of technology acceptance, including the TAM (Davis, 1989) and the Unified Theory of Acceptance and Use of Technology (UTAUT) (Venkatesh et al., 2003), including its current discussion in relationship with AI (Venkatesh, 2022). Behavioral intention is considered a proximal determinant of actual technology use, and this item measures individuals’ intentions to adopt and use AI technologies in the future. 4) I think AI technology is a threat to humans (reverse item). This reverse-scored item assesses individuals’ concerns about the potential risks and negative consequences of AI technology. The item is grounded in the literature on the risk of super intelligent AI (Barrett and Baum, 2017) and risk perception associated with technology (Slovic, 1987; Frey and Osborne, 2017). These theoretical standpoints highlight the importance of understanding individuals’ concerns about the potential dangers posed by AI, such as job displacement, loss of privacy, or even existential threats. 5) I think AI technology is positive for humanity. This item captures individuals’ overall evaluation of the impact of AI technology on society and humanity as a whole. The item is grounded in the literature on the social implications of AI (Brynjolfsson and McAfee, 2014) and reflects the belief that AI can contribute to societal progress and well-being by addressing complex global challenges, improving healthcare (Loh et al., 2022), enhancing education (Xia et al., 2022; Yang, 2022), and fostering economic growth (Jones, 2022; Matytsin et al., 2023). In conclusion, the selection of these items was aimed at achieving a balance between theoretical grounding, and coverage of the range of public attitudes toward AI. It is also noteworthy that these items may not be exhaustive and cover all the aspects related to AI attitude, and that future iterations of the scale may incorporate additional items to further enrich the measurement of attitudes toward AI. 2.2. Participants and procedure Prior to the experiments, participants were given an overview on what it is AI, and how it is used. This included information on current AI developments like virtual assistants, content recommendation algorithms for media streaming services, and AI-powered communication tools such as grammar checkers and chatbots. These uses of AI systems were explicitly mentioned in the information document prior to the presentation of the questionnaires. The aim was to engage the participants in thinking about the essence of AI and its ongoing advancements, thus minimizing potential sources of misinterpretation. In the first study (EFA), a gender-balanced convenience sample of 230 UK adults was assembled using Prolific, an online platform for participant recruitment. A sample size of >200 for factor analysis is generally accepted as fair according with the literature (Comrey and Lee, 2013). All participants indicated they used a computer or laptop with a physical keyboard. The sample size was determined by the number of variables analyzed in a more comprehensive survey on AI-generated data perception, including the questions investigated in this article. EFA is typically used for exploratory purposes, where the goal is to uncover the underlying factor structure of a set of observed variables, and it is a data-driven technique that allows for flexibility in model specification. While the scale items emerged from theoretical standpoints, it was preferred to first establish their underlying factor structure through EFA to allow for an unbiased exploration of the data, and secondly perform a CFA with a different participant sample. For the second study (CFA), a separate convenience sample of 300 US adults, with characteristics like the UK sample, was recruited through the same platform. The sample size was based on the number of variables examined in an extensive survey focusing on human-computer and human-AI interaction, incorporating the questions examined in this article. In both studies, participants were required to be fluent in English and over 18 years old. All participants in both studies were asked to read and explicitly accept an informed consent form before participating. They were informed about the tasks they would be performing and reminded of their right to withdraw from the study at any time. Both studies adhered to the Declaration of Helsinki for scientific studies involving human participants and complied with local and national regulations. No personally identifiable information was collected from study participants. Participants in both studies completed an online survey that included the questions included in the AIAS, along with other measures of attitudes toward technology, as well as demographic questions. The attitude dimension of the Media and Technology Usage and Attitudes Scale (MTUAS, Rosen et al., 2013), was measured and used to explore convergent validity in both studies. The MTUAS is a comprehensive psychometric instrument designed to assess individuals’ usage patterns and attitudes toward various forms of media and technology. It has shown to have a good validity as psychometric measure (e.g., Sigerson and Cheng, 2018). This scale encompasses multiple dimensions, and the one considered in the present study are positive and negative attitudes toward technology, and anxiety related to technology use. The MTUAS has been extensively used by researchers and practitioners to gain insights into how people interact with and perceive technology in their daily lives or in professional environments (e.g., Rashid and Asghar, 2016; Becker et al., 2022; Srivastava et al., 2022). The MTUAS factors of positive, negative, multi-tasking and anxiety were included in the original survey (as they are presented together in the original scale), however, after preliminary analyses it was decided to not report the analyses for the multi-tasking factor of the scale, as not directly related to technology attitude. For both studies, the surveys were developed using Psyktoolkit (Stoet, 2010, 2017). The surveys included a questionnaire battery containing attention check questions, such as “select the highest value for this item” or “select the lowest value for this item.” Participants who failed one or more attention checks were excluded from the sample and replaced until the pre-determined final samples were reached. In the first study, six participants needed replacement, while in the second study, 18 participants required replacement due to failing one or more attention checks. In the survey, the sample characteristics were examined, including age, gender, and level of education. In study 1, the sample featured a balanced distribution of males and females (114 each), with two participants choosing not to answer the question or selecting a third gender option. The ages of the participants ranged from 18 to 76 years old (M = 40.2, SD = 14.6). A majority held a university degree (59.6%), followed by high school (38.7%). A smaller portion of the sample had completed middle school (0.9%) or elementary school (0.9%), and none reported having no formal schooling. The age of the participants ranged from 18 to 76 years old (M = 41.5, SD = 15). A majority held a university degree (60.7%), followed by high school (38.7%). A smaller portion of the sample had completed middle school (0.9%) or elementary school (0.9%), and none reported having no formal schooling. In study 2, the sample as well featured a balanced distribution of males (N = 142) and females (N = 150), with eight participants choosing not to answer the question or selecting a third gender option. The ages of the participants ranged from 18 to 81 years old (M = 40.2, SD = 14.6). A majority held a university degree (59.6%), followed by high school (39%), and only one reported only completion of middle school (0.3%). None of the participants reported having completed only elementary school or not having any formal schooling. The age of the participants ranged from 18 to 76 years old (M = 40.2, SD = 14.6). A majority held a university degree (59.6%), followed by high school (38.7%). A smaller portion of the sample had completed middle school (0.9%) or elementary school (0.9%), and none reported having no formal schooling. 2.3. Data analysis EFAs were performed in data from Study 1 to assess the factor structure of the AIAS (Fabrigar and Wegener, 2011). The factor structure (1 factors, 4 items) identified in study 1, was further validated using a CFA with the data collected in study 2. Internal consistency, and convergent validity with related measures were also examined for the sample in both studies (Cronbach, 1951; Shrout and Fleiss, 1979). Correlation matrixes were computed to evaluate convergent validity of the developed scale with the MTUAS scale (positive, negative, and anxiety factors). Multiple linear regressions were computed to understand the relationship between the background information collected for the samples and AIAS score. Data analysis was performed using the statistical software Jamovi 2.3.21 (The Jamovi Project, 2022). 3. Results 3.1. Study 1 (EFA) 3.1.1. The 5-items original scale First, descriptive statistics were obtained for each item of the scale. The descriptive statistics for the AI attitude scale items are presented in Table 1. The skewness values for the items indicated that the data is roughly symmetric. The kurtosis values for the items indicated that the data is platykurtic. TABLE 1 Table 1. Descriptive statistics for each item. The data was analyzed to assess whether the assumptions necessary for performing an EFA were satisfied. The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy, a critical index for determining the suitability of the data for factor analysis, was computed. With an overall KMO value of 0.827, the data demonstrated meritorious sampling adequacy, thereby confirming its appropriateness for exploratory factor analysis (Williams et al., 2010). Furthermore, Bartlett’s test of sphericity was conducted to evaluate the presence of significant correlations between the items, a prerequisite for factor analysis. The test yielded a significant result [χ2(10) = 612, p < 0.001], providing evidence of substantial correlations among the items. Consequently, these findings support the validity of proceeding with the EFA. In the EFA, maximum likelihood extraction method and oblimin rotation were used. Oblimin rotation was preferred as it was assumed that the factors extracted may correlated with each other. Two factors were extracted based on parallel analysis. As shown in Table 2, the factor loadings suggest that Items 1, 2, and 3 loaded moderately to highly on Factor 1, while Items 4 and 5 loaded highly on Factor 2. The uniqueness values indicate the proportion of variance in each item that is not explained by the factors. Inter-factor correlation was shown to be high (0.749), confirming the adequacy of the use of Oblimin rotation method. Reliability of the scale was analyzed using Cronbach alpha and McDonald’s omega. TABLE 2 Table 2. Factor loadings from the EFA of the 5 items scale. Table 3 shows the mean scores, standard deviations, Cronbach’s alpha, and McDonald’s omega values for each factor and for the entire scale. Factor 1, with a Cronbach’s alpha of 0.892 and McDonald’s omega of 0.892, showed good internal consistency. Factor 2 with a Cronbach’s alpha of 0.496 and McDonald’s omega of 0.504, showed an overall internal consistency lower than Factor 1. The entire scale had a Cronbach’s alpha of 0.830 and McDonald’s omega of 0.860, indicating good internal consistency. TABLE 3 Table 3. Mean, SD, Cronbach’s alpha, and McDonald’s omega values for each factor and for the entire scale. A correlation matrix was computed to visualize the correlations between the questionnaire items. Table 4 shows a matrix of the 5 items included in the AI attitude scale. TABLE 4 Table 4. Correlation between the items included in the AIAS questionnaire. These analyses showed relatively poor loading of item 4 in the established factors. Therefore, the item was removed from the analyses and the scale analyzed again without the item. 3.1.2. The 4-items revised AIAS scale Due to the relatively poor loading on factor 2 and the poor correlation with the other items, item 4 of the questionnaire was eliminated from the analyses, trying to establish a scale with better psychometric characteristics compared to the 2-factors structure previously proposed. For the 4-items questionnaire, the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy obtained a value of 0.83, demonstrated again meritorious sampling adequacies (Williams et al., 2010). Bartlett’s test of sphericity yielded a significant result [χ2(6) = 583, p < 0.001], providing evidence of substantial correlations among the items. For the EFA of the 4-items version of the questionnaire were sed the same extraction and rotation methods as the ones previously used (maximum likelihood and oblimin rotation). Table 5 shows that all the items highly load on just one factor. TABLE 5 Table 5. Factor loadings from the EFA of the 4 items scale. The 4-items scale shows a mean score of 5.48 (SD = 1.99) and a Cronbach’s alpha of 0.902 and McDonald’s omega of 0.904, indicating good internal consistency, and exceeding the internal consistency reported for both the total 5-items scale and the 3-items factor 1 identified in the previous analyses. Correlation analyses were computed between the 4-items AIAS and the attitude factors of the Media and Technology Usage and Attitudes Scale (MTUAS), as previously. The correlation matrix is presented in Table 6. The analyses for the 4-items AIAS do not show significant different relationship with the attitude dimensions of MTUAS compared to what observed for the same analyses where the Factor 1 of the 2-factors 5-items AIAS was included in the analyses, confirming the interpretation of the relationship between AIAS and MTUAS proposed earlier. TABLE 6 Table 6. Correlation matrix for the factors of MTUAS-attitude and the 1-factors 4-items AIAS scale. A multiple linear regression was computed to understand how the background information collected for the sample can explain the total score of the 4-items AIAS. The analysis was conducted to examine the relationships between age, gender, and education levels with the total score of the 4-items AIAS. The overall model was statistically significant. Gender emerged as the only statistically significant predictors of the 4-items AIAS total score among the ones used in the model, with female participants showing lower scores compared to male participants. Age and education levels (p > 0.05 for all comparisons) were not found to be significant predictors of the 4-items AIAS total score. Table 7 reports the results of the multiple linear regression model. Estimates are obtained with ordinary least squares minimizing the sum of squared deviations between each observed value and predicted values. TABLE 7 Table 7. Multiple linear regression model summarizing the relationships between age, gender, and education levels as predictors of 4-items AIAS total score. 3.2. Study 2 (CFA) In study 2, the 4-items version of the AIAS validated from study 1 was tested, and CFA was conducted. Descriptive statistics were calculated for each of the four items in the scale. Table 8 presents the descriptive statistics for the AI attitude scale items. TABLE 8 Table 8. Descriptive statistics for each item of 4-items AIAS in study 2. A Confirmatory Factor Analysis was conducted to examine the factor structure of the 4-item questionnaire with the sample from Study 2. The factor loadings for all items were statistically significant, suggesting that the items were strong indicators of the underlying factor. Results including standardized estimate (Rosseel, 2012) are reported in Table 9. TABLE 9 Table 9. CFA factor loading and statistics including standard estimate. The model fit was assessed using several fit indices. The chi-square test for exact fit was not significant, indicating a good model fit [χ2(2) = 2.49, p = 0.289] (but see DiStefano and Hess, 2005). Additionally, the Comparative Fit Index (CFI) and the Tucker-Lewis Index (TLI) were both above the recommended threshold (see Byrne, 2016) of 0.95 (CFI = 0.999, TLI = 0.998), suggesting a good fit between the hypothesized model and the observed data. The results for the Root Mean Square Error of Approximation (RMSEA) indicate a good model fit as the value is below the suggested cutoff (MacCallum et al., 1996) of 0.05 (RMSEA = 0.0285, 90% CI [0 0.122]). Overall, the Confirmatory Factor Analysis results support the proposed single-factor structure of the 4-item questionnaire, with strong factor loadings and a good model fit (see guidelines as in Sun, 2005). Reliability analysis was then computed similarly as in Study 1. The analysis for the 4-items scale indicated good internal consistency, similar as what was observed for Study 1. A correlation table was computed to visualize the correlations between the questionnaire items. Table 10 shows a correlation matrix of the 4 items included in the second version of the AI attitude scale. TABLE 10 Table 10. Correlation between the items included in the AIAS questionnaire. Similarly, to what was done for Study 1, an additional statistical analysis was conducted to assess the convergent validity between the Artificial Intelligence Attitude Scale (AIAS) and the attitude components of the Media and Technology Usage and Attitudes Scale (MTUAS). Pearson’s correlation coefficients were calculated between the attitude components of MTUAS and the two-factor components of AIAS. The correlation matrix is shown in Table 11. The results of the correlation matrix confirm the results already reported for the 4-items AIAS shown in Study 1, with the AIAS being moderately associated with MTUAS-p and MTUAS-n, and weakly associated with MTUAS-a. Results are shown in Table 12. TABLE 11 Table 11. Correlation matrix for the factors of MTUAS-attitude and the 4-items AIAS scale. TABLE 12 Table 12. Multiple linear regression model summarizing the relationships between age, gender, and education levels as predictors of 4-items AIAS total score for Study 2. A further multiple linear regression was computer to understand how the background information collected for the sample can explain the total score of the 4-items AIAS for the sample of Study 2. These analyses follow the same steps of the multiple linear regression performed for the sample of Study 1. The overall model was statistically significant. As for the analysis reported for Study 1, gender emerged as the only statistically significant predictors of the 4-items AIAS score among the ones used in the model, with female participants showing lower scores compared to male participants. Age and education levels were not found to be significant predictors of the 4-items AIAS total score. Table 12 reports the results of the multiple linear regression model. Estimates are obtained as described earlier. 4. Discussion In this article, the AI Attitude Scale (AIAS) is presented, and its psychometric properties were evaluated. The AIAS items were developed based on established theoretical frameworks and empirical research in technology acceptance, AI, and risk perception (Davis, 1989; Venkatesh et al., 2003; Barrett and Baum, 2017). The scale captures key dimensions of AI attitudes, including perceived benefits, potential risks, and intentions to use AI technologies (Brynjolfsson and McAfee, 2014; Bostrom, 2016). Drawing from the Technology Acceptance Model (TAM), the Unified Theory of Acceptance and Use of Technology (UTAUT), and literature on AI-related risks and societal implications, the AIAS provides a quick and psychometrically validated approach to understanding general attitude toward AI (e.g., dos Santos et al., 2019; Persson et al., 2021; Scott et al., 2021; Vasiljeva et al., 2021; Young et al., 2021). Furthermore, some of the items included in the AIAS were similar to items that were included in scales previously developed to understand attitude toward AI: Item 4 of the AIAS preliminary 5-items scale is similar with the item number 3 of the ATAI scale (“Artificial intelligence will destroy humankind”), however, it has a softer and more nuanced tone, not directly suggesting that AI will annihilate humankind. Item 5 of the preliminary version of the scale is also similar with the item number 4 of the ATAI scale (“Artificial intelligence will benefit humankind”). An exploratory factor analysis (EFA) was conducted to investigate the factor structure of the original 5-item AIAS scale. The preliminary EFA indicated that the scale could be divided into two factors with moderate-to-high correlations between them. Factor 1, consisting of items related to individual attitudes toward AI (AIAS-IA), comprises the following items: “I believe that AI will improve my life,” “I believe that AI will improve my work,” and “I think I will use AI technology in the future.” Factor 2, which can be interpreted as a social attitude toward AI and its risks, includes the items “I think AI technology is a threat to humans” and “I think AI technology is positive for humanity.” Factor 1 demonstrated high internal consistency and reliability, as evidenced by the high values of Cronbach’s alpha coefficient and McDonald’s omega coefficient (>0.8) (Venkatesh, 2022). This suggests that the items in Factor 1 are closely related and measure the same underlying construct of attitudes toward the potential benefits of AI. The high reliability of Factor 1 indicates that it is a suitable and trustworthy measure for assessing attitudes toward the positive aspects of AI. In contrast, Factor 2 exhibited lower internal consistency and reliability compared to Factor 1, with a Cronbach’s alpha coefficient value of 0.496 and a McDonald’s omega coefficient value of 0.504 (Rosen et al., 2013). These findings imply that the items in Factor 2 may not be as strongly related or measure the same underlying construct as effectively as Factor 1. While Factor 2 may still be useful in assessing attitudes toward the potential risks of AI, further development of this factor may be needed in future studies. The 4-items AIAS scale, developed by excluding item 4 from the original 5-item scale, demonstrated higher levels of internal consistency compared to the previously analyzed 2-factor scale. This suggests that the 4-item scale may provide a more reliable and cohesive measure of AI attitudes, thereby enhancing the usefulness of the AIAS in research and future applications within the information systems domain (Collins et al., 2021). The excluded item 4 may have been poorly understood by participants or might have measured a different psychological construct. Upon examination, this item could potentially be measuring fear toward AI (or a perceived direct threat posed by AI technology), rather than general positive or negative attitudes toward the technology (this is in line with the fact that such item is reported in the “fear” factor in the Sindermann et al., 2021). In the survey, sample background variables were tested as predictors of the 4-items AIAS total score. Among the independent variables, gender emerged as the only statistically significant predictor of the 4-items AIAS total score. Female participants scored lower compared to males, suggesting that men may have more positive attitudes toward AI technology. This finding aligns with existing research indicating that men generally have more positive attitudes and higher levels of acceptance toward technology than women (e.g., Schumacher and Morahan-Martin, 2001; Ong and Lai, 2006; Ziefle and Wilkowska, 2010). However, the effect could depend on the specific technology (e.g., da Silva and Moura, 2020). A confirmatory factor analysis (CFA) was conducted in a second study with 300 participants from the USA, who had similar characteristics to the first study’s sample. The analysis confirmed that the 1-factor model (4-items) of the IAPS demonstrated an adequate fit to the data, providing further evidence for the scale’s structural validity and generalizability across different populations. Follow-up analyses in Study 2 confirmed that the AIAS exhibited a good level of internal consistency, comparable to the findings in Study 1. Additionally, further analyses on convergent validity (with the analyzed factors of MTUAS) and individual differences of participants revealed similar trends to those reported in Study 1, strengthening the generalizability of the reported findings. In summary, the reported analyses suggest the use of the one-factor 4-items scale. Future studies may want to further investigate how to efficiently measure social attitude toward AI, reworking and modifying items n. 4 and n. 5 from the originally proposed 5-items AIAS scale, or developing multi-factorial scales that specialized in particular aspects of the attitude toward AI technology. 4.1. Limitation and future research The present research has some limitations that should be considered by researchers and professionals interested in using the proposed scale. First, the sample used in this study is not representative of the general population. The samples should be considered a convenience sample from UK (study 1) and USA (study 2) population, and the findings may not generalize to other populations with different demographic characteristics or cultural backgrounds (Bryman, 2016). As the sample was recruited online among people offering to perform research tasks using the online platform Prolific, the sample may be composed of people who are generally more IT-savvy compared to the general population. Despite participants using Prolific having generally been found to deliver high data quality (see, e.g., Peer et al., 2017), participants in online studies can be less motivated compared to, e.g., students participating in filling surveys in campus. However, the uses of control question for attention check, should have limited participants (or excluded those) that responded randomly or without reading all the items. Future research could validate the AIAS in more diverse samples, including those from different countries and selected age groups, to enhance the scale external validity and its possible impact. Furthermore, this study only utilized self-report measures, which may be subject to social desirability bias or response biases (Podsakoff et al., 2003). Future research could benefit from incorporating behavioral, physiological, and implicit measures to complement self-report data, enhancing the understanding of AI attitudes as both a phenomenon and a psychological construct (Rosen et al., 2013). Physiological measures, in particular, could be employed in lab-based experiments where participants are exposed to information about AI or engage with AI directly. These measures may help researchers to better comprehend automatic reactions to AI-related stress or negative dispositions toward the technology. For example, researchers could track heart rate variability, skin conductance, and cortisol levels as indicators of stress responses or arousal when individuals interact with AI technologies or are presented with AI-related stimuli. Similarly, facial expressions or eye-tracking data could provide valuable information on attention, cognitive processing, and emotional responses to AI. By incorporating these physiological measures, researchers can gain a more comprehensive and nuanced understanding of the factors influencing individuals’ attitudes toward AI and their subsequent behavior (as has been done in the related research field on technostress, Riedl, 2012; La Torre et al., 2019, 2020). Moreover, combining physiological measures with other data collection methods, such as implicit measures or behavioral observations (e.g., choice tasks or response time), could provide valuable insights into the complex interplay of cognitive, affective, and behavioral components that shape AI attitudes. Integrating these complementary methods would enable researchers to not only identify potential biases in self-report data but also to uncover the underlying psychological processes that drive individuals’ reactions to AI and related technologies. Future research may benefit from exploring the impact of various factors on AI attitudes, such as personality traits, cognitive abilities, and exposure to AI technology. This would contribute to a more comprehensive understanding of the factors that influence AI attitudes and help identify potential intervention areas to enhance AI acceptance and adoption. Future studies might also investigate the relationship between AI attitudes and actual AI technology usage, examining whether 4-items AIAS scores predict technology adoption and user behavior. This would provide evidence of the scale’s criterion validity and practical utility for understanding and predicting AI technology adoption in real-world contexts. The 4-items AIAS scale could also be employed to longitudinally evaluate changes in societal attitudes toward AI during periods of rapid development and implementation of AI technologies. This would provide an index to track the status of AI technology acceptability and contribute to a better understanding of the evolving public perception of AI. While the AIAS has shown effectiveness in gaining insights into people’s attitudes toward AI, it presents a somewhat simplified and general perspective of these attitudes. AI, as a technology, is becoming more nuanced, sophisticated, and human-like in its abilities and characteristics. This advancement and complexity of AI technology inherently make people’s attitudes and perceptions toward it more complex. For instance, the recent theoretical developments of AI’s self-prolongation and autonomous minds are elements that go beyond the bounds of what AIAS can directly assess. The expansion of AI’s roles in society and its increasing sophistication adds several layers of complexity that the human mind must challenge when thinking about AI agents. Recent research based on the information-processing-based Bayesian Mindsponge Framework (BMF, see “Mindsponge Theory,” Vuong, 2023) found that people’s perceptions of an AI’s autonomous mind were influenced by their beliefs about the AI’s desire for self-prolongation (Vuong et al., 2023). This suggests a directional pattern of value reinforcement in perceptions of AI and indicates that as AI becomes more sophisticated in the future, it will be harder to set clear boundaries about what it means to have an autonomous mind, therefore changing the perception, evaluation, and the core attitude toward AI agents. The value of the AIAS primarily lies in its convenience and its ability to capture a snapshot of basic attitudes toward AI. However, its simplicity also implies a limitation in capturing the full complexity and range of attitudes toward increasingly human-like AI. Therefore, it is imperative to acknowledge this lack of complexity in the scale as a limitation of the current study. 5. Conclusion In conclusion, the AIAS offers a valuable tool for researchers and practitioners to assess attitudes toward AI technology. The scale can be used to explore factors influencing AI acceptance and adoption, inform the development of AI applications that are better aligned with user needs and expectations, evaluate development of the perception of the AI while the technology and contribute to a more comprehensive understanding of the complex interplay between AI technology and society. The 4-items AIAS scale that was validated in this article is available in pdf format in the supplementary material of the present article (CC BY 4.0) as well as on the open research data repository Zenodo. The file also contains instructions for scoring. The validated 4-items AIAS can be also be retrieved from the following DOI and used freely.1 Please refer to this article when using the scale. Data availability statement The raw data supporting the conclusions of this article will be made available by the author, without undue reservation. Ethics statement Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. The patients/participants provided their written informed consent to participate in this study. Author contributions SG was responsible for the development of the main survey, funding acquisition, the analysis, and article writing. Acknowledgments I thank Aleksandra Sevic (Department of Public Health, University of Stavanger) for giving valuable feedback on the draft of the manuscript. Conflict of interest The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Publisher’s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Supplementary material The Supplementary material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1191628/full#supplementary-material Footnotes 1. ^https://doi.org/10.5281/zenodo.7757534 References Anderson, J., Rainie, L., and Luchsinger, A. (2018). Artificial intelligence and the future of humans. Pew Res. Center 10. Google Scholar Araujo, T., Helberger, N., Kruikemeier, S., and De Vreese, C. H. (2020). In AI we trust? Perceptions about automated decision-making by artificial intelligence. AI & Soc. 35, 611–623. doi: 10.1007/s00146-019-00931-w CrossRef Full Text | Google Scholar Archer, M. S. (2021). “Can humans and AI robots be friends?” in Post-human futures (Routledge), 132–152. Google Scholar Barnes, M., Elliott, L. R., Wright, J. L., Scharine, A., and Chen, J. (2019). Human-robot interaction design research: from teleoperations to human-agent teaming US Army Combat Capabilities Development Command, Army Research Laboratory. Google Scholar Barrett, A. M., and Baum, S. D. (2017). A model of pathways to artificial superintelligence catastrophe for risk and decision analysis. J. Exp. Theor. Artif. Intell. 29, 397–414. doi: 10.1080/0952813X.2016.1186228 CrossRef Full Text | Google Scholar Becker, L., Kaltenegger, H. C., Nowak, D., Weigl, M., and Rohleder, N. (2022). Physiological stress in response to multitasking and work interruptions: study protocol. PLoS One 17:e0263785. doi: 10.1371/journal.pone.0263785 PubMed Abstract | CrossRef Full Text | Google Scholar Bostrom, N. (2016). Fundamental issues of artificial intelligence. 376:520. Google Scholar Brill, T. M., Munoz, L., and Miller, R. J. (2019). Siri, Alexa, and other digital assistants: a study of customer satisfaction with artificial intelligence applications. J. Mark. Manag. 35, 1401–1436. doi: 10.1080/0267257X.2019.1687571 CrossRef Full Text | Google Scholar Bryman, A. (2016). Social research methods. Oxford university press. Google Scholar Brynjolfsson, E., and McAfee, A. (2014). The second machine age: work, progress, and prosperity in a time of brilliant technologies. New York, USA: W. W. Norton & Company. Google Scholar Byrne, B. M. (2016). Structural equation modeling with AMOS: basic concepts, applications, and programming. New York, USA: Psychology Press. Google Scholar Cave, S., Coughlan, K., and Dihal, K. (2019). “Scary robots” examining public responses to AI. in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. Honolulu, HI, USA, 331–337. Google Scholar Cellan-Jones, R. (2014). Stephen hawking warns artificial intelligence could end mankind. BBC News 2:2014. Google Scholar Cheatham, B., Javanmardian, K., and Samandari, H. (2019). Confronting the risks of artificial intelligence. McKinsey Q. 2, 1–9. Google Scholar Chui, M., Manyika, J., and Miremadi, M. (2016). Where machines could replace humans-and where they can’t (yet). Available at: www.mckinsey.com Google Scholar Collins, C., Dennehy, D., Conboy, K., and Mikalef, P. (2021). Artificial intelligence in information systems research: a systematic literature review and research agenda. Int. J. Inf. Manag. 60:102383. doi: 10.1016/j.ijinfomgt.2021.102383 CrossRef Full Text | Google Scholar Comrey, A. L., and Lee, H. B. (2013). A first course in factor analysis. Hlilsdale, New Jersey, USA: Lawrence Arlbaum associated. Google Scholar Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika 16, 297–334. doi: 10.1007/BF02310555 CrossRef Full Text | Google Scholar da Silva, H. A., and Moura, A. S. (2020). Teaching introductory statistical classes in medical schools using RStudio and R statistical language: evaluating technology acceptance and change in attitude toward statistics. J. Stat. Educ. 28, 212–219. doi: 10.1080/10691898.2020.1773354 CrossRef Full Text | Google Scholar Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Q. 13, 319–340. doi: 10.2307/249008 CrossRef Full Text | Google Scholar DeVellis, R. F., and Thorpe, C. T. (2021). Scale development: Theory and applications. Sage publications. Google Scholar DiStefano, C., and Hess, B. (2005). Using confirmatory factor analysis for construct validation: an empirical review. J. Psychoeduc. Assess. 23, 225–241. doi: 10.1177/073428290502300303 CrossRef Full Text | Google Scholar dos Santos, D. P., Giese, D., Brodehl, S., Chon, S. H., Staab, W., Kleinert, R., et al. (2019). Medical students' attitude towards artificial intelligence: a multicentre survey. Eur. Radiol. 29, 1640–1646. doi: 10.1007/s00330-018-5601-1 PubMed Abstract | CrossRef Full Text | Google Scholar Eitel-Porter, R. (2021). Beyond the promise: implementing ethical AI. AI Ethics 1, 73–80. doi: 10.1007/s43681-020-00011-6 CrossRef Full Text | Google Scholar Ernst, E., Merola, R., and Samaan, D. (2019). Economics of artificial intelligence: Implications for the future of work. IZA J. Labor Policy 9. Google Scholar Fabrigar, L. R., and Wegener, D. T. (2011). Exploratory factor analysis Oxford University Press. Google Scholar Fast, E., and Horvitz, E. (2017). “Long-term trends in the public perception of artificial intelligence” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31. San Francisco, California, USA. Google Scholar Fetzer, J. H., and Fetzer, J. H. (1990). What is artificial intelligence? Springer Netherlands. Google Scholar Frey, C. B., and Osborne, M. A. (2017). The future of employment: how susceptible are jobs to computerisation? Technol. Forecast. Soc. Chang. 114, 254–280. doi: 10.1016/j.techfore.2016.08.019 CrossRef Full Text | Google Scholar Gibbs, S. (2017). Elon Musk leads 116 experts calling for outright ban of killer robots. The Guardian 20. Google Scholar Glas, D. F., Minato, T., Ishi, C. T., Kawahara, T., and Ishiguro, H. (2016). “Erica: the erato intelligent conversational android” in 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN) (New York, NY, USA: IEEE), 22–29. Google Scholar Harari, Y. N. (2017). Reboot for the AI revolution. Nature 550, 324–327. doi: 10.1038/550324a PubMed Abstract | CrossRef Full Text | Google Scholar Hentout, A., Aouache, M., Maoudj, A., and Akli, I. (2019). Human–robot interaction in industrial collaborative robotics: a literature review of the decade 2008–2017. Adv. Robot. 33, 764–799. doi: 10.1080/01691864.2019.1636714 CrossRef Full Text | Google Scholar Hong, J. W., Cruz, I., and Williams, D. (2021). AI, you can drive my car: how we evaluate human drivers vs. self-driving cars. Comput. Hum. Behav. 125:106944. doi: 10.1016/j.chb.2021.106944 CrossRef Full Text | Google Scholar Johnson, K. B., Wei, W. Q., Weeraratne, D., Frisse, M. E., Misulis, K., Rhee, K., et al. (2021). Precision medicine, AI, and the future of personalized health care. Clin. Transl. Sci. 14, 86–93. doi: 10.1111/cts.12884 PubMed Abstract | CrossRef Full Text | Google Scholar Jones, C. I. (2022). The past and future of economic growth: a semi-endogenous perspective. Annu. Rev. Econ. 14, 125–152. doi: 10.1146/annurev-economics-080521-012458 CrossRef Full Text | Google Scholar Kaur, D., Uslu, S., Rittichier, K. J., and Durresi, A. (2022). Trustworthy artificial intelligence: a review. ACM Comput. Surv. 55, 1–38. doi: 10.1145/3491209 CrossRef Full Text | Google Scholar Kieslich, K., Lünich, M., and Marcinkowski, F. (2021). The threats of artificial intelligence scale (TAI) development, measurement and test over three application domains. Int. J. Soc. Robot. 13, 1563–1577. doi: 10.1007/s12369-020-00734-w CrossRef Full Text | Google Scholar Kumar, S., and Choudhury, S. (2022). Humans, super humans, and super humanoids: debating Stephen Hawking’s doomsday AI forecast. AI Ethics, 1–10. doi: 10.1007/s43681-022-00213-0 CrossRef Full Text | Google Scholar La Torre, G., De Leonardis, V., and Chiappetta, M. (2020). Technostress: how does it affect the productivity and life of an individual? Results of an observational study. Public Health 189, 60–65. doi: 10.1016/j.puhe.2020.09.013 PubMed Abstract | CrossRef Full Text | Google Scholar La Torre, G., Esposito, A., Sciarra, I., and Chiappetta, M. (2019). Definition, symptoms and risk of techno-stress: a systematic review. Int. Arch. Occup. Environ. Health 92, 13–35. doi: 10.1007/s00420-018-1352-1 PubMed Abstract | CrossRef Full Text | Google Scholar Leiter, C., Zhang, R., Chen, Y., Belouadi, J., Larionov, D., Fresen, V., et al. (2023). Chatgpt: A meta-analysis after 2.5 months. arXiv. arXiv:2302-13795. Google Scholar Logg, J. M., Minson, J. A., and Moore, D. A. (2019). Algorithm appreciation: people prefer algorithmic to human judgment. Organ. Behav. Hum. Decis. Process. 151, 90–103. doi: 10.1016/j.obhdp.2018.12.005 CrossRef Full Text | Google Scholar Loh, H. W., Ooi, C. P., Seoni, S., Barua, P. D., Molinari, F., and Acharya, U. R. (2022). Application of explainable artificial intelligence for healthcare: a systematic review of the last decade (2011–2022). Comput. Methods Prog. Biomed. 226:107161. doi: 10.1016/j.cmpb.2022.107161 CrossRef Full Text | Google Scholar MacCallum, R. C., Browne, M. W., and Sugawara, H. M. (1996). Power analysis and determination of sample size for covariance structure modeling. Psychol. Methods 1, 130–149. doi: 10.1037/1082-989X.1.2.130 CrossRef Full Text | Google Scholar Makridakis, S. (2017). The forthcoming artificial intelligence (AI) revolution: its impact on society and firms. Futures 90, 46–60. doi: 10.1016/j.futures.2017.03.006 CrossRef Full Text | Google Scholar Manoharan, S. (2019). An improved safety algorithm for artificial intelligence enabled processors in self driving cars. J. Artif. Intell. 1, 95–104. doi: 10.36548/jaicn.2019.2.005 CrossRef Full Text | Google Scholar Matytsin, D. E., Dzedik, V. A., Makeeva, G. A., and Boldyreva, S. B. (2023). “Smart” outsourcing in support of the humanization of entrepreneurship in the artificial intelligence economy. Humanit. Soc. Sci. Commun. 10, 1–8. doi: 10.1057/s41599-022-01493-x CrossRef Full Text | Google Scholar Middleton, S. E., Letouzé, E., Hossaini, A., and Chapman, A. (2022). Trust, regulation, and human-in-the-loop AI: within the European region. Commun. ACM 65, 64–68. doi: 10.1145/3511597 CrossRef Full Text | Google Scholar Morley, J., Machado, C. C., Burr, C., Cowls, J., Joshi, I., Taddeo, M., et al. (2020). The ethics of AI in health care: a mapping review. Soc. Sci. Med. 260:113172. doi: 10.1016/j.socscimed.2020.113172 PubMed Abstract | CrossRef Full Text | Google Scholar Nadimpalli, M. (2017). Artificial intelligence risks and benefits. Int. J. Innov. Res. Sci. Eng. Technol. 6 Google Scholar O’Shaughnessy, M. R., Schiff, D. S., Varshney, L. R., Rozell, C. J., and Davenport, M. A. (2022). What governs attitudes toward artificial intelligence adoption and governance? Sci. Public Policy 50, 161–176. doi: 10.1093/scipol/scac056 CrossRef Full Text | Google Scholar Ong, C. S., and Lai, J. Y. (2006). Gender differences in perceptions and relationships among dominants of e-learning acceptance. Comput. Hum. Behav. 22, 816–829. doi: 10.1016/j.chb.2004.03.006 CrossRef Full Text | Google Scholar Parasuraman, A., and Colby, C. L. (2015). An updated and streamlined technology readiness index: TRI 2.0. J. Serv. Res. 18, 59–74. doi: 10.1177/1094670514539730 CrossRef Full Text | Google Scholar Pause Giant AI Experiments: An Open Letter. (2023). Available at: https://futureoflife.org/openletter/pause-giant-ai-experiments/ Google Scholar Peer, E., Brandimarte, L., Samat, S., and Acquisti, A. (2017). Beyond the Turk: alternative platforms for crowdsourcing behavioral research. J. Exp. Soc. Psychol. 70, 153–163. doi: 10.1016/j.jesp.2017.01.006 CrossRef Full Text | Google Scholar Persson, A., Laaksoharju, M., and Koga, H. (2021). We mostly think alike: individual differences in attitude towards AI in Sweden and Japan. Rev. Socionetw. Strat. 15, 123–142. doi: 10.1007/s12626-021-00071-y CrossRef Full Text | Google Scholar Podsakoff, P. M., MacKenzie, S. B., Lee, J. Y., and Podsakoff, N. P. (2003). Common method biases in behavioral research: a critical review of the literature and recommended remedies. J. Appl. Psychol. 88, 879–903. doi: 10.1037/0021-9010.88.5.879 PubMed Abstract | CrossRef Full Text | Google Scholar Preston, C. C., and Colman, A. M. (2000). Optimal number of response categories in rating scales: reliability, validity, discriminating power, and respondent preferences. Acta Psychol. 104, 1–15. doi: 10.1016/S0001-6918(99)00050-5 PubMed Abstract | CrossRef Full Text | Google Scholar Rashid, T., and Asghar, H. M. (2016). Technology use, self-directed learning, student engagement and academic performance: examining the interrelations. Comput. Hum. Behav. 63, 604–612. doi: 10.1016/j.chb.2016.05.084 CrossRef Full Text | Google Scholar Riedl, R. (2012). On the biology of technostress: literature review and research agenda. ACM SIGMIS Database 44, 18–55. doi: 10.1145/2436239.2436242 CrossRef Full Text | Google Scholar Rosen, L. D., Whaling, K., Carrier, L. M., Cheever, N. A., and Rokkum, J. (2013). The media and technology usage and attitudes scale: an empirical investigation. Comput. Hum. Behav. 29, 2501–2511. doi: 10.1016/j.chb.2013.06.006 PubMed Abstract | CrossRef Full Text | Google Scholar Rosseel, Y. (2012). Lavaan: an R package for structural equation modeling. J. Stat. Softw. 48, 1–36. doi: 10.18637/jss.v048.i02 CrossRef Full Text | Google Scholar Samuel, J. (2021). A quick-draft response to the march 2023 “pause Giant AI experiments: an open letter” by Yoshua Bengio, signed by Stuart Russell, Elon musk, Steve Wozniak, Yuval Noah Harari and others. Available at: https://ssrn.com/abstract=4412516 Google Scholar Samuel, J. (2023). Two keys for surviving the inevitable AI invasion. Available at: https://aboveai.substack.com/p/two-keys-for-surviving-the-inevitable Google Scholar Sandoval, E. B., Mubin, O., and Obaid, M. (2014). Human robot interaction and fiction: a contradiction. in Proceedings of Social Robotics: 6th International Conference, ICSR 2014, Sydney, NSW, October 27–29, 2014. 6. New York, NY, USA: Springer International Publishing, 54–63. Google Scholar Schaefer, K. E., Chen, J. Y., Szalma, J. L., and Hancock, P. A. (2016). A meta-analysis of factors influencing the development of trust in automation: implications for understanding autonomy in future systems. Hum. Factors 58, 377–400. doi: 10.1177/0018720816634228 PubMed Abstract | CrossRef Full Text | Google Scholar Schepman, A., and Rodway, P. (2020). Initial validation of the general attitudes towards Artificial Intelligence Scale. Comp. Hum. Behav. Rep. 1:100014. Google Scholar Schumacher, P., and Morahan-Martin, J. (2001). Gender, internet and computer attitudes and experiences. Comput. Hum. Behav. 17, 95–110. doi: 10.1016/S0747-5632(00)00032-7 CrossRef Full Text | Google Scholar Scott, I. A., Carter, S. M., and Coiera, E. (2021). Exploring stakeholder attitudes towards AI in clinical practice. BMJ Health Care Informatics 28. doi: 10.1136/bmjhci-2021-100450 CrossRef Full Text | Google Scholar Sheridan, T. B. (2019). Individual differences in attributes of trust in automation: measurement and application to system design. Front. Psychol. 10:1117. doi: 10.3389/fpsyg.2019.01117 PubMed Abstract | CrossRef Full Text | Google Scholar Shrout, P. E., and Fleiss, J. L. (1979). Intraclass correlations: uses in assessing rater reliability. Psychol. Bull. 86, 420–428. doi: 10.1037/0033-2909.86.2.420 CrossRef Full Text | Google Scholar Sigerson, L., and Cheng, C. (2018). Scales for measuring user engagement with social network sites: a systematic review of psychometric properties. Comput. Hum. Behav. 83, 87–105. doi: 10.1016/j.chb.2018.01.023 CrossRef Full Text | Google Scholar Sindermann, C., Sha, P., Zhou, M., Wernicke, J., Schmitt, H. S., Li, M., et al. (2021). Assessing the attitude towards artificial intelligence: introduction of a short measure in German, Chinese, and English language. KI-Künstliche Intelligenz 35, 109–118. doi: 10.1007/s13218-020-00689-0 CrossRef Full Text | Google Scholar Slovic, P. (1987). Perception of risk. Science 236, 280–285. doi: 10.1126/science.3563507 CrossRef Full Text | Google Scholar Sohn, K., and Kwon, O. (2020). Technology acceptance theories and factors influencing artificial intelligence-based intelligent products. Telematics Inform. 47:101324. doi: 10.1016/j.tele.2019.101324 CrossRef Full Text | Google Scholar Srivastava, T., Shen, A. K., Browne, S., Michel, J. J., Tan, A. S., and Kornides, M. L. (2022). Comparing COVID-19 vaccination outcomes with parental values, beliefs, attitudes, and hesitancy status, 2021–2022. Vaccine 10:1632. doi: 10.3390/vaccines10101632 PubMed Abstract | CrossRef Full Text | Google Scholar Stoet, G. (2010). PsyToolkit - a software package for programming psychological experiments using Linux. Behav. Res. Methods 42, 1096–1104. doi: 10.3758/BRM.42.4.1096 PubMed Abstract | CrossRef Full Text | Google Scholar Stoet, G. (2017). PsyToolkit: a novel web-based method for running online questionnaires and reaction-time experiments. Teach. Psychol. 44, 24–31. doi: 10.1177/0098628316677643 CrossRef Full Text | Google Scholar Sun, J. (2005). Assessing goodness of fit in confirmatory factor analysis. Meas. Eval. Couns. Dev. 37, 240–256. doi: 10.1080/07481756.2005.11909764 CrossRef Full Text | Google Scholar The Jamovi Project (2022). Jamovi (version 2.3) [computer software]. Available at: https://www.jamovi.org (Accessed March 20, 2023). Google Scholar Tschang, F. T., and Almirall, E. (2021). Artificial intelligence as augmenting automation: implications for employment. Acad. Manag. Perspect. 35, 642–659. doi: 10.5465/amp.2019.0062 CrossRef Full Text | Google Scholar Vasiljeva, T., Kreituss, I., and Lulle, I. (2021). Artificial intelligence: the attitude of the public and representatives of various industries. J. Risk Finan. Manag. 14:339. doi: 10.3390/jrfm14080339 CrossRef Full Text | Google Scholar Vayena, E., Blasimme, A., and Cohen, I. G. (2018). Machine learning in medicine: addressing ethical challenges. PLoS Med. 15:e1002689. doi: 10.1371/journal.pmed.1002689 PubMed Abstract | CrossRef Full Text | Google Scholar Venkatesh, V. (2022). Adoption and use of AI tools: a research agenda grounded in UTAUT. Annals Oper. Res. 1–12. Google Scholar Venkatesh, V., Morris, M. G., Davis, G. B., and Davis, F. D. (2003). User acceptance of information technology: toward a unified view. MIS Q. 27, 425–478. doi: 10.2307/30036540 CrossRef Full Text | Google Scholar Vuong, Q. H. (2023). Mindsponge theory. Berlin, Germany: De Gruyter. Google Scholar Vuong, Q. H., La, V. P., Nguyen, M. H., Jin, R., La, M. K., and Le, T. T. (2023). How AI’s self-prolongation influences people’s perceptions of its autonomous mind: the case of US residents. Behav. Sci. 13:470. doi: 10.3390/bs13060470 CrossRef Full Text | Google Scholar Wijayati, D. T., Rahman, Z., Rahman, M. F. W., Arifah, I. D. C., and Kautsar, A. (2022). A study of artificial intelligence on employee performance and work engagement: the moderating role of change leadership. Int. J. Manpow. 43, 486–512. doi: 10.1108/IJM-07-2021-0423 CrossRef Full Text | Google Scholar Williams, B., Onsman, A., and Brown, T. (2010). Exploratory factor analysis: a five-step guide for novices. Aust. J. Paramed. 8, 1–13. doi: 10.33151/ajp.8.3.93 CrossRef Full Text | Google Scholar Wilson, H. J., Daugherty, P., and Bianzino, N. (2017). The jobs that artificial intelligence will create. MIT Sloan Manag. Rev. 58:14. Google Scholar Xia, Q., Chiu, T. K., Lee, M., Sanusi, I. T., Dai, Y., and Chai, C. S. (2022). A self-determination theory (SDT) design approach for inclusive and diverse artificial intelligence (AI) education. Comput. Educ. 189:104582. doi: 10.1016/j.compedu.2022.104582 CrossRef Full Text | Google Scholar Yang, W. (2022). Artificial intelligence education for young children: why, what, and how in curriculum design and implementation. Comput. Educ. 3:100061. doi: 10.1016/j.caeai.2022.100061 CrossRef Full Text | Google Scholar Young, A. T., Amara, D., Bhattacharya, A., and Wei, M. L. (2021). Patient and general public attitudes towards clinical artificial intelligence: a mixed methods systematic review. Lancet Digit. Health 3, e599–e611. doi: 10.1016/S2589-7500(21)00132-1 PubMed Abstract | CrossRef Full Text | Google Scholar Zhang, B., and Dafoe, A. (2019). Artificial intelligence: American attitudes and trends. Available at SSRN 3312874. Google Scholar Ziefle, M., and Wilkowska, W. (2010). “Technology acceptability for medical assistance,” in 2010 4th International Conference on Pervasive Computing Technologies for Healthcare (New York, NY, USA: IEEE), 1–9. Google Scholar Keywords: artificial intelligence, questionnaire, factor analysis, psychology, human-computer interaction (HCI) Citation: Grassini S (2023) Development and validation of the AI attitude scale (AIAS-4): a brief measure of general attitude toward artificial intelligence. Front. Psychol. 14:1191628. doi: 10.3389/fpsyg.2023.1191628 Received: 22 March 2023; Accepted: 16 June 2023; Published: 24 July 2023. Edited by: Runxi Zeng, Chongqing University, China Reviewed by: Verena Nitsch, RWTH Aachen University, Germany Minh-Hoang Nguyen, Phenikaa University, Vietnam Copyright © 2023 Grassini. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Simone Grassini, simone.grassini@uib.no Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Footer Guidelines Author guidelinesEditor guidelinesPolicies and publication ethicsFee policy Explore ArticlesResearch Topics JournalsHow we publish Outreach Frontiers Forum Frontiers Policy Labs Frontiers for Young MindsFrontiers Planet Prize Connect Help centerEmails and alerts Contact us SubmitCareer opportunities Follow us © 2024 Frontiers Media S.A. All rights reserved Privacy policy | Terms and conditions X (1) Mendeley (142) See more details"
Our attitudes towards AI reveal how we really feel about human intelligence | Blaise Agüera y Arcas | The Guardian,https://www.theguardian.com/technology/article/2024/jul/03/ai-human-intelligence,未知,2024-11-21 17:10:07,en,AI AND attitude,google,"Our attitudes towards AI reveal how we really feel about human intelligence | Blaise Agüera y Arcas | The Guardian <img src=""https://sb.scorecardresearch.com/p?c1=2&c2=6035250&cv=2.0&cj=1&cs_ucfr=0&comscorekw=Artificial+intelligence+%28AI%29%2CTechnology%2CGoogle%2CSociety%2CWork+%26+careers"" /> Skip to main contentSkip to navigationClose dialogue1/1Next imagePrevious imageToggle captionSkip to navigationPrint subscriptionsSearch jobs Sign inIntInternational editionUK editionUS editionAustralia editionEurope editionThe Guardian - Back to homeThe GuardianNewsOpinionSportCultureLifestyleShow moreHide expanded menuNewsView all NewsWorld newsUS politicsUK newsClimate crisisMiddle EastUkraineEnvironmentScienceGlobal developmentFootballTechBusinessObituariesOpinionView all OpinionThe Guardian viewColumnistsCartoonsOpinion videosLettersSportView all SportFootballCricketRugby unionTennisCyclingF1GolfUS sportsCultureView all CultureBooksMusicTV & radioArt & designFilmGamesClassicalStageLifestyleView all LifestyleFashionFoodRecipesLove & sexHealth & fitnessHome & gardenWomenMenFamilyTravelMoneySearch input google-search SearchSupport usPrint subscriptionsSearch jobsHolidaysDigital ArchiveGuardian LicensingAbout UsThe Guardian appVideoPodcastsPicturesNewslettersToday's paperInside the GuardianThe ObserverGuardian WeeklyCrosswordsWordiplyCorrectionsSearch input google-search SearchSearch jobsHolidaysDigital ArchiveGuardian LicensingAbout UsMoneyPropertyPensionsSavingsBorrowingCareers ‘Labor is not a zero-sum game, and robots aren’t an “other” that competes with us.’ Photograph: Kilito Chan/Getty ImagesView image in fullscreen‘Labor is not a zero-sum game, and robots aren’t an “other” that competes with us.’ Photograph: Kilito Chan/Getty ImagesArtificial intelligence (AI) This article is more than 4 months oldOur attitudes towards AI reveal how we really feel about human intelligenceThis article is more than 4 months oldBlaise Agüera y ArcasWe’re in the untenable position of regarding the AI as alien because we’re already in the position of alienating each otherWed 3 Jul 2024 16.00 BSTShareThe idea that superintelligent robots are alien invaders coming to “steal our jobs” reveals profound shortcomings in the way we think about work, value, and intelligence itself. Labor is not a zero-sum game, and robots aren’t an “other” that competes with us. Like any technology, they’re part of us, growing out of civilization the same way hair and nails grow out of a living body. They’re part of humanity – and we’re partly machine.When we “other” a fruit-picking robot – thinking of it as a competitor in a zero-sum game – we take our eyes off the real problem: the human who used to pick the fruit is considered disposable by the farm’s owners and by society when no longer fit for that job. This implies that the human laborer was already being treated like a non-person – that is, like a machine. We’re in the untenable position of regarding the machine as alien because we are already in the untenable position of alienating each other.Many of our anxieties about artificial intelligence are rooted in that ancient, often regrettable part of our heritage that emphasizes dominance and hierarchy. However, the larger story of evolution is one in which cooperation allows simpler entities to join forces, creating larger, more complex, and more enduring ones; that’s how eukaryotic cells evolved out of prokaryotes, how multicellular animals evolved out of single cells, and how human culture evolved out of groups of humans, domesticated animals, and crops. Mutualism is what has allowed us to scale.Many of our anxieties about AI are rooted in that ancient, often regrettable part of our heritage that emphasizes dominance and hierarchyAs an AI researcher, my chief interest is not so much in computers – the “artificial” in AI – as in intelligence itself. And it has become clear that, no matter how it is embodied, intelligence requires scale. The “Language Model for Dialogue Applications” or “LaMDA”, an early large language model we built internally at Google Research, convinced me in 2021 that we had crossed an important threshold. While it was still very hit-or-miss, LaMDA, with its (for the time) whopping 137bn parameters, could almost hold down a conversation. Three years later, state-of-the-art models have grown by an order of magnitude, and accordingly, they have gotten a lot better. In another few years, we’ll likely see models with as many parameters as there are synapses in the human brain.As a species, modern human beings are likewise the result of an explosion in brain size. Over the past several million years, our hominin ancestors’ skulls quadrupled in volume. Social group size has grown in lockstep as researchers find when they correlate primate troop size with brain volume. Bigger brains allow larger groups to cooperate effectively. Larger groups are, in turn, more intelligent.What we think of as “human intelligence” is a collective phenomenon arising from cooperation among many individually narrower intelligences, like you and me. When we catalog our intellectual achievements – antibiotics and indoor plumbing, art and architecture, higher mathematics and hot fudge sundaes – let’s acknowledge how clueless most of us are, individually. Could you make a sundae, even if you began with domesticated cows, cacao pods, vanilla beans, sugar cane and refrigeration – that is, with 99% of the hard work already done?Human intelligence consists not only of people, but also of an array of plant and animal species, microbes, and even technologies from paleolithic to contemporary. Those cows and cacao plants, the rice and wheat, the ships, trucks and railroads that have supported explosive population growth are all fundamental. To neglect the existence of all these companion species and technologies is akin to imagining us as a disembodied brain in a vat.Further, our intelligence is variously embodied and distributed. It will become even more so as AI systems proliferate, making it increasingly hard to pretend that our achievements are individual or even solely human. Perhaps we should adopt a broader definition of “human”, to include this entire bio-technological package.Some of our most impressive feats, like making silicon chips, are truly global in scale. Our challenges, too, are increasingly global. Threats like the climate crisis and the resurgent possibility of nuclear war weren’t created by any one actor, but by all of us, and we can only solve them collectively. The increasing depth and breadth of collective intelligence is a good thing if we want to flourish at planetary scale, but that growth isn’t often perceived as something cumulative and mutual. Why?How’s this for a bombshell – the US must make AI its next Manhattan Project | John NaughtonRead morePut simply, because we’re worried about who will be on top. But dominance hierarchies are nothing more than a particular trick for allowing troops of cooperating animals with otherwise aggressive tendencies toward each other, borne of internal competition for mates and food, to avoid constant squabbling by agreeing on who would win, were a fight over priority to break out. Such hierarchies may be, in other words, just a hack for half-clever monkeys, not some universal law of nature.AI models can embody considerable intelligence, just as human brains can, but they aren’t fellow apes vying for status. As a product of high human technology, they depend on people, wheat, cows, and human culture in general to an even greater extent than Homo sapiens do. They aren’t conniving to eat our food or steal our romantic partners. They depend on us; we may come to depend on them just as deeply. Yet concern about dominance hierarchy has shadowed the development of AI from the start.The very term “robot”, introduced by Karel Çapek in his 1920 play Rossum’s Universal Robots, comes from the Czech word for forced labor, robota. Nearly a century later, a highly-regarded AI ethicist titled an article Robots should be slaves, and though she later regretted her choice of words, the robot debate still turns on domination. AI doomers are now concerned that humans will be enslaved or exterminated by superintelligent robots. On the other hand, AI deniers believe that computers are incapable by definition of any agency, but are instead mere tools humans use to dominate each other. Both perspectives are rooted in zero-sum, us-versus-them thinking.Many labs today are developing AI agents. They will become commonplace in the coming years, not because the robots are “taking over”, but because a cooperating agent can be a lot more helpful, both to individual humans and to human society, than a mindless robota.If there is any threat to our social order here, it comes not from robots but from inequalities among human beings. Too many of us haven’t understood yet that we’re interdependent. We’re all in it together – human, animal, plant, and machine alike.Explore more on these topicsArtificial intelligence (AI)GoogleWork & careerscommentShareReuse this contentMost viewedMost viewedMoneyPropertyPensionsSavingsBorrowingCareersNewsOpinionSportCultureLifestyleOriginal reporting and incisive analysis, direct from the Guardian every morningSign up for our emailHelpComplaints & correctionsSecureDropWork for us Privacy policyCookie policyTerms & conditionsContact usAll topicsAll writersDigital newspaper archiveTax strategyFacebookYouTubeInstagramLinkedInNewslettersAdvertise with usSearch UK jobsBack to top© 2024 Guardian News & Media Limited or its affiliated companies. All rights reserved. (dcr)"
Attitudes towards AI: measurement and associations with personality | Scientific Reports,https://www.nature.com/articles/s41598-024-53335-2,未知,2024-11-21 17:10:19,en,AI AND attitude,google,"Attitudes towards AI: measurement and associations with personality | Scientific Reports <link rel=""stylesheet"" type=""text/css"" href=""/static/css/enhanced-article-912e265451.css"" media=""only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)""> <iframe src=""https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"" height=""0"" width=""0"" style=""display:none;visibility:hidden""></iframe> Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement <a href=""//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=-1183037481&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-024-53335-2%26doi%3D10.1038/s41598-024-53335-2%26subjmeta%3D258,2811,477,631,639,705%26kwrd%3DHuman+behaviour,Information+technology,Psychology""> <img data-test=""gpt-advert-fallback-img"" src=""//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=-1183037481&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-024-53335-2%26doi%3D10.1038/s41598-024-53335-2%26subjmeta%3D258,2811,477,631,639,705%26kwrd%3DHuman+behaviour,Information+technology,Psychology"" alt=""Advertisement"" width=""728"" height=""90""></a> View all journals Search Log in Explore content About the journal Publish with us Sign up for alerts RSS feed nature scientific reports articles article Attitudes towards AI: measurement and associations with personality Download PDF Download PDF Article Open access Published: 05 February 2024 Attitudes towards AI: measurement and associations with personality Jan-Philipp Stein ORCID: orcid.org/0000-0003-3874-02771, Tanja Messingschlager ORCID: orcid.org/0000-0001-6737-89972, Timo Gnambs ORCID: orcid.org/0000-0002-6984-12763, Fabian Hutmacher ORCID: orcid.org/0000-0002-0018-25592 & …Markus Appel ORCID: orcid.org/0000-0003-4111-13082 Show authors Scientific Reports volume 14, Article number: 2909 (2024) Cite this article 22k Accesses 9 Citations 8 Altmetric Metrics details Subjects Human behaviourInformation technologyPsychology AbstractArtificial intelligence (AI) has become an integral part of many contemporary technologies, such as social media platforms, smart devices, and global logistics systems. At the same time, research on the public acceptance of AI shows that many people feel quite apprehensive about the potential of such technologies—an observation that has been connected to both demographic and sociocultural user variables (e.g., age, previous media exposure). Yet, due to divergent and often ad-hoc measurements of AI-related attitudes, the current body of evidence remains inconclusive. Likewise, it is still unclear if attitudes towards AI are also affected by users’ personality traits. In response to these research gaps, we offer a two-fold contribution. First, we present a novel, psychologically informed questionnaire (ATTARI-12) that captures attitudes towards AI as a single construct, independent of specific contexts or applications. Having observed good reliability and validity for our new measure across two studies (N1 = 490; N2 = 150), we examine several personality traits—the Big Five, the Dark Triad, and conspiracy mentality—as potential predictors of AI-related attitudes in a third study (N3 = 298). We find that agreeableness and younger age predict a more positive view towards artificially intelligent technology, whereas the susceptibility to conspiracy beliefs connects to a more negative attitude. Our findings are discussed considering potential limitations and future directions for research and practice. Similar content being viewed by others Psychological factors underlying attitudes toward AI tools Article 20 November 2023 When combinations of humans and AI are useful: A systematic review and meta-analysis Article Open access 28 October 2024 Human intelligence can safeguard against artificial intelligence: individual differences in the discernment of human from AI texts Article Open access 29 October 2024 IntroductionArtificial intelligence (AI) promises to make human life much more comfortable: It may foster intercultural collaboration via sophisticated translating tools, guide online customers towards the products they are most likely going to buy, or carry out jobs that feel tedious to human workers1,2. At the same time, the proliferation of AI-based technologies is met with serious reservations. It is argued, for instance, that AI could lead to the downsizing of human jobs3,4, the creation of new intelligent weaponry5,6, or a growing lack of control over the emerging technologies7. Similarly, the recent and much-publicized presentation of new text- and image-creating AI (such as ChatGPT and Midjourney) has raised concerns about artistic license, academic fraud, and the value of human creativity8.Crucially, individuals may differ regarding their evaluation of such chances and risks, and in turn, hold different attitudes towards AI. However, we note that scholars interested in these differences often used a basic ad-hoc approach to assess participants’ AI attitudes6,9, or focused only on highly domain-specific sub-types of AI10,11. Against this background, the current paper offers a twofold contribution. First, we present a novel questionnaire on AI-related attitudes—the ATTARI-12, which incorporates the psychological trichotomy of cognition, emotion, and behavior to facilitate a comprehensive yet economic measurement—and scrutinize its validity across two studies. Subsequently, we connect participants’ attitudes (as measured by our new scale) to several fundamental personality traits, namely the Big Five, the Dark Triad, and conspiracy mentality.Research objective 1: measuring attitudes towards AIBroadly speaking, the term artificial intelligence (AI) is used to describe technologies that can execute tasks one may expect to require human intelligence—i.e., technologies that possess a certain degree of autonomy, the capacity for learning and adapting, and the ability to handle large amounts of data12,13. As AI becomes deeply embedded in many technical systems and, thus, people’s daily lives, it emerges as an important task for numerous scientific disciplines (e.g. psychology, communication science, computer science, philosophy) to better understand users’ response to—and acceptance of—artificially intelligent technology. Of course, in order to achieve this goal, it is essential to employ theoretically sound measures of high psychometric quality when assessing AI-related attitudes. Furthermore, researchers need to decide whether they want to examine only a specific type of application (such as self-driving cars or intelligent robots) or focus on AI as an abstract technological concept that can be applied to many different settings. While both approaches have undeniable merit, it may be argued that the comparability of different research efforts—especially those situated at the intersection of different disciplines—clearly benefits from the latter, i.e., an empirical focus on attitudes towards AI as a set of technological capabilities instead of concrete use cases.In line with this argument, scientific efforts have produced several measures to assess attitudes towards AI as a more overarching concept. These include the General Attitudes Towards Artificial Intelligence Scale (GAAIS13), the Attitudes Towards Artificial Intelligence Scale (ATAI14), the AI Anxiety Scale (AIAS15), the Threats of Artificial Intelligence Scale (TAI16), and the Concerns About Autonomous Technology questionnaire17. Upon thorough examination, however, we believe that none of these measures offer an entirely satisfactory option for at least one of five reasons. First, many of the above-mentioned measures only inquire participants about negative impressions and concerns regarding AI, leaving aside the possibility of distinctly positive attitudes15,16,17. Second, the measured attitudes towards AI are often subdivided into several factors13,14,15, which complicates handling and interpreting these scales from both a theoretical and a practical perspective. For instance, the GAAIS13 presents two sub-scales called “acceptance” and “fear,” even though it can be argued that these merely cover different poles of the same spectrum. Relatedly, the AIAS scale15 offers a four-factor solution with the (statistically induced) dimensions “learning,” “job replacement,” “sociotechnical blindness,” and “AI configuration”—clusters whose practical merit may be related to more specific use cases. Third, while some scales have a relatively low number of items (e.g., five items14), other scales are comparably long (e.g., 21 items15), which may complicate their use in research settings that need to include attitudes towards AI as one concept among multiple others. Fourth, for one of the reviewed scales14, the low number of items resulted in a notable lack of reliability. Fifth, none of the available scales acknowledge the cognitive, affective, and behavioral facets of attitudes in their designs18,19.Hence, based on the identified lack of a one-dimensional, psychometrically sound, yet economic measure that captures both positive and negative aspects of the attitude towards AI, our first aim was to develop a scale that overcomes these limitations. For the sake of broad interdisciplinary applicability, we explicitly focused the creation of our scale on the perception of AI as a general set of technological capabilities, independent of its physical embodiment or use context. This perspective was not least informed by recent research, which indicated that people’s evaluation of digital ‘minds’ is likely to change once visual factors come into play20—further highlighting the merit of addressing AI as a disembodied concept.Research objective 2: understanding associations between AI-related attitudes and personality traitsThus far, most studies on public AI acceptance have explored the role of demographic and sociocultural variables, revealing that concerns about AI seem to be more prevalent among women, the elderly, ethnic minorities, and individuals with lower levels of education6,21. Furthermore, it was demonstrated that egalitarian values and distrust in science9, attachment anxiety22, and the exposure to dystopian science fiction6,23 constitute meaningful predictors of negative attitudes towards AI. Indeed, many of these findings are echoed by a large body of human–robot interaction literature, which also emphasizes the crucial impact of demographic variables, cultural norms, and media use on the public acceptance of robotic machinery24,25,26. Yet, considering that studies from this discipline rarely elucidate whether the reported effects arise from the perception of robotic bodies or robotic minds (i.e., AI), we suggest that they should only be considered as secondary evidence for the current topic.Apart from considerable insight into the sociocultural predictors of (dis-)liking AI technology, much less is known about the impact of users’ personality factors on their AI-related attitudes. To our knowledge, only a handful of scientific studies have actually addressed this issue, and most of them have yielded rather limited results—either by focusing on very specific types of AI10,11,27 or by measuring reactions to physically embodied AI (e.g., robots), which makes it much harder to pinpoint the cause of the effect28,29. While another recent study indeed explored attitudes towards AI as a technical concept regardless of its specific use or embodiment30, this effort focused exclusively on negative attitudes, thus leaving out a substantial part (i.e., the positive side) of how users think and feel about artificially intelligent systems.Therefore, building upon the reviewed work, our second objective was to scrutinize several central personality dimensions as predictors for people’s AI-related attitudes, including the well-known Big Five31 and Dark Triad of personality32. In addition to that, we include the rather novel construct of conspiracy mentality in our work33, considering its high contemporary relevance in an increasingly digital world34.Overview of studies and predictionsAddressing the outlined research propositions, we present three studies (see Table 1 for a brief overview of main study characteristics). All materials, obtained data, and analysis codes for these three studies can be found in our project’s Open Science Framework repository (https://osf.io/3j67a/). Informed consent was collected from all participants before they took part in our research efforts.Table 1 Overview of the conducted studies.Full size tableStudies 1 and 2 mainly served to investigate the reliability and validity of our new attitudes towards AI scale, the ATTARI-12. Hence, these studies were mainly guided by several analytical steps examining the statistical properties of our measure, including its convergent validity, re-test reliability, and susceptibility to social desirability bias. In Study 3, we then set out to connect participants’ attitude towards AI (as measured by the developed questionnaire) to fundamental personality traits and demographic factors. For this third and final study, we will detail all theoretical considerations and hypotheses in the following.Faced with a lack of previous findings regarding personality predictors of AI-related attitudes, we deemed it important to anchor our investigation in a broader view at human personality. This perspective led us to first include the Big Five model31, which has remained the most widely used taxonomy of human personality for several decades. As the name suggests, the Big Five consist of five fundamental personality dimensions—openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism—which are presumed to cover a substantial portion of the dispositional variance between individuals35,36. Adhering to this notion, we developed a first set of hypotheses about the individual contribution of each Big Five trait to people’s AI-related attitudes.For openness to experience—which can be defined as the tendency to be adventurous, intellectually curious, and imaginative—we expected a clear positive relationship to AI-related attitudes. Since AI technologies promise many new possibilities for human society, people who are open to experience should feel more excited and curious about the respective innovations (this is mirrored by recent findings on self-driving cars11). We hypothesize: H1 The higher a person’s openness for experience, the more positive attitudes they hold towards AI. Conscientiousness can be described as the inclination to be diligent, efficient, and careful, and to act in a disciplined or even perfectionist manner. Considering that it may be difficult for humans to understand the inner workings of an AI system or to anticipate its behavior, it seems likely that conscientious people would express more negative views about AI, a technological concept that might make the world less comprehensible for them29. H2 The higher a person’s conscientiousness, the more negative attitudes they hold towards AI. The third of the Big Five traits, extraversion, encompasses the tendency to be out-going, talkative, and gregarious; in turn, it is negatively related to apprehensive and restrained behavior. Based on this definition, it comes as no surprise that extroverted individuals reported fewer concerns about autonomous technologies in previous studies29,30,37. Thus, we hypothesize: H3 The higher people’s extraversion, the more positive attitudes they hold towards AI. High agreeableness manifests itself in the proclivity to show warm, cooperative, and kind-hearted behavior. Regarding the topic at hand, previous research suggests that agreeableness might be (moderately) correlated with positive views about automation or specific types of AI11,27,30. In line with this, we assume: H4 The higher a person’s agreeableness, the more positive attitudes they hold towards AI. The last Big Five trait, neuroticism, is connected to self-conscious and shy behavior. People with high scores in this trait tend to be more vulnerable to external stressors and have trouble controlling spontaneous impulses. MacDorman and Entezari28 discovered that neurotic individuals reported significantly stronger eeriness after they had encountered an autonomous android than those with lower scores in the trait. Similar findings emerged with regards to people’s views on self-driving cars11 and narrow AI applications30. Based on this evidence as well as the generally anxious nature of neurotic individuals, our hypothesis is as follows: H5 The higher a person’s neuroticism, the more negative attitudes they hold towards AI. Although the Big Five are considered a relatively comprehensive set of human personality traits, research has yielded several other concepts that serve to make sense of interpersonal differences (and how they relate to attitudes and behaviors). Importantly, tapping into more negatively connotated aspects of human nature, Paulhus and Williams32 introduced the Dark Triad of personality, a taxonomy that gathers three malevolent character traits: Machiavellianism, psychopathy, and narcissism. Due to their antisocial qualities, the Dark Triad have been frequently connected to deviant behaviors, interpersonal problems, and increased difficulties in the workplace38,39. Moreover, they have become an important part of exploring interpersonal differences in attitude formation, not least including views on modern-day technology10. In consequence, our second set of hypotheses revolves around the potential influence of these rather vindictive personality traits.Machiavellianism is a personality dimension that encompasses manipulative, callous, and amoral qualities. For our research topic, we contemplated two opposing notions as to how this trait could relate to AI attitudes. On the one hand, recent literature suggests that Machiavellian individuals might feel less concerned about amoral uses of AI and instead focus on its utilitarian value, which may reflect in more positive attitudes10. On the other hand, the prospect of AI-driven surveillance remains a prominent topic in the public discourse40—and we deemed it likely that people high in Machiavellianism would be wary of this, wanting for their more deviant actions to remain undetected. Weighing both arguments, we ultimately settled for the latter and hypothesized: H6 The higher a person’s Machiavellianism, the more negative attitudes they hold towards AI. A person scoring high on psychopathy is likely prone to thrill-seeking behavior and may experience only little anxiety. Unlike the deliberate manipulations typical for Machiavellian people, the psychopathic personality trait involves much more impulsive antisocial tendencies. Based on this, we came to assume a positive relationship between psychopathy and attitudes towards AI—not least considering that our dependent variable would also involve people’s emotional reactions towards AI, which should turn out less fearful among those high in psychopathy. We assumed: H7 The higher a person’s psychopathy, the more positive attitudes they hold towards AI. Broadly speaking, narcissists tend to show egocentric, proud, and unempathetic behavior, which is often accompanied by feelings of grandeur and entitlement. Whereas some qualitative research suggests that intelligent computers might deal a severe “blow to our narcissism” (41, p. 145), certain possibilities offered by AI could also appear quite attractive to people who score high in this trait—such as the futuristic notion of inserting aspects of oneself into a computer algorithm, to be preserved for all eternity. Surprisingly enough, inquiring participants about this very idea did not reveal a significant relation to narcissism in a recent study10. Then again, it should be noted that highly narcissistic individuals also tend to show open and extroverted behavior32, which would suggest a more positive relationship with AI-related attitudes depending on our other hypotheses. In summary, we settled on the following, cautiously positive assumption: H8 The higher a person’s narcissism, the more positive attitudes they hold towards AI. Lastly, we proceeded to the exploration of a dispositional variable on a notably smaller scale of abstraction, which appeared promising to us from a contemporary perspective: people’s conspiracy mentality. Strongly related to general distrust in people, institutions, and whole political systems, this personality trait focuses on the susceptibility to conspiracy theories, i.e., explanations about famous events that defy common sense or publicly presented facts33. Psychological research has shown that people who believe in one conspiracy theory are typically more likely to also believe in another, indicating a stable trait-like tendency to subscribe to an overly skeptical and distrusting way of thinking42. Further emphasizing this angle, recent research has suggested that the tendency to accept epistemically suspect information might constitute a single dimension that further connects to difficulties in analytic thinking, an overestimation of one’s own knowledge, as well as receptivity for ‘pseudo-profound bullshit’43.Although they might not be quite as well-known as other prominent conspiracy theories (e.g., concerning the 9/11 attacks), there are in fact several conspiracist beliefs about the use of sophisticated technology and AI. The respective theories range from the idea that those in power want to replace certain individuals with intelligent machines44 to the belief that COVID-19 vaccinations are actually injections of sophisticated nano-chips to establish a global surveillance system45. While these notions might seem somewhat obscure to many, the triumph of social media as a news source has presented proponents of conspiracy theories with a powerful platform to disseminate their ideas46. In addition to that, the well-documented (mis-)uses of big data—e.g., in the Cambridge Analytica scandal—has led many people to at least think about how computer algorithms might be used for sinister purposes47. Taking into account all of these observations, we anticipated that conspiracy mentality relates to more negative attitudes towards AI: H9 The higher a person’s level of conspiracy mentality, the more negative attitudes they hold towards AI. Concluding the theoretical preparation of our study, we devised hypotheses on two basic demographic variables that were previously connected to AI-related attitudes in scientific publications. Specifically, it has been found that women typically hold more negative attitudes towards AI than men, which may, among other causes, be explained by societal barriers that limit women’s access to (and interest in) technical domains6,21. Likewise, studies have hinted at the higher apprehensiveness towards AI-powered technology among the elderly, potentially due to a lack of understanding and accessibility6. In the expectation to replicate these prior findings, we proposed: H10 Women hold more negative attitudes towards AI than men. H11 Older individuals hold more negative attitudes towards AI than younger individuals. Study 1The goal underlying Study 1 was to develop a psychometrically sound scale that was (a) one-dimensional, (b) incorporated items reflecting the three bases or facets of attitudes (cognitive, affective, behavioral) that have guided attitude research in psychology for the last decades18,19, and (c) consisted of positively and negatively worded items to ensure that attitudes were measured on a full spectrum between aversion and enthusiasm (and agreement bias did not confound the results systematically). Based on theory and existing attitude scales in applied fields13,48, 24 original items were generated. Item generation was guided by the goals of developing an equal number of negatively and positively worded items and an equal number of items representing the three attitude facets (cognitive, affective, behavioral). Adhering to prior AI theory49 and existing measurement approaches13, the items were preceded by an instruction that introduced the phenomenon of artificial intelligence to participants, in order to reduce semantic ambiguities about the attitude target. This instruction is an integral part of the measurement.In a second step, the authors discussed the items, and excluded items that were ambiguous in content, too specific in terms of attitude target, or involved too much linguistic overlap with other items. The remaining scale consisted of twelve items, with each of the three psychological facets of human attitudes—cognitive, affective, and behavioral—being represented by two positively and two negatively worded items. Nevertheless, all twelve items were expected to represent one general factor: People’s attitude towards AI. The full scale (ATTARI-12) can be found in the Appendix.To gain insight into the validity of our new scale, Study 1 further included items on more specific AI applications. We expected that general attitudes towards AI as measured by the ATTARI-12 would be positively associated with specific attitudes towards electronic personal assistants (e.g., Alexa) and attitudes towards robots. Moreover, we assessed participants’ tendency to give socially desirable answers to establish whether our novel instrument would be affected by social desirability bias. Based on our careful wording during the construction of the scale, we expected that people’s attitude towards AI (per the ATTARI-12) would not be substantially related to social desirability. The hypotheses and planned data analysis steps for Study 1 were preregistered at https://aspredicted.org/8B5_GHZ (see also Supplement S1).MethodEthics statementIn Germany, institutional ethics approval is not required for psychological research as long as it does not involve issues regulated by law50. All reported studies (Study 1, Study 2, and Study 3) were conducted in full accordance with the Declaration of Helsinki, as well as the ethical guidelines provided by the German Psychological Society (DGPs51). Of course, this also included obtaining informed consent from all participants before they were able to take part in this study.Participants and procedurePower analyses with semPower (Version 2.0.152) suggested a necessary sample size of at least 500 respondents for the planned study. Therefore, at least 600 participants were aspired in order to have a proper data basis for our analyses—while still retaining a buffer for potential exclusions. As such, a total of 601 participants were recruited from the US-American MTurk participant pool. To ensure satisfactory data quality, we set our MTurk request to at least 500 previously completed tasks (also known as HITs), as well as > 98% HIT approval rate. Each participant was rewarded $1.00 for their participation, which lasted around three to five minutes.Following preregistered exclusion criteria (i.e., completion time, failing at least one of two attention checks), 111 participants were excluded from our analyses. More specifically, 31 participants completed the questionnaire in less than 120 s, 79 participants did not describe the study as instructed, and one participant reported a year of birth that differed from the reported age by more than three years. Thus, the final sample consisted of 490 participants (212 female, 273 male, 5 other or no answer). The participants were between 19 and 72 years old (M = 39.78 years, SD = 11.06). For additional demographic information, please consult Supplement S2. After participants had given their informed consent, they completed a first attention check before proceeding to the ATTARI-12 questionnaire. Following this, we assessed their attitudes towards electronic personal assistants and robots, before presenting a measure of socially desirable answering. Lastly, participants had to summarize the study’s topic as an additional attention check and respond to several sociodemographic questions.MeasuresAttitudes towards artificial intelligenceWe applied the newly created ATTARI-12, using a five-point answer format to capture participants’ responses (1 = strongly disagree, 5 = strongly agree). Descriptive measures and the reliability of the scale are reported in the Results section.Attitudes towards personal voice assistants (PVA)Participants indicated their attitudes towards PVAs on three semantic differential items (with five gradation points) that were created for the purpose of this study (e.g., “hate it—love it”; Cronbach’s α = 0.94, M = 3.72, SD = 1.09; see Supplement S3).Attitudes towards robotsPeople’s general assessment of robots was measured with three items that were previously used in robot acceptance research24. The items were answered on a four-point scale (Cronbach’s α = 0.77, M = 3.24, SD = 0.57; e.g., “Robots are a good thing for societies because they help people”, with 0 = totally disagree to 3 = totally agree, see Supplement S3).Social desirabilityParticipants’ tendency to give socially desirable answers was measured with the Social Desirability Scale53. In this measure, participants have to answer whether 16 socially desirable or undesirable actions match their own behavior on a dichotomous scale (“true” or “false”). The number of socially desirable responses is added up for each participant, resulting in a range between 0 and 16 (Cronbach’s α = 0.85, M = 8.74, SD = 0.83).ResultsOur analysis of factorial validity was guided by the assumption that the items represent a single construct: People’s attitude towards AI. To this end, we compared different, increasingly less restrictive, models in a confirmatory factor analysis: The first model (a) specified a single factor and, thus, assumed that individual differences in item responses could be explained by a single, general attitude construct. However, this assumption is often too strong in practice because specific content facets or item wording might lead to minor multidimensionality. Therefore, Model (b) estimated a bifactor S-1 structure that, in addition to the general factor, included two orthogonal specific factors for the cognitive or affective items. These specific factors captured the unique variance resulting from the two content domains that were not accounted by the general attitude factor. Following Eid and colleagues54, no specific factor was specified for the behavioral items which, thus, acted as reference domain. Because the ATTARI-12 included positively and negatively worded items, Model (c) explored potential wording effects by evaluating a bifactor S-1 model that included an orthogonal specific factor for the negatively worded items. Finally, we combined Models (b) and (c) to study the joint effects of content and method effects.The goodness of fit of these models are summarized in Table 2. Using established recommendations for the interpretation of these fit indices55, the single factor model exhibited an inferior fit to the data. Also, acknowledging the different content facets (cognitive, affective, behavioral) did not improve model fit. However, the ATTARI-12 exhibited non-negligible wording effects as demonstrated by Model (c). This model showed a good fit to the data with a comparative fit index (CFI) of 0.98, a root mean squared error (RMSEA) of 0.03, and a standardized root mean residual (SRMR) of 0.03. Despite the observed multidimensionality, all items showed good to excellent associations with the general factor as evidenced by rather high factor loadings falling between 0.48 and 0.88 (see Table 3). To examine the consequence of the multidimensional measurement structure in more detail, we examined how much of the systematic variance in the ATTARI-12 was explained by the general factor or the specific wording factor (cf.56). While the wording factor explained about 21% of the variance, most of the common variance (79%) was reflected by the general factor. As a result, the hierarchical omega reliability for the general factor was ωH = 0.83 but only ωH = 0.38 for the specific factor. These results emphasize that all twelve items comprised an essentially unidimensional scale operationalizing a strong general factor reflecting the overarching attitude towards AI. Based on the analyses, no items needed to be excluded from the scale due to subpar psychometric properties.Table 2 Goodness of fit for competing confirmatory factor models for the ATTARI-12 (Study 1; US-American MTurk Panelists).Full size tableTable 3 Factor loading pattern for the ATTARI-12 (Study 1; US-American MTurk panelists).Full size tableDescriptive and correlational results on the composite measure of the ATTARI-12 are shown in Table 4. The reliability in terms of internal consistency was excellent, Cronbach’s α = 0.93. The distribution approximated a normal distribution, skewness (= − 0.63) and kurtosis (= 0.22) were within the boundaries expected for psychometrically tested scales (also see Fig. 1). We acknowledge, however, that the distribution was somewhat left-skewed. As expected, the measure was positively correlated with attitudes towards electronic personal assistants (r = 0.60, p < 0.001) and with attitudes towards robots (r = 0.68, p < 0.001). ATTARI-12 scores were not substantially related to social desirability bias (r = 0.04, p = 0.411).Table 4 Descriptive statistics and zero-order correlations (Study 1; US-American MTurk panelists).Full size tableFigure 1Distributions of the ATTARI-12 Results in Study 1 and Study 2 (at Time 1 and Time 2).Full size imageIn sum, the one-dimensional ATTARI-12 measured attitudes towards AI in a reliable way, and the indicators of convergent (attitudes towards specific AI applications) and divergent (social desirability) construct validity corroborate the validity of the scale.Study 2The aim of the second study was to develop a German-language version of the scale and to gain further insight into the scale’s reliability and validity. In particular, we examined the re-test reliability of the scale, by administering the items at two points in time. We further assessed the extent to which the participants (undergraduates) wished to work with (or without) AI in their future careers. We expected a positive association between the latter variable and attitudes towards AI.MethodThis study consisted of two parts, administered online with an average delay of 31.40 days (SD = 2.31; range 26–36 days). Students of a social science major at the University of Würzburg, Germany participated for course credit. According to calculations with G*Power software—based on an expected small to moderate correlation of r = 0.25, 80% power, and α = 0.05—a minimum sample size of 123 participants was required. Yet, as we expected dropouts from Time 1 to Time 2, a starting sample of 180 participants was targeted. Eventually, 166 participants completed the survey at Time 1, and a total of 150 students provided data at both points in time. This final sample of 150 participants with complete data had an average age of 21.21 years (SD = 2.60; range: 18 to 41 years) and consisted predominantly of women (113 female, 36 male, 1 other). Nearly all of the participants were German native speakers (see Supplement S4 for detailed descriptive data and Supplement S8 for the German-language version of the scale). Importantly, since participants for Study 2 were recruited among German students (in contrast to the use of English-language survey panel members in Study 1), we are confident that our two validation studies were based on entirely different samples.At time 1, participants answered the ATTARI-12, as well as four items that measured their interest in a career involving AI technology (two items were reverse-coded, e.g., “I would prefer a position in which AI plays no role”). The items went with a 5-point scale (1 = strongly disagree, 5 = strongly agree), Cronbach’s α = 0.85 (see Supplement S5). In the resulting index, higher scores indicated higher aspirations for AI-related careers. At the study’s second measurement time, the ATTARI-12 had to be answered once again. Furthermore, the study was initially designed to also include a measure of social desirability at this point (53, in its German translation); however, the respective assessment did not achieve satisfactory reliability (Cronbach’s α < 0.40) and was, therefore, excluded from our analysis.ResultsThe main results are shown in Table 5. The reliability in terms of internal consistency was very good for the German-language version applied in this study, T1 Cronbach’s α = 0.91; T2 Cronbach’s α = 0.89. The distribution approximated a normal distribution, skewness (skewness T1 = − 0.58; skewness T2 = − 0.48) and kurtosis (kurtosis T1 = 0.07; kurtosis T2 = − 0.24) were within the boundaries expected for psychometrically sound scales (Fig. 1). Importantly, the test-re-test association was large, r(148) = 0.804, p < 0.001, supporting the reliability of the scale. Associations with the AI-career measure were significant and ranged around r = 0.60. In sum, the one-dimensional ATTARI-12 (German version) showed good psychometric properties.Table 5 Descriptives and zero-order correlations (Study 2; German University Students).Full size tableStudy 3Having found empirical support for the validity of our newly created measure, we proceeded with our second main research goal: Exploring potential associations between personality traits and attitudes towards AI. To ensure transparency, we decided to preregister Study 3 before data collection, including all hypotheses and planned analyses (https://aspredicted.org/VRU_BJI).MethodParticipantsAn a priori calculation of minimum sample size by means of G*Power software (assuming a small to moderate effect of f2 = 0.08, 80% power, α = 0.05, and eleven predictors in a hierarchical linear regression) resulted in a lower threshold of 221 participants. Since we intended to screen our sample for several data quality indicators and, thus, desired some leeway for potential exclusions, we recruited 353 US-American participants via the Amazon MTurk participant pool (age: M = 38.34 years, SD = 10.93; 112 female, 239 male, 2 other; payment $1.50). Specifically, we used the following MTurk criteria to ensure high data quality57,58: (a) at least 100 approved HITs; (b) HIT approval rate > 97%. Moreover, the web service “IPhub.info” was used to check whether users’ individual IP address correctly indicated the United States as a current location.Examining the collected data, several measures were taken to exclude MTurk workers whose answers indicated careless and inattentive responding. First, we asked all participants to indicate their age and year of birth on two separate pages of the survey; if these answers deviated more than two years from each other, the participant was excluded (n = 10). Second, as a specific check against bot workers, participants were asked to name a type of vegetable (“eggplant”) that was depicted on a large-scale photograph. Apart from minor typos, all answers that did not resemble the correct answer in English language led to the removal of the data (n = 6). Third, participants were asked to choose the correct study topic (“artificial intelligence”) in a multiple-choice item, a task that was not fulfilled correctly by another n = 9 participants.In addition, we initially considered removing all participants who had filled in the survey in less than four minutes, a threshold we had measured as the minimum time for attentive responding. Looking at our obtained data, this would have led to the additional exclusion of 81 participants. However, as we noticed that a lot of MTurk workers had finished the questionnaire in slightly less than four minutes, we decided to ease our initial exclusion criterion to a minimum duration of three minutes—leading to the exclusion of n = 30 participants. To make sure that this deviation from our preregistered analysis plan would not substantially change our results, we repeated all planned analyses with the initial four-minute criterion. Doing so, we found no noteworthy statistical differences (the results of this additional analysis are presented in Supplement S6).Thus, in summary, our final sample consisted of 298 participants with a mean age of 39.29 years (SD = 11.08; range from 22 to 73 years). Gender balance was slightly skewed towards men (102 female, 195 male, 1 other). Regarding ethnicity, most participants identified themselves as White (77.9%), followed by Asian (8.1%), Black (7.7%), and Hispanic (3.7%). Level of education was relatively high, with most participants having obtained either a bachelor’s degree (49.7%), a master’s degree (16.1%) or a Ph.D. (2.7%). Lastly, we note that our sample was quite balanced in terms of political orientation. For a complete overview of the obtained descriptive data, please consult Supplement S7.MeasuresAll items of the following measures were presented on 5-point scales (1 = strongly disagree; 5 = strongly agree).Attitudes towards AITo measure the main outcome variable, we administered the ATTARI-12 scale in its English version. Our data analysis showed that the scale again yielded excellent internal consistency this time around (Cronbach’s α = 0.92).Big FiveThe Big Five were assessed with the Big Five Inventory59, which consists of 44 items (openness: ten items, e.g., “I am a someone who is curious about many different things”; conscientiousness: nine items, e.g., “…does a thorough job”; extraversion: 8 items, e.g., “…is talkative”; agreeableness: nine items, e.g., “…is considerate and kind to almost everyone”; neuroticism: 8 items, e.g., “…can be tense”). We observed very good to excellent internal consistency for all five measured dimensions (openness: Cronbach’s α = 0.85; conscientiousness: Cronbach’s α = 0.85; extraversion: Cronbach’s α = 0.90; agreeableness: Cronbach’s α = 0.81; neuroticism: Cronbach’s α = 0.90).Dark TriadWe assessed participants’ Dark Triad personality traits with the Short Dark Triad scale (SD360). This instrument includes nine items on Machiavellianism (e.g., “It’s not wise to tell your secrets”), six items on psychopathy (e.g., “People often say I’m out of control.”), and nine items on narcissism (e.g., “Many group activities tend to be dull without me.”). Reliability analyses suggested good to very good internal consistencies for all three scales (Machiavellianism: Cronbach’s α = 0.84; psychopathy: Cronbach’s α = 0.83; narcissism: Cronbach’s α = 0.78).Conspiracy mentalityThe Conspiracy Mentality Questionnaire (CMQ33) offers a measure of people’s general susceptibility to conspiracy theories and beliefs. It consists of five items that encapsulate an inherent skepticism about the workings of society, governments, and secret organizations (e.g., “I think that many important things happen in the world, which the public is never informed about.”; “I think that events which superficially seem to lack a connection are often the result of secret activities.”). With our data, we observed a very good Cronbach’s α of 0.86 for the CMQ.ResultsTable 6 summarizes descriptive information for and zero-order correlations between all study variables. On average, participants’ AI-related attitudes were slightly positive, M = 3.60, SD = 0.81—considering that a value of 3 indicates the neutral midpoint of our 5-point scale, which was assembled from equal numbers of negative (inversed) and positive items.Table 6 Descriptive statistics and correlations for study variables (Study 3; US-American MTurk Panelists).Full size tableWith our main data analysis slated to involve multiple regression, we first made sure that all necessary assumptions were met61: Residuals were independent and normally distributed, and neither multicollinearity nor heteroskedascity issues could be found. Moreover, satisfying Cook’s distance values revealed that no influential cases were biasing our model. As such, we proceeded with hierarchical linear regression as the core procedure of our data analysis. Using participants’ ATTARI score as the criterion, we first entered their age and gender as predictors (Step 1), before adding the Big Five (Step 2), the Dark Triad (Step 3), and conspiracy mentality (Step 4) to an extended regression model. Table 7 presents the main calculations and coefficients of this hierarchical regression analysis. As can be seen here, the first step of the regression resulted in an insignificant equation, F(2, 294) = 2.18, p = 0.115, with an R2 of 0.02. In contrast to this, the second step of the procedure (including the Big Five) yielded a significant result, F(7, 289) = 4.10, p < 0.001, ΔR2 = 0.08. Proceeding with Step 3, we found that adding the Dark Triad traits did not lead to a significant increase of R2 (ΔR2 < 0.01, p = 0.52). Entering participants’ conspiracy mentality as a final predictor, however, Step 4 resulted in a solution with significantly higher explained variance, F(11,285) = 4.17, p < 0.001, R2 = 0.14 (ΔR2 = 0.04). In the following, we will take a closer look at the predictive value of each entered predictor.Table 7 Hierarchical regression predicting participants’ attitudes towards AI (Study 3).Full size tablePredictive value of the Big FiveIn response to hypotheses H1 through H5, we first directed our attention to the Big Five personality dimensions. Based on the second regression step, which was designed to include these variables, it was found that only agreeableness significantly predicted participants’ attitudes towards AI (p < 0.001), with a positive beta coefficient of 0.29. This positive association also persisted after entering other personality traits in Steps 3 and 4 of the regression. Hence, we conclude that higher agreeableness was related to more positive views about AI technology in our sample—providing empirical support for Hypothesis 4. Out of the other Big Five dimensions, openness to experience slightly missed the threshold of statistical significance in the final regression step, β = 0.11, p = 0.065, so that further exploration of the respective hypothesis may be warranted before rejecting it entirely. The remaining Big Five traits conscientiousness (H2), extraversion (H3), and neuroticism (H5), however, remained clearly insignificant as predictors.Predictive value of the Dark TriadProceeding to the dark personality traits Machiavellianism, psychopathy, and narcissism (as entered during Step 3), we note that none of the three predictors approached the conventional threshold of statistical significance. As such, we cannot accept hypotheses H6 through H8; according to our data, more malevolent character traits were not predictive of people’s general attitudes about AI.Predictive value of conspiracy mentalityConcluding our investigation of dispositional influences, we focused on participants’ conspiracy mentality as it was added during the regression’s final step. We found that this variable emerged as another meaningful predictor, β = − 0.22, p < 0.001. In support of Hypothesis 9, we therefore report that a stronger conspiracy mentality was related to more negative views towards artificially intelligent technology.Predictive value of age and genderLastly, the impact of participants’ age and gender on their AI attitudes was explored. To this end, we focused on our final regression model with all predictors entered into the equation. While the influence of gender remained insignificant, it was examined that higher age was significantly related to lower scores in the ATTARI-12 scale, i.e., to more aversive cognitions, feelings, and behavioral intentions towards AI (β = − 0.17, p = 0.005). In light of this, our data successfully replicated previous findings regarding this sociodemographic variable, resulting in a positive answer to H11. Meanwhile, we reject H10 on the influence of gender.General discussionIn a world shaped by autonomous technologies that are supposed to make human life safer, healthier, and more convenient, it is important to understand how people evaluate the very notion of artificially intelligent technology—and to identify factors that account for notable interindividual variance in this regard. Thus, the current project set out to explore the role of fundamental personality traits as predictors for people’s AI-related attitudes. In order to build our research upon a valid and reliable measurement of the outcome in question, we first developed a novel questionnaire—the ATTARI-12—and confirmed its psychometric quality across two studies. Distinguishing our instrument from extant alternatives, we note that our one-dimensional scale assesses attitudes towards AI on a full spectrum between aversion and enthusiasm. Furthermore, the ATTARI-12 items incorporate the classic trichotomy of human attitude (cognition, emotion, behavior), thus emerging as a conceptually sound way to measure people’s evaluation of AI. Lastly, since the instruction of the ATTARI-12 does not focus on specific use cases but rather on a broader understanding of AI as a set of technological abilities, we believe it may be suitable to be used across many different disciplines and research settings.Utilizing our newly developed measure, we proceeded to our second research aim: Investigating potential connections between individuals’ AI-related attitudes and their personality traits. Specifically, we focused on two central taxonomies from the field of personality psychology (the Big Five, the Dark Triad), as well as conspiracy mentality as a trait of high contemporary relevance. By these means, we found significant effects for two of the explored dispositional predictors: Agreeableness (one of the Big Five traits) was significantly related to more positive attitudes about AI, whereas stronger conspiracy mentality predicted the opposite. We believe that both of these findings may offer intriguing implications for researchers, developers, and users of autonomous technology.A personality dimension that typically goes along with an optimistic outlook at life62,63, agreeableness has been found to predict people’s acceptance of innovations in many different domains64. Psychologically speaking, this makes perfect sense: In line with their own benevolent nature, agreeable individuals tend to perceive others more positively as well—and may therefore come to think more about the opportunities than the risks of a new invention when forming their opinion. Since our specific measurement of participants’ attitudes requested them to consider both positive and negative aspects of AI, this tendency to focus more on the upside might have played an important role for the observed effect. Furthermore, we suppose that the trusting nature that often characterizes agreeable individuals65 offers another reasonable explanation for our result. Faced with a complex concept such as AI, which may appear obscure or downright incomprehensible to the layperson user, it arguably becomes all the more important for people to understand the intentions of the decision makers and stakeholders behind it. While the technological companies that develop AI systems might not always inspire the necessary confidence with their actions (e.g., excessive data collection, convoluted company policies), having a stronger inclination to trust others will likely compensate for this—presenting another reason as to why agreeableness might have emerged as a significant predictor in the current study.A surprisingly similar argument may be put forward when addressing the second dispositional factor that achieved notable significance in our analyses, i.e., people’s conspiracy mentality. By definition, this personality trait is also anchored in perceptions of trust (or rather, a lack thereof), a notion that is echoed by our findings: The stronger participants expressed their skepticism about governments, organizations, and related news coverage (as indicated by items such as “I think many very important things happen in the world, which the public is never informed about”), the more negatively did their attitudes about AI turn out. In our reading, this further illustrates how transparency and perceived trustworthiness crucially affect the public acceptance of AI; just as having an agreeable, credulous character predicted more favorable attitudes, the disposition to suspect sinister forces on the global stage related to more negative views about autonomous technology.On a societal level, we believe that our identification of conspiracy beliefs as a pitfall for AI-related attitudes is a particularly timely result, as it connects to the on-going debate about fake news in modern society. In the past few years, many people have started turning to social media as a source for news and education, which has paved the way for an unusual proliferation of misinformation66. Virtual spaces, in which like-minded individuals mutually confirm their attitudes and suspicious about different issues (so-called echo chambers; e.g., discussion forums or text messaging groups) have become highly prevalent, and, by these means, turned into a cause for concern among media scholars. Indeed, research suggests that using platforms such as Facebook, Twitter, or YouTube not only provides an immensely effective way to disseminate postfactual beliefs but may even increase people’s susceptibility to conspiracy theories in the first place67. Taken together with the fact that it is mostly complex and ambiguous topics for which people seek out postfactual information68, negative views about intelligent technology may proliferate most easily in the online realm. Arguably, the much-observed social media claim that COVID-19 vaccines secretly inject AI nanotechnology into unwilling patients45 illustrates this perfectly.Based on the presented arguments, it stands to reason that the successful mass-adoption of intelligent technologies may also depend on whether postfactual theories about AI can be publicly refuted. In order to do so, it might be key to increase the public’s trust, not only in the concept of AI itself, but also in the organizations and systems employing it. For instance, developers may set out to educate users about the capabilities and limits of autonomous technologies in a comprehensible way, whereas companies might strive for more accessible policies and disclosures. At the same time, we note that overcoming deep-rooted fears and suspicions about AI will likely remain a great challenge in the future; in our expectation, the sheer sophistication and complexity of intelligent computers will continue to offer fertile ground for conspiracy theorists. Also, it cannot be ruled out that attempts to make AI more transparent in the future could also backfire, as conspiracy theorists might interpret these attempts as actual proof for a conspiracy in the first place. More so, the fact that popular movies and TV shows frequently present AI-based technology in a dangerous or creepy manner (e.g., in dystopian science fiction media23) may make it even more difficult to establish sufficient levels of trust—especially among those who tend to have a more skeptical mentality to begin with.Apart from the interesting implications offered by the two significant personality predictors, several other examined traits showed no significant association with participants’ AI-related attitudes. Specifically, we found that three of the Big Five (conscientiousness, extraversion, and neuroticism) and all Dark Triad variables fell short in explaining notable variance in the obtained ATTARI-12 scores. Taken together with the abovementioned results, this indicates that user personality may have valuable yet limited potential to explain public acceptance of AI technology. In their nature as overarching meta-level traits, certain Big Five dimensions might simply be too abstract to address the nuances that characterize people’s experiences with—and attitudes towards—AI technology. For example, being a more extraverted person might involve aspects that both increase acceptance of AI (e.g., by being less apprehensive and restrained in general) and decrease it (e.g., by valuing genuine social connection to other humans, which may be challenged by new AI technology). Similarly, the more deviant personality traits included in the Dark Triad might be connected to both positive and negative views towards AI, thus preventing a significant prediction in a specific direction. Just as the many possibilities brought by new intelligent technology could be seen as useful tools to act manipulatively or enhance the self—suggesting a positive link to Machiavellianism and narcissism—they may also raise concerns among people scoring high in these traits, as AI might make it harder to act out devious impulses in an undetected or well-accepted manner.Limitations and future directionsWe would like to point out important limitations that need to be considered when interpreting the presented results of our project, especially the third and final study that served to address our main hypotheses. Although we recruited a relatively diverse sample in terms of age, gender, and socioeconomic background—and put several measures into place to ensure high data quality—our data on the association between AI attitudes and user personality are based on only one group of participants. Moreover, given that the sample of our third study was recruited via the same method as used in Study 1 (MTurk), we cannot rule out that certain participants might have taken part in both research efforts (even though, based on the very large participant pool of the chosen panel, we deem it highly unlikely that duplicate responses occurred). As such, replication efforts with different samples are encouraged in order to consolidate the yielded evidence. Not least, this concerns the recruitment of samples with a more balanced gender distribution, as our own recruitment yielded a notable majority of male participants. Also, considering that socio-economic and educational factors exert notable impact on people’s opinions towards technology (not least regarding AI; see6), shifting focus to different backgrounds should be most enlightening regarding the generalizability of our work. We believe that this might be especially important when focusing further on the dimension of conspiracy mentality—keeping in mind that a lack of education, media literacy, and analytic thinking ability have all been shown to predict the susceptibility to post-factual information43,69,70.Along these lines, we suggest that studies with samples from different cultures should be carried out in order to establish whether our findings are consistent across national borders. Most recently, a study focusing on a sample of South Korean participants also reported positive associations between attitudes towards AI and the Big Five dimension agreeableness, especially regarding the sociality and functionality of AI-powered systems71. In contrast to our research, however, the authors did not pursue a one-dimensional measurement, thus yielding ambivalent findings (e.g., conscientiousness related to negative emotions towards AI but also to expectations of high functionality). Hence, we consider it worthwhile to apply the one-dimensional ATTARI-12 in other cultures in order to better understand people’s stance towards AI—and how this attitude is shaped by certain personality traits. Keeping in mind that our newly developed scale facilitated a reliable, one-dimensional measurement with samples in two countries, it appears as a promising tool for such examinations; even though, of course, further validation of our instrument is clearly welcome, especially for its non-English versions. Moreover, intercultural comparisons will have to ensure a consistent measurement of the personality traits in question. While both the Big Five and the Dark Triad have been highlighted for their cultural invariance72,73, some studies have also raised doubts about this claim74—so that the applicability of the used personality taxonomies might pose an additional challenge in this regard. A potential solution here would be to use the HEXACO inventory of personality75, which combines both the Big Five and the Dark Triad’s deviant traits into a novel taxonomy of human personality. Not only has the HEXACO model received strong support in terms of cultural invariance76, using it would also enable scholars to cross-validate the findings of the current study with another well-established instrument. Of course, completely different personality traits than the ones included in our endeavor—e.g., impulsivity, sensation seeking, or fear of missing out—could also be helpful to gain a thorough understanding of interindividual differences regarding people’s attitudes towards AI.Additionally, it should be noted that future research into this topic can clearly benefit from assessing situational and motivational aspects that affect participants’ views on AI beyond their personality characteristics. For instance, recent research suggests that views on sophisticated technology are strongly modulated by prior exposure to science fiction media23,77, as well as philosophical views and moral norms78. Furthermore, people have been found to change their attitudes after several encounters with AI technology 79, adding another factor to the equation. Taken together, this suggests that future studies might tap into several profound covariates to disentangle the states and traits affecting attitudes towards AI. In any case, keeping in mind that our newly developed ATTARI-12 scale facilitated a reliable, one-dimensional measurement with samples of different age ranges and cultural backgrounds, we suggest that it constitutes a promising methodological cornerstone for future examinations of AI attitudes.ConclusionIn our project, we investigated attitudes towards AI as a composite measure of people’s thoughts, feelings, and behavioral intentions. Striving to complement previous research that focused more on the acceptance of specialized AI applications, we created and utilized a new unidimensional measure: the ATTARI-12. We are confident that the developed measure may now serve as a useful tool to practically address AI as a macro-level phenomenon, which—despite encompassing a host of different programs and applications—is united by several shared fundamentals. In our expectation, the created scale can still be administered even when new and unforeseen AI technologies emerge, as it focuses more on underlying principles than specific capabilities. For practitioners and developers, this perspective may be particularly insightful, suggesting that individuals’ responses to new AI technology (e.g., as customers or employees) will not only be driven by the specific features of a certain technology, but also by a general attitude towards AI. Similarly, we hope that large-scale sociodemographic efforts (e.g., national opinion compasses) might be able to benefit from the provided measure.Apart from that, we established significant connections between general AI attitudes and two selected personality traits. In our opinion, it is especially the observed association with participants’ conspiracy mentality that holds notable relevance in our increasingly complex world. If AI developers cannot find suitable ways to make their technology appear innocuous to observers (in particular to those who tend to seek out post-factual explanations), it might become increasingly challenging to establish innovations on a larger scale. However, we emphasize that negative attitudes and objections against AI technology are not necessarily unjustified; hence, when educating the public on AI technologies, people should be encouraged to reflect upon both potential risks and benefits. To reduce the risk for new conspiracy theories, we further encourage industry professionals to stay as transparent as possible when introducing their innovations to the public.Lastly, we suggest that follow-up work is all but needed to elaborate upon our current contribution. While we remain convinced that measuring attitudes towards AI as a general set of technological abilities is meritorious, more tangible results could stem from including a theoretical dichotomy that has recently emerged in the field of human–machine interaction80,81,82. More specifically, it might be worthwhile to distinguish between AI abilities that relate to agency (i.e., planning, thinking, and acting) and those that relate to experience (i.e., sensing and feeling). In turn, scholars may be able to find out whether different user traits also relate to different attitudes towards “acting” and “feeling” AI—a nuanced perspective that would still allow for broader implications across many different technological contexts. Data availability All materials, obtained data, and analysis codes for the three reported studies can be found in the project’s Open Science Framework repository (https://osf.io/3j67a/). ReferencesGrewal, D., Roggeveen, A. L. & Nordfält, J. The future of retailing. J. Retail. 93(1), 1–6. https://doi.org/10.1016/j.jretai.2016.12.008 (2017).Article Google Scholar Vanian, J. Artificial Intelligence will Obliterate These Jobs by 2030 (Fortune, 2019). https://fortune.com/2019/11/19/artificial-intelligence-will-obliterate-these-jobs-by-2030/.Ivanov, S., Kuyumdzhiev, M. & Webster, C. Automation fears: Drivers and solutions. Technol. Soc. 63, 101431. https://doi.org/10.1016/j.techsoc.2020.101431 (2020).Article Google Scholar Waytz, A. & Norton, M. I. Botsourcing and outsourcing: Robot, British, Chinese, and German workers are for thinking—not feeling—jobs. Emotion 14, 434–444. https://doi.org/10.1037/a0036054 (2014).Article PubMed Google Scholar Gherheş, V. Why are we afraid of artificial intelligence (AI)?. Eur. Rev. Appl. Sociol. 11(17), 6–15. https://doi.org/10.1515/eras-2018-0006 (2018).Article Google Scholar Liang, Y. & Lee, S. A. Fear of autonomous robots and artificial intelligence: Evidence from national representative data with probability sampling. Int. J. Soc. Robot. 9, 379–384. https://doi.org/10.1007/s12369-017-0401-3 (2017).Article Google Scholar Fast, E., & Horvitz, E. Long-term trends in the public perception of artificial intelligence. In Proceedings of the 31st AAAI Conference on Artificial Intelligence pp. 963–969 (AAAI Press, 2016).Thorp, H. H. ChatGPT is fun, but not an author. Science 379(6630), 313. https://doi.org/10.1126/science.adg7879 (2023).Article ADS PubMed Google Scholar Lobera, J., Fernández Rodríguez, C. J. & Torres-Albero, C. Privacy, values and machines: Predicting opposition to artificial intelligence. Commun. Stud. 71(3), 448–465. https://doi.org/10.1080/10510974.2020.1736114 (2020).Article Google Scholar Laakasuo, M. et al. The dark path to eternal life: Machiavellianism predicts approval of mind upload technology. Pers. Individ. Differ. 177, 110731. https://doi.org/10.1016/j.paid.2021.110731 (2021).Article Google Scholar Qu, W., Sun, H. & Ge, Y. The effects of trait anxiety and the big five personality traits on self-driving car acceptance. Transportation. https://doi.org/10.1007/s11116-020-10143-7 (2020).Kaplan, J. Artificial Intelligence: What Everyone Needs to Know. (Oxford University Press, 2016).Schepman, A. & Rodway, P. Initial validation of the general attitudes towards Artificial Intelligence Scale. Comput. Hum. Behav. Rep. 1, 100014. https://doi.org/10.1016/j.chbr.2020.100014 (2020).Article PubMed PubMed Central Google Scholar Sindermann, C. et al. Assessing the attitude towards artificial intelligence: Introduction of a short measure in German, Chinese, and English language. Künstliche Intelligenz 35(1), 109–118. https://doi.org/10.1007/s13218-020-00689-0 (2020).Article Google Scholar Wang, Y. Y., & Wang, Y. S. Development and validation of an artificial intelligence anxiety scale: An initial application in predicting motivated learning behavior. Interact. Learn. Environ. https://doi.org/10.1080/10494820.2019.1674887 (2019).Kieslich, K., Lünich, M. & Marcinkowski, F. The Threats of Artificial Intelligence Scale (TAI): Development, measurement and test over three application domains. Int. J. Soc. Robot. https://doi.org/10.1007/s12369-020-00734-w (2021).Article Google Scholar Stein, J.-P., Liebold, B. & Ohler, P. Stay back, clever thing! Linking situational control and human uniqueness concerns to the aversion against autonomous technology. Comput. Hum. Behav. 95, 73–82. https://doi.org/10.1016/j.chb.2019.01.021 (2019).Article Google Scholar Rosenberg, M. J., & Hovland, C. I. Attitude Organization and Change: An Analysis of Consistency Among Attitude Components (Yale University Press, 1960).Zanna, M. P. & Rempel, J. K. Attitudes: A new look at an old concept. In Attitudes: Their Structure, Function, and Consequences (eds Fazio, R. H. & Petty, R. E.) 7–15 (Psychology Press, 2008). Google Scholar Stein, J.-P., Appel, M., Jost, A. & Ohler, P. Matter over mind? How the acceptance of digital entities depends on their appearance, mental prowess, and the interaction between both. Int. J. Hum.-Comput. Stud. 142, 102463. https://doi.org/10.1016/j.ijhcs.2020.102463 (2020).Article Google Scholar McClure, P. K. “You’re fired”, says the robot. Soc. Sci. Comput. Rev. 36(2), 139–156. https://doi.org/10.1177/0894439317698637 (2017).Article Google Scholar Gillath, O. et al. Attachment and trust in artificial intelligence. Comput. Hum. Behav. 115, 106607. https://doi.org/10.1016/j.chb.2020.106607 (2021).Article Google Scholar Young, K. L. & Carpenter, C. Does science fiction affect political fact? Yes and no: A survey experiment on “Killer Robots”. Int. Stud. Q. 62(3), 562–576. https://doi.org/10.1093/isq/sqy028 (2018).Article Google Scholar Gnambs, T. & Appel, M. Are robots becoming unpopular? Changes in attitudes towards autonomous robotic systems in Europe. Comput. Hum. Behav. 93, 53–61. https://doi.org/10.1016/j.chb.2018.11.045 (2019).Article Google Scholar Horstmann, A. C. & Krämer, N. C. Great expectations? Relation of previous experiences with social robots in real life or in the media and expectancies based on qualitative and quantitative assessment. Front. Psychol. 10, 939. https://doi.org/10.3389/fpsyg.2019.00939 (2019).Article PubMed PubMed Central Google Scholar Sundar, S. S., Waddell, T. F. & Jung, E. H. The Hollywood robot syndrome: Media effects on older adults’ attitudes toward robots and adoption intentions. Paper presented at the 11th Annual ACM/IEEE International Conference on Human–Robot Interaction, Christchurch, New Zealand (2016).Chien, S.-Y., Sycara, K., Liu, J.-S. & Kumru, A. Relation between trust attitudes toward automation, Hofstede’s cultural dimensions, and Big Five personality traits. Proc. Hum. Factors Ergon. Soc. Annu. Meet. 60(1), 841–845. https://doi.org/10.1177/1541931213601192 (2016).Article Google Scholar MacDorman, K. F. & Entezari, S. Individual differences predict sensitivity to the uncanny valley. Interact. Stud. 16(2), 141–172. https://doi.org/10.1075/is.16.2.01mac (2015).Article Google Scholar Oksanen, A., Savela, N., Latikka, R. & Koivula, A. Trust toward robots and artificial intelligence: An experimental approach to Human-Technology interactions online. Front. Psychol. 11, 3336. https://doi.org/10.3389/fpsyg.2020.568256 (2020).Article Google Scholar Wissing, B. G. & Reinhard, M.-A. Individual differences in risk perception of artificial intelligence. Swiss J. Psychol. 77(4), 149–157. https://doi.org/10.1024/1421-0185/a000214 (2018).Article Google Scholar Costa, P. T. & McCrae, R. R. Four ways five factors are basic. Pers. Individ. Differ. 13, 653–665. https://doi.org/10.1016/0191-8869(92)90236-I (1992).Article Google Scholar Paulhus, D. L. & Williams, K. M. The Dark Triad of personality: Narcissism, Machiavellianism, and psychopathy. J. Res. Pers. 36, 556–563. https://doi.org/10.1016/S0092-6566(02)00505-6 (2002).Article Google Scholar Bruder, M., Haffke, P., Neave, N., Nouripanah, N. & Imhoff, R. Measuring individual differences in generic beliefs in conspiracy theories across cultures: Conspiracy mentality questionnaire. Front. Psychol. 4, 225. https://doi.org/10.3389/fpsyg.2013.00225 (2013).Article PubMed PubMed Central Google Scholar Imhoff, R. et al. Conspiracy mentality and political orientation across 26 countries. Nat. Hum. Behav. 6(3), 392–403. https://doi.org/10.1038/s41562-021-01258-7 (2022).Nettle, D. & Penke, L. Personality: Bridging the literatures from human psychology and behavioural ecology. Philos. Trans. R. Soc. B 365(1560), 4043–4050. https://doi.org/10.1098/rstb.2010.0061 (2010).Article Google Scholar Poropat, A. E. A meta-analysis of the five-factor model of personality and academic performance. Psychol. Bull. 135(2), 322–338. https://doi.org/10.1037/a0014996 (2009).Article PubMed Google Scholar Kaplan, A. D., Sanders, T. & Hancock, P. A. The relationship between extroversion and the tendency to anthropomorphize robots: A bayesian analysis. Front. Robot. AI 5, 135. https://doi.org/10.3389/frobt.2018.00135 (2019).Article PubMed PubMed Central Google Scholar Kaufman, S. B., Yaden, D. B., Hyde, E. & Tsukayama, E. The light vs. Dark triad of personality: Contrasting two very different profiles of human nature. Front. Psychol. 10, 467. https://doi.org/10.3389/fpsyg.2019.00467 (2019).Article PubMed PubMed Central Google Scholar LeBreton, J. M., Shiverdecker, L. K. & Grimaldi, E. M. The Dark Triad and workplace behavior. Annu. Rev. Organ. Psychol. Organ. Behav. 5(1), 387–414. https://doi.org/10.1146/annurev-orgpsych-032117-104451 (2018).Article Google Scholar Feldstein, S. The Global Expansion of AI Surveillance. Carnegie Endowment for International Piece. https://carnegieendowment.org/files/WP-Feldstein-AISurveillance_final1.pdf (2019).Malabou, C. Morphing Intelligence: From IQ Measurement to Artificial Brains (Columbia University Press, 2019).Swami, V., Chamorro-Premuzic, T. & Furnham, A. Unanswered questions: A preliminary investigation of personality and individual difference predictors of 9/11 conspiracist beliefs. Appl. Cognit. Psychol. 24, 749–761. https://doi.org/10.1002/acp.1583 (2010).Article Google Scholar Pennycook, G. & Rand, D. G. Who falls for fake news? The roles of bullshit receptivity, overclaiming, familiarity, and analytic thinking. J. Pers. 88(2), 185–200. https://doi.org/10.1111/jopy.12476 (2019).Article PubMed Google Scholar Shakarian, A. The Artificial Intelligence Conspiracy (2016).McEvoy, J. Microchips, Magnets and Shedding: Here are 5 (Debunked) Covid Vaccine Conspiracy Theories Spreading Online (Forbes, 2021). https://www.forbes.com/sites/jemimamcevoy/2021/06/03/microchips-and-shedding-here-are-5-debunked-covid-vaccine-conspiracy-theories-spreading-online/De Vynck, G., & Lerman, R. Facebook and YouTube Spent a Year Fighting Covid Misinformation. It’s Still Spreading (The Washington Post, 2021). https://www.washingtonpost.com/technology/2021/07/22/facebook-youtube-vaccine-misinformation/.González, F., Yu, Y., Figueroa, A., López, C., & Aragon, C. Global reactions to the Cambridge Analytica scandal: A cross-language social media study. In Proceedings of the 2019 World Wide Web Conference 799–806. ACM Press. https://doi.org/10.1145/3308560.3316456 (2019).Joyce, M. & Kirakowski, J. Measuring attitudes towards the internet: The general internet attitude scale. Int. J. Hum.-Comput. Interact. 31, 506–517. https://doi.org/10.1080/10447318.2015.1064657 (2015).Article Google Scholar Bartneck, C., Lütge, C., Wagner, A., & Welsh, S. An Introduction to Ethics in Robotics and AI (Springer, 2021).German Research Foundation. Statement by an Ethics Committee. https://www.dfg.de/en/research_funding/faq/faq_humanities_social_science/index.html (2023).German Psychological Society. Berufsethische Richtlinien [Work Ethical Guidelines]. https://www.dgps.de/fileadmin/user_upload/PDF/Berufsetische_Richtlinien/BER-Foederation-20230426-Web-1.pdf (2022).Jobst, L. J., Bader, M. & Moshagen, M. A tutorial on assessing statistical power and determining sample size for structural equation models. Psychol. Methods 28(1), 207–221. https://doi.org/10.1037/met0000423 (2023).Article PubMed Google Scholar Stöber, J. The Social Desirability Scale-17 (SDS-17): Convergent validity, discriminant validity, and relationship with age. Eur. J. Psychol. Assess. 17(3), 222–232. https://doi.org/10.1027/1015-5759.17.3.222 (2001).Article Google Scholar Eid, M., Geiser, C., Koch, T. & Heene, M. Anomalous results in G-factor models: Explanations and alternatives. Psychol. Methods 22, 541–562. https://doi.org/10.1037/met0000083 (2017).Article PubMed Google Scholar Schermelleh-Engel, K., Moosbrugger, H. & Müller, H. Evaluating the fit of structural equation models: Tests of significance and descriptive goodness-of-fit measures. Methods Psychol. Res. 8(2), 23–74 (2003). Google Scholar Rodriguez, A., Reise, S. P. & Haviland, M. G. Evaluating bifactor models: Calculating and interpreting statistical indices. Psychol. Methods 21(2), 137–150. https://doi.org/10.1037/met0000045 (2016).Article PubMed Google Scholar Kennedy, R. et al. The shape of and solutions to the MTurk quality crisis. Polit. Sci. Res. Methods 8(4), 614–629. https://doi.org/10.1017/psrm.2020.6 (2020).Article Google Scholar Peer, E., Vosgerau, J. & Acquisti, A. Reputation as a sufficient condition for data quality on Amazon Mechanical Turk. Behav. Res. Methods 46(4), 1023–1031. https://doi.org/10.3758/s13428-013-0434-y (2013).Article Google Scholar John, O. P., Naumann, L. P. & Soto, C. J. Paradigm shift to the integrative Big-Five trait taxonomy: History, measurement, and conceptual issues. In Handbook of Personality: Theory and Research (eds John, O. P. et al.) 114–158 (Guilford Press, 2008). Google Scholar Jones, D. N. & Paulhus, D. L. Introducing the Short Dark Triad (SD3): A brief measure of dark personality traits. Assessment 21(1), 28–41. https://doi.org/10.1177/1073191113514105 (2014).Article PubMed Google Scholar Field, A. Discovering Statistics Using IBM SPSS Statistics (SAGE, 2013). Google Scholar Baranski, E., Sweeny, K., Gardiner, G. & Funder, D. C. International optimism: Correlates and consequences of dispositional optimism across 61 countries. J. Pers. 89(2), 288–304. https://doi.org/10.1111/jopy.12582 (2020).Article PubMed Google Scholar Sharpe, J. P., Martin, N. R. & Roth, K. A. Optimism and the Big Five factors of personality: Beyond neuroticism and extraversion. Pers. Individ. Differ. 51(8), 946–951. https://doi.org/10.1016/j.paid.2011.07.033 (2011).Article Google Scholar Steel, G. D., Rinne, T. & Fairweather, J. Personality, nations, and innovation. Cross-Cult. Res. 46(1), 3–30. https://doi.org/10.1177/1069397111409124 (2011).Article Google Scholar McCarthy, M. H., Wood, J. V. & Holmes, J. G. Dispositional pathways to trust: Self-esteem and agreeableness interact to predict trust and negative emotional disclosure. J. Pers. Soc. Psychol. 113(1), 95–116. https://doi.org/10.1037/pspi0000093 (2017).Article PubMed Google Scholar Lazer, D. M. J. et al. The science of fake news. Science 359(6380), 1094–1096. https://doi.org/10.1126/science.aao2998 (2018).Article ADS CAS PubMed Google Scholar Stecula, D. A. & Pickup, M. Social media, cognitive reflection, and conspiracy beliefs. Front. Polit. Sci. 3, 62. https://doi.org/10.3389/fpos.2021.647957 (2021).Article Google Scholar van Prooijen, J. W. Why education predicts decreased belief in conspiracy theories. Appl. Cognit. Psychol. 31(1), 50–58. https://doi.org/10.1002/acp.3301 (2016).Article Google Scholar Scheibenzuber, C., Hofer, S. & Nistor, N. Designing for fake news literacy training: A problem-based undergraduate online-course. Comput. Hum. Behav. 121, 106796. https://doi.org/10.1016/j.chb.2021.106796 (2021).Article Google Scholar Sindermann, C., Schmitt, H. S., Rozgonjuk, D., Elhai, J. D. & Montag, C. The evaluation of fake and true news: On the role of intelligence, personality, interpersonal trust, ideological attitudes, and news consumption. Heliyon 7(3), e06503. https://doi.org/10.1016/j.heliyon.2021.e06503 (2021).Article PubMed PubMed Central Google Scholar Park, J. & Woo, S. E. Who likes artificial intelligence? Personality predictors of attitudes toward artificial intelligence. J. Psychol. 156(1), 68–94. https://doi.org/10.1080/00223980.2021.2012109 (2022).Article PubMed Google Scholar McCrae, R. R. & Costa, P. T. Jr. Personality trait structure as a human universal. Am. Psychol. 52(5), 509–516. https://doi.org/10.1037/0003-066X.52.5.509 (1997).Article CAS PubMed Google Scholar Rogoza, R. et al. Structure of Dark Triad Dirty Dozen across eight world regions. Assessment 28(4), 1125–1135. https://doi.org/10.1177/1073191120922611 (2020).Article PubMed Google Scholar Gurven, M., von Rueden, C., Massenkoff, M., Kaplan, H. & Lero Vie, M. How universal is the Big Five? Testing the five-factor model of personality variation among forager–farmers in the Bolivian Amazon. J. Pers. Soc. Psychol. 104(2), 354–370. https://doi.org/10.1037/a0030841 (2013).Article PubMed Google Scholar Lee, K. & Ashton, M. C. Psychometric properties of the HEXACO-100. Assessment 25, 543–556. https://doi.org/10.1177/1073191116659134 (2018).Article PubMed Google Scholar Thielmann, I. et al. The HEXACO–100 across 16 languages: A Large-Scale test of measurement invariance. J. Pers. Assess. 102(5), 714–726. https://doi.org/10.1080/00223891.2019.1614011 (2019).Article PubMed Google Scholar Mara, M. & Appel, M. Science fiction reduces the eeriness of android robots: A field experiment. Comput. Hum. Behav. 48, 156–162. https://doi.org/10.1016/j.chb.2015.01.007 (2015).Article Google Scholar Laakasuo, M. et al. What makes people approve or condemn mind upload technology? Untangling the effects of sexual disgust, purity and science fiction familiarity. Palgrave Commun. https://doi.org/10.1057/s41599-018-0124-6 (2018).Latikka, R., Savela, N., Koivula, A. & Oksanen, A. Attitudes toward robots as equipment and coworkers and the impact of robot autonomy level. Int. J. Soc. Robot. 13(7), 1747–1759. https://doi.org/10.1007/s12369-020-00743-9 (2021).Article Google Scholar Appel, M., Izydorczyk, D., Weber, S., Mara, M. & Lischetzke, T. The uncanny of mind in a machine: Humanoid robots as tools, agents, and experiencers. Comput. Hum. Behav. 102, 274–286. https://doi.org/10.1016/j.chb.2019.07.031 (2020).Article Google Scholar Gray, K. & Wegner, D. M. Feeling robots and human zombies: Mind perception and the uncanny valley. Cognition 125, 125–130. https://doi.org/10.1016/j.cognition.2012.06.007 (2012).Article PubMed Google Scholar Grundke, A., Stein, J.-P. & Appel, M. Mind-reading machines: Distinct user responses to thought-detecting and emotion-detecting robots. Technol. Mind Behav. https://doi.org/10.17605/OSF.IO/U52KM (2021).Yuan, K. H. & Bentler, P. M. Three likelihood-based methods for mean and covariance structure analysis with non-normal missing data. Sociol. Methodol. 30(1), 165–200. https://doi.org/10.1111/0081-1750.00078 (2000).Article Google Scholar Satorra, A. & Bentler, P. M. Ensuring positiveness of the scaled difference chi-square test statistic. Psychometrika 75, 243–248. https://doi.org/10.1007/s11336-009-9135-y (2010).Article MathSciNet PubMed PubMed Central Google Scholar Download referencesFundingOpen Access funding enabled and organized by Projekt DEAL.Author informationAuthors and AffiliationsDepartment of Media Psychology, Institute for Media Research, Chemnitz University of Technology, Thüringer Weg 11, 09126, Chemnitz, GermanyJan-Philipp SteinPsychology of Communication and New Media, Human-Computer-Media Institute, University of Würzburg, Würzburg, GermanyTanja Messingschlager, Fabian Hutmacher & Markus AppelLeibniz Institute for Educational Trajectories, Bamberg, GermanyTimo GnambsAuthorsJan-Philipp SteinView author publicationsYou can also search for this author in PubMed Google ScholarTanja MessingschlagerView author publicationsYou can also search for this author in PubMed Google ScholarTimo GnambsView author publicationsYou can also search for this author in PubMed Google ScholarFabian HutmacherView author publicationsYou can also search for this author in PubMed Google ScholarMarkus AppelView author publicationsYou can also search for this author in PubMed Google ScholarContributionsJ.-P.S.: Conceptualization, Methodology, Investigation, Formal Analysis, Data Curation, Writing (original draft), Writing (reviewing and editing). T.M.: Conceptualization, Methodology, Investigation, Data Curation, Writing (reviewing and editing). T.G.: Methodology, Investigation, Formal Analysis, Data Curation, Writing (reviewing and editing). F.H.: Conceptualization, Methodology, Investigation, Writing (reviewing and editing). M.A.: Conceptualization, Methodology, Formal Analysis, Data Curation, Writing (original draft), Writing (reviewing and editing), Supervision.Corresponding authorCorrespondence to Jan-Philipp Stein.Ethics declarations Competing interests The authors declare no competing interests. Additional informationPublisher's noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary InformationSupplementary Information.AppendixAppendixAttitudes towards artificial intelligence Scale (ATTARI-12), English versionInstruction: In the following, we are interested in your attitudes towards artificial intelligence (AI). AI can execute tasks that typically require human intelligence. It enables machines to sense, act, learn, and adapt in an autonomous, human-like way. AI may be part of a computer or online platform—but it can also be encountered in various other hardware devices such as robots.Item List: WordingFacetValence1AI will make this world a better place.CognitivePositive2I have strong negative emotions about AI.AffectiveNegative (reverse-coded)3I want to use technologies that rely on AI.BehavioralPositive4AI has more disadvantages than advantages.CognitiveNegative (reverse-coded)5I look forward to future AI developments.AffectivePositive6AI offers solutions to many world problems.CognitivePositive7I prefer technologies that do not feature AI.BehavioralNegative (reverse-coded)8I am afraid of AI.AffectiveNegative (reverse-coded)9I would rather choose a technology with AI than one without it.BehavioralPositive10AI creates problems rather than solving them.CognitiveNegative (reverse-coded)11When I think about AI, I have mostly positive feelings.AffectivePositive12I would rather avoid technologies that are based on AI.BehavioralNegative (reverse-coded)Rights and permissions Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissionsAbout this articleCite this articleStein, JP., Messingschlager, T., Gnambs, T. et al. Attitudes towards AI: measurement and associations with personality. Sci Rep 14, 2909 (2024). https://doi.org/10.1038/s41598-024-53335-2Download citationReceived: 03 August 2023Accepted: 31 January 2024Published: 05 February 2024DOI: https://doi.org/10.1038/s41598-024-53335-2Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative Download PDF Advertisement <a href=""//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-1526966090&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-024-53335-2%26doi%3D10.1038/s41598-024-53335-2%26subjmeta%3D258,2811,477,631,639,705%26kwrd%3DHuman+behaviour,Information+technology,Psychology""> <img data-test=""gpt-advert-fallback-img"" src=""//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-1526966090&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-024-53335-2%26doi%3D10.1038/s41598-024-53335-2%26subjmeta%3D258,2811,477,631,639,705%26kwrd%3DHuman+behaviour,Information+technology,Psychology"" alt=""Advertisement"" width=""300"" height=""250""></a> Explore content Research articles News & Comment Collections Subjects Follow us on Facebook Follow us on Twitter Sign up for alerts RSS feed About the journal About Scientific Reports Contact Journal policies Guide to referees Calls for Papers Editor's Choice Journal highlights Open Access Fees and Funding Publish with us For authors Language editing services Submit manuscript Search Search articles by subject, keyword or author Show results from All journals This journal Search Advanced search Quick links Explore articles by subject Find a job Guide to authors Editorial policies Scientific Reports (Sci Rep) ISSN 2045-2322 (online) nature.com sitemap About Nature Portfolio About us Press releases Press office Contact us Discover content Journals A-Z Articles by subject protocols.io Nature Index Publishing policies Nature portfolio policies Open access Author & Researcher services Reprints & permissions Research data Language editing Scientific editing Nature Masterclasses Research Solutions Libraries & institutions Librarian service & tools Librarian portal Open research Recommend to library Advertising & partnerships Advertising Partnerships & Services Media kits Branded content Professional development Nature Careers Nature Conferences Regional websites Nature Africa Nature China Nature India Nature Italy Nature Japan Nature Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility statement Terms & Conditions Your US state privacy rights © 2024 Springer Nature Limited Close banner Close Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly. Email address Sign up I agree my information will be processed in accordance with the Nature and Springer Nature Limited Privacy Policy. Close banner Close Get the most important science stories of the day, free in your inbox. Sign up for Nature Briefing: AI and Robotics <img hidden src=""https://verify.nature.com/verify/nature.png"" width=""0"" height=""0"" style=""display: none"" alt="""">"
